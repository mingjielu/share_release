2026-01-11 13:37:10,111 [INFO] (Process-138888 agentlightning.instrumentation.agentops)   AgentOpsServerManager initialized.
2026-01-11 13:37:10,133 [INFO] (Process-138888 agentlightning.execution.client_server)   Starting client-server execution with 128 runner(s) [role=algorithm, main_process=algorithm]
2026-01-11 13:37:10,134 [INFO] (Process-138888 agentlightning.execution.client_server)   Running algorithm solely...
2026-01-11 13:37:10,140	INFO worker.py:1832 -- Connecting to existing Ray cluster at address: 10.235.192.105:8266...
2026-01-11 13:37:10,152	INFO worker.py:2003 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://10.235.192.105:8265 [39m[22m
Connecting to external store at: http://localhost:9999
Starting training with 'qwen' configuration...
Active agent: None
Adapter agent match acknowledged: None
Store is set. Assuming v1 execution mode.
[36m(TaskRunner pid=147334)[0m {'actor_rollout_ref': {'actor': {'_target_': 'verl.workers.config.FSDPActorConfig',
[36m(TaskRunner pid=147334)[0m                                  'calculate_entropy': False,
[36m(TaskRunner pid=147334)[0m                                  'checkpoint': {'_target_': 'verl.trainer.config.CheckpointConfig',
[36m(TaskRunner pid=147334)[0m                                                 'async_save': False,
[36m(TaskRunner pid=147334)[0m                                                 'load_contents': ['model',
[36m(TaskRunner pid=147334)[0m                                                                   'optimizer',
[36m(TaskRunner pid=147334)[0m                                                                   'extra'],
[36m(TaskRunner pid=147334)[0m                                                 'save_contents': ['model',
[36m(TaskRunner pid=147334)[0m                                                                   'optimizer',
[36m(TaskRunner pid=147334)[0m                                                                   'extra']},
[36m(TaskRunner pid=147334)[0m                                  'clip_ratio': 0.2,
[36m(TaskRunner pid=147334)[0m                                  'clip_ratio_c': 3.0,
[36m(TaskRunner pid=147334)[0m                                  'clip_ratio_high': 0.3,
[36m(TaskRunner pid=147334)[0m                                  'clip_ratio_low': 0.2,
[36m(TaskRunner pid=147334)[0m                                  'data_loader_seed': 42,
[36m(TaskRunner pid=147334)[0m                                  'entropy_checkpointing': False,
[36m(TaskRunner pid=147334)[0m                                  'entropy_coeff': 0,
[36m(TaskRunner pid=147334)[0m                                  'entropy_from_logits_with_chunking': False,
[36m(TaskRunner pid=147334)[0m                                  'freeze_vision_tower': False,
[36m(TaskRunner pid=147334)[0m                                  'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig',
[36m(TaskRunner pid=147334)[0m                                                  'dtype': 'bfloat16',
[36m(TaskRunner pid=147334)[0m                                                  'entropy_checkpointing': False,
[36m(TaskRunner pid=147334)[0m                                                  'entropy_from_logits_with_chunking': False,
[36m(TaskRunner pid=147334)[0m                                                  'forward_only': False,
[36m(TaskRunner pid=147334)[0m                                                  'forward_prefetch': False,
[36m(TaskRunner pid=147334)[0m                                                  'fsdp_size': -1,
[36m(TaskRunner pid=147334)[0m                                                  'full_determinism': False,
[36m(TaskRunner pid=147334)[0m                                                  'model_dtype': 'fp32',
[36m(TaskRunner pid=147334)[0m                                                  'offload_policy': False,
[36m(TaskRunner pid=147334)[0m                                                  'optimizer_offload': True,
[36m(TaskRunner pid=147334)[0m                                                  'param_offload': True,
[36m(TaskRunner pid=147334)[0m                                                  'reshard_after_forward': True,
[36m(TaskRunner pid=147334)[0m                                                  'seed': 42,
[36m(TaskRunner pid=147334)[0m                                                  'strategy': 'fsdp',
[36m(TaskRunner pid=147334)[0m                                                  'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=147334)[0m                                                  'use_orig_params': False,
[36m(TaskRunner pid=147334)[0m                                                  'use_torch_compile': True,
[36m(TaskRunner pid=147334)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=147334)[0m                                  'grad_clip': 1.0,
[36m(TaskRunner pid=147334)[0m                                  'kl_loss_coef': 0.0,
[36m(TaskRunner pid=147334)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(TaskRunner pid=147334)[0m                                  'loss_agg_mode': 'token-mean',
[36m(TaskRunner pid=147334)[0m                                  'loss_scale_factor': None,
[36m(TaskRunner pid=147334)[0m                                  'optim': {'_target_': 'verl.workers.config.FSDPOptimizerConfig',
[36m(TaskRunner pid=147334)[0m                                            'betas': [0.9, 0.999],
[36m(TaskRunner pid=147334)[0m                                            'clip_grad': 1.0,
[36m(TaskRunner pid=147334)[0m                                            'lr': 1e-06,
[36m(TaskRunner pid=147334)[0m                                            'lr_scheduler_type': 'constant',
[36m(TaskRunner pid=147334)[0m                                            'lr_warmup_steps': -1,
[36m(TaskRunner pid=147334)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=147334)[0m                                            'min_lr_ratio': 0.0,
[36m(TaskRunner pid=147334)[0m                                            'num_cycles': 0.5,
[36m(TaskRunner pid=147334)[0m                                            'optimizer': 'AdamW',
[36m(TaskRunner pid=147334)[0m                                            'optimizer_impl': 'torch.optim',
[36m(TaskRunner pid=147334)[0m                                            'override_optimizer_config': None,
[36m(TaskRunner pid=147334)[0m                                            'total_training_steps': -1,
[36m(TaskRunner pid=147334)[0m                                            'warmup_style': None,
[36m(TaskRunner pid=147334)[0m                                            'weight_decay': 0.01},
[36m(TaskRunner pid=147334)[0m                                  'policy_loss': {'_target_': 'verl.workers.config.PolicyLossConfig',
[36m(TaskRunner pid=147334)[0m                                                  'clip_cov_lb': 1.0,
[36m(TaskRunner pid=147334)[0m                                                  'clip_cov_ratio': 0.0002,
[36m(TaskRunner pid=147334)[0m                                                  'clip_cov_ub': 5.0,
[36m(TaskRunner pid=147334)[0m                                                  'kl_cov_ratio': 0.0002,
[36m(TaskRunner pid=147334)[0m                                                  'loss_mode': 'vanilla',
[36m(TaskRunner pid=147334)[0m                                                  'ppo_kl_coef': 0.1},
[36m(TaskRunner pid=147334)[0m                                  'ppo_epochs': 1,
[36m(TaskRunner pid=147334)[0m                                  'ppo_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=147334)[0m                                  'ppo_micro_batch_size': None,
[36m(TaskRunner pid=147334)[0m                                  'ppo_micro_batch_size_per_gpu': 4,
[36m(TaskRunner pid=147334)[0m                                  'ppo_mini_batch_size': 32,
[36m(TaskRunner pid=147334)[0m                                  'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=147334)[0m                                               'all_ranks': False,
[36m(TaskRunner pid=147334)[0m                                               'enable': False,
[36m(TaskRunner pid=147334)[0m                                               'ranks': [],
[36m(TaskRunner pid=147334)[0m                                               'save_path': 'outputs/profile',
[36m(TaskRunner pid=147334)[0m                                               'tool': None,
[36m(TaskRunner pid=147334)[0m                                               'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=147334)[0m                                                                       'analysis': True,
[36m(TaskRunner pid=147334)[0m                                                                       'contents': [],
[36m(TaskRunner pid=147334)[0m                                                                       'discrete': False,
[36m(TaskRunner pid=147334)[0m                                                                       'level': 'level0'},
[36m(TaskRunner pid=147334)[0m                                                               'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=147334)[0m                                                                        'discrete': False},
[36m(TaskRunner pid=147334)[0m                                                               'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=147334)[0m                                                                         'step_end': None,
[36m(TaskRunner pid=147334)[0m                                                                         'step_start': 0},
[36m(TaskRunner pid=147334)[0m                                                               'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=147334)[0m                                                                                'stack_depth': 32,
[36m(TaskRunner pid=147334)[0m                                                                                'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=147334)[0m                                  'rollout_n': 8,
[36m(TaskRunner pid=147334)[0m                                  'router_replay': {'_target_': 'verl.workers.config.RouterReplayConfig',
[36m(TaskRunner pid=147334)[0m                                                    'mode': 'disabled',
[36m(TaskRunner pid=147334)[0m                                                    'record_file': None,
[36m(TaskRunner pid=147334)[0m                                                    'replay_file': None},
[36m(TaskRunner pid=147334)[0m                                  'shuffle': False,
[36m(TaskRunner pid=147334)[0m                                  'strategy': 'fsdp',
[36m(TaskRunner pid=147334)[0m                                  'tau_neg': 1.05,
[36m(TaskRunner pid=147334)[0m                                  'tau_pos': 1.0,
[36m(TaskRunner pid=147334)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=147334)[0m                                  'use_dynamic_bsz': False,
[36m(TaskRunner pid=147334)[0m                                  'use_fused_kernels': False,
[36m(TaskRunner pid=147334)[0m                                  'use_kl_loss': False,
[36m(TaskRunner pid=147334)[0m                                  'use_remove_padding': True,
[36m(TaskRunner pid=147334)[0m                                  'use_torch_compile': True},
[36m(TaskRunner pid=147334)[0m                        'hybrid_engine': True,
[36m(TaskRunner pid=147334)[0m                        'model': {'_target_': 'verl.workers.config.HFModelConfig',
[36m(TaskRunner pid=147334)[0m                                  'custom_chat_template': None,
[36m(TaskRunner pid=147334)[0m                                  'enable_activation_offload': False,
[36m(TaskRunner pid=147334)[0m                                  'enable_gradient_checkpointing': True,
[36m(TaskRunner pid=147334)[0m                                  'exclude_modules': None,
[36m(TaskRunner pid=147334)[0m                                  'external_lib': None,
[36m(TaskRunner pid=147334)[0m                                  'fused_kernel_options': {'impl_backend': 'torch'},
[36m(TaskRunner pid=147334)[0m                                  'hf_config_path': None,
[36m(TaskRunner pid=147334)[0m                                  'lora_adapter_path': None,
[36m(TaskRunner pid=147334)[0m                                  'lora_alpha': 16,
[36m(TaskRunner pid=147334)[0m                                  'lora_rank': 0,
[36m(TaskRunner pid=147334)[0m                                  'override_config': {},
[36m(TaskRunner pid=147334)[0m                                  'path': '/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct',
[36m(TaskRunner pid=147334)[0m                                  'target_modules': 'all-linear',
[36m(TaskRunner pid=147334)[0m                                  'tiled_mlp': {'enabled': False,
[36m(TaskRunner pid=147334)[0m                                                'num_shards': 4},
[36m(TaskRunner pid=147334)[0m                                  'tokenizer_path': None,
[36m(TaskRunner pid=147334)[0m                                  'trust_remote_code': False,
[36m(TaskRunner pid=147334)[0m                                  'use_fused_kernels': False,
[36m(TaskRunner pid=147334)[0m                                  'use_liger': False,
[36m(TaskRunner pid=147334)[0m                                  'use_remove_padding': True,
[36m(TaskRunner pid=147334)[0m                                  'use_shm': False},
[36m(TaskRunner pid=147334)[0m                        'nccl_timeout': 600,
[36m(TaskRunner pid=147334)[0m                        'ref': {'_target_': 'verl.workers.config.FSDPActorConfig',
[36m(TaskRunner pid=147334)[0m                                'entropy_checkpointing': False,
[36m(TaskRunner pid=147334)[0m                                'entropy_from_logits_with_chunking': False,
[36m(TaskRunner pid=147334)[0m                                'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig',
[36m(TaskRunner pid=147334)[0m                                                'dtype': 'bfloat16',
[36m(TaskRunner pid=147334)[0m                                                'entropy_checkpointing': False,
[36m(TaskRunner pid=147334)[0m                                                'entropy_from_logits_with_chunking': False,
[36m(TaskRunner pid=147334)[0m                                                'forward_only': True,
[36m(TaskRunner pid=147334)[0m                                                'forward_prefetch': False,
[36m(TaskRunner pid=147334)[0m                                                'fsdp_size': -1,
[36m(TaskRunner pid=147334)[0m                                                'full_determinism': False,
[36m(TaskRunner pid=147334)[0m                                                'model_dtype': 'fp32',
[36m(TaskRunner pid=147334)[0m                                                'offload_policy': False,
[36m(TaskRunner pid=147334)[0m                                                'optimizer_offload': False,
[36m(TaskRunner pid=147334)[0m                                                'param_offload': True,
[36m(TaskRunner pid=147334)[0m                                                'reshard_after_forward': True,
[36m(TaskRunner pid=147334)[0m                                                'seed': 42,
[36m(TaskRunner pid=147334)[0m                                                'strategy': 'fsdp',
[36m(TaskRunner pid=147334)[0m                                                'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=147334)[0m                                                'use_orig_params': False,
[36m(TaskRunner pid=147334)[0m                                                'use_torch_compile': True,
[36m(TaskRunner pid=147334)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=147334)[0m                                'log_prob_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=147334)[0m                                'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=147334)[0m                                'log_prob_micro_batch_size_per_gpu': 8,
[36m(TaskRunner pid=147334)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(TaskRunner pid=147334)[0m                                'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=147334)[0m                                             'all_ranks': False,
[36m(TaskRunner pid=147334)[0m                                             'enable': False,
[36m(TaskRunner pid=147334)[0m                                             'ranks': [],
[36m(TaskRunner pid=147334)[0m                                             'save_path': 'outputs/profile',
[36m(TaskRunner pid=147334)[0m                                             'tool': None,
[36m(TaskRunner pid=147334)[0m                                             'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=147334)[0m                                                                     'analysis': True,
[36m(TaskRunner pid=147334)[0m                                                                     'contents': [],
[36m(TaskRunner pid=147334)[0m                                                                     'discrete': False,
[36m(TaskRunner pid=147334)[0m                                                                     'level': 'level0'},
[36m(TaskRunner pid=147334)[0m                                                             'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=147334)[0m                                                                      'discrete': False},
[36m(TaskRunner pid=147334)[0m                                                             'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=147334)[0m                                                                       'step_end': None,
[36m(TaskRunner pid=147334)[0m                                                                       'step_start': 0},
[36m(TaskRunner pid=147334)[0m                                                             'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=147334)[0m                                                                              'stack_depth': 32,
[36m(TaskRunner pid=147334)[0m                                                                              'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=147334)[0m                                'rollout_n': 8,
[36m(TaskRunner pid=147334)[0m                                'router_replay': {'_target_': 'verl.workers.config.RouterReplayConfig',
[36m(TaskRunner pid=147334)[0m                                                  'mode': 'disabled',
[36m(TaskRunner pid=147334)[0m                                                  'record_file': None,
[36m(TaskRunner pid=147334)[0m                                                  'replay_file': None},
[36m(TaskRunner pid=147334)[0m                                'strategy': 'fsdp',
[36m(TaskRunner pid=147334)[0m                                'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=147334)[0m                                'use_torch_compile': True},
[36m(TaskRunner pid=147334)[0m                        'rollout': {'_target_': 'verl.workers.config.RolloutConfig',
[36m(TaskRunner pid=147334)[0m                                    'agent': {'_target_': 'verl.workers.config.AgentLoopConfig',
[36m(TaskRunner pid=147334)[0m                                              'agent_loop_config_path': None,
[36m(TaskRunner pid=147334)[0m                                              'custom_async_server': {'_target_': 'verl.workers.config.CustomAsyncServerConfig',
[36m(TaskRunner pid=147334)[0m                                                                      'name': 'PatchedvLLMServer',
[36m(TaskRunner pid=147334)[0m                                                                      'path': 'pkg://agentlightning.verl.async_server'},
[36m(TaskRunner pid=147334)[0m                                              'default_agent_loop': 'single_turn_agent',
[36m(TaskRunner pid=147334)[0m                                              'num_workers': 8},
[36m(TaskRunner pid=147334)[0m                                    'calculate_log_probs': False,
[36m(TaskRunner pid=147334)[0m                                    'cudagraph_capture_sizes': None,
[36m(TaskRunner pid=147334)[0m                                    'data_parallel_size': 1,
[36m(TaskRunner pid=147334)[0m                                    'disable_log_stats': True,
[36m(TaskRunner pid=147334)[0m                                    'do_sample': True,
[36m(TaskRunner pid=147334)[0m                                    'dtype': 'bfloat16',
[36m(TaskRunner pid=147334)[0m                                    'enable_chunked_prefill': True,
[36m(TaskRunner pid=147334)[0m                                    'enable_prefix_caching': True,
[36m(TaskRunner pid=147334)[0m                                    'enable_rollout_routing_replay': False,
[36m(TaskRunner pid=147334)[0m                                    'enforce_eager': False,
[36m(TaskRunner pid=147334)[0m                                    'engine_kwargs': {'sglang': {}, 'vllm': {}},
[36m(TaskRunner pid=147334)[0m                                    'expert_parallel_size': 1,
[36m(TaskRunner pid=147334)[0m                                    'free_cache_engine': True,
[36m(TaskRunner pid=147334)[0m                                    'gpu_memory_utilization': 0.8,
[36m(TaskRunner pid=147334)[0m                                    'ignore_eos': False,
[36m(TaskRunner pid=147334)[0m                                    'layered_summon': False,
[36m(TaskRunner pid=147334)[0m                                    'load_format': 'dummy',
[36m(TaskRunner pid=147334)[0m                                    'log_prob_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=147334)[0m                                    'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=147334)[0m                                    'log_prob_micro_batch_size_per_gpu': 4,
[36m(TaskRunner pid=147334)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(TaskRunner pid=147334)[0m                                    'logprobs_mode': 'processed_logprobs',
[36m(TaskRunner pid=147334)[0m                                    'max_model_len': None,
[36m(TaskRunner pid=147334)[0m                                    'max_num_batched_tokens': 8192,
[36m(TaskRunner pid=147334)[0m                                    'max_num_seqs': 1024,
[36m(TaskRunner pid=147334)[0m                                    'mode': 'async',
[36m(TaskRunner pid=147334)[0m                                    'multi_stage_wake_up': False,
[36m(TaskRunner pid=147334)[0m                                    'multi_turn': {'_target_': 'verl.workers.config.MultiTurnConfig',
[36m(TaskRunner pid=147334)[0m                                                   'enable': False,
[36m(TaskRunner pid=147334)[0m                                                   'format': 'hermes',
[36m(TaskRunner pid=147334)[0m                                                   'interaction_config_path': None,
[36m(TaskRunner pid=147334)[0m                                                   'max_assistant_turns': None,
[36m(TaskRunner pid=147334)[0m                                                   'max_parallel_calls': 1,
[36m(TaskRunner pid=147334)[0m                                                   'max_tool_response_length': 256,
[36m(TaskRunner pid=147334)[0m                                                   'max_user_turns': None,
[36m(TaskRunner pid=147334)[0m                                                   'num_repeat_rollouts': None,
[36m(TaskRunner pid=147334)[0m                                                   'tokenization_sanity_check_mode': 'strict',
[36m(TaskRunner pid=147334)[0m                                                   'tool_config_path': None,
[36m(TaskRunner pid=147334)[0m                                                   'tool_response_truncate_side': 'middle',
[36m(TaskRunner pid=147334)[0m                                                   'use_inference_chat_template': False},
[36m(TaskRunner pid=147334)[0m                                    'n': 8,
[36m(TaskRunner pid=147334)[0m                                    'name': 'vllm',
[36m(TaskRunner pid=147334)[0m                                    'over_sample_rate': 0,
[36m(TaskRunner pid=147334)[0m                                    'pipeline_model_parallel_size': 1,
[36m(TaskRunner pid=147334)[0m                                    'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=147334)[0m                                                 'all_ranks': False,
[36m(TaskRunner pid=147334)[0m                                                 'enable': False,
[36m(TaskRunner pid=147334)[0m                                                 'ranks': [],
[36m(TaskRunner pid=147334)[0m                                                 'save_path': 'outputs/profile',
[36m(TaskRunner pid=147334)[0m                                                 'tool': None,
[36m(TaskRunner pid=147334)[0m                                                 'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=147334)[0m                                                                         'analysis': True,
[36m(TaskRunner pid=147334)[0m                                                                         'contents': [],
[36m(TaskRunner pid=147334)[0m                                                                         'discrete': False,
[36m(TaskRunner pid=147334)[0m                                                                         'level': 'level0'},
[36m(TaskRunner pid=147334)[0m                                                                 'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=147334)[0m                                                                          'discrete': False},
[36m(TaskRunner pid=147334)[0m                                                                 'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=147334)[0m                                                                           'step_end': None,
[36m(TaskRunner pid=147334)[0m                                                                           'step_start': 0},
[36m(TaskRunner pid=147334)[0m                                                                 'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=147334)[0m                                                                                  'stack_depth': 32,
[36m(TaskRunner pid=147334)[0m                                                                                  'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=147334)[0m                                    'prometheus': {'_target_': 'verl.workers.config.PrometheusConfig',
[36m(TaskRunner pid=147334)[0m                                                   'enable': False,
[36m(TaskRunner pid=147334)[0m                                                   'file': '/tmp/ray/session_latest/metrics/prometheus/prometheus.yml',
[36m(TaskRunner pid=147334)[0m                                                   'port': 9090,
[36m(TaskRunner pid=147334)[0m                                                   'served_model_name': '/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct'},
[36m(TaskRunner pid=147334)[0m                                    'prompt_length': 4096,
[36m(TaskRunner pid=147334)[0m                                    'quantization': None,
[36m(TaskRunner pid=147334)[0m                                    'quantization_config_file': None,
[36m(TaskRunner pid=147334)[0m                                    'response_length': 2048,
[36m(TaskRunner pid=147334)[0m                                    'skip_dump_dir': '/tmp/rollout_dump',
[36m(TaskRunner pid=147334)[0m                                    'skip_rollout': False,
[36m(TaskRunner pid=147334)[0m                                    'skip_tokenizer_init': True,
[36m(TaskRunner pid=147334)[0m                                    'temperature': 1.0,
[36m(TaskRunner pid=147334)[0m                                    'tensor_model_parallel_size': 1,
[36m(TaskRunner pid=147334)[0m                                    'top_k': -1,
[36m(TaskRunner pid=147334)[0m                                    'top_p': 1,
[36m(TaskRunner pid=147334)[0m                                    'trace': {'_target_': 'verl.workers.config.TraceConfig',
[36m(TaskRunner pid=147334)[0m                                              'backend': None,
[36m(TaskRunner pid=147334)[0m                                              'max_samples_per_step_per_worker': None,
[36m(TaskRunner pid=147334)[0m                                              'token2text': False},
[36m(TaskRunner pid=147334)[0m                                    'update_weights_bucket_megabytes': 512,
[36m(TaskRunner pid=147334)[0m                                    'val_kwargs': {'_target_': 'verl.workers.config.SamplingConfig',
[36m(TaskRunner pid=147334)[0m                                                   'do_sample': False,
[36m(TaskRunner pid=147334)[0m                                                   'n': 1,
[36m(TaskRunner pid=147334)[0m                                                   'temperature': 0,
[36m(TaskRunner pid=147334)[0m                                                   'top_k': -1,
[36m(TaskRunner pid=147334)[0m                                                   'top_p': 1.0}}},
[36m(TaskRunner pid=147334)[0m  'agentlightning': {'port': 9999},
[36m(TaskRunner pid=147334)[0m  'agentlightningserver': {'llm_timeout_seconds': 1200},
[36m(TaskRunner pid=147334)[0m  'algorithm': {'_target_': 'verl.trainer.config.AlgoConfig',
[36m(TaskRunner pid=147334)[0m                'adv_estimator': 'grpo',
[36m(TaskRunner pid=147334)[0m                'gamma': 1.0,
[36m(TaskRunner pid=147334)[0m                'kl_ctrl': {'_target_': 'verl.trainer.config.KLControlConfig',
[36m(TaskRunner pid=147334)[0m                            'horizon': 10000,
[36m(TaskRunner pid=147334)[0m                            'kl_coef': 0.001,
[36m(TaskRunner pid=147334)[0m                            'target_kl': 0.1,
[36m(TaskRunner pid=147334)[0m                            'type': 'fixed'},
[36m(TaskRunner pid=147334)[0m                'kl_penalty': 'kl',
[36m(TaskRunner pid=147334)[0m                'lam': 1.0,
[36m(TaskRunner pid=147334)[0m                'norm_adv_by_std_in_grpo': True,
[36m(TaskRunner pid=147334)[0m                'pf_ppo': {'reweight_method': 'pow', 'weight_pow': 2.0},
[36m(TaskRunner pid=147334)[0m                'rollout_correction': {'bypass_mode': False,
[36m(TaskRunner pid=147334)[0m                                       'loss_type': 'ppo_clip',
[36m(TaskRunner pid=147334)[0m                                       'rollout_is': None,
[36m(TaskRunner pid=147334)[0m                                       'rollout_is_batch_normalize': False,
[36m(TaskRunner pid=147334)[0m                                       'rollout_is_threshold': 2.0,
[36m(TaskRunner pid=147334)[0m                                       'rollout_rs': None,
[36m(TaskRunner pid=147334)[0m                                       'rollout_rs_threshold': None,
[36m(TaskRunner pid=147334)[0m                                       'rollout_rs_threshold_lower': None,
[36m(TaskRunner pid=147334)[0m                                       'rollout_token_veto_threshold': None},
[36m(TaskRunner pid=147334)[0m                'use_kl_in_reward': False,
[36m(TaskRunner pid=147334)[0m                'use_pf_ppo': False},
[36m(TaskRunner pid=147334)[0m  'critic': {'_target_': 'verl.workers.config.FSDPCriticConfig',
[36m(TaskRunner pid=147334)[0m             'checkpoint': {'_target_': 'verl.trainer.config.CheckpointConfig',
[36m(TaskRunner pid=147334)[0m                            'async_save': False,
[36m(TaskRunner pid=147334)[0m                            'load_contents': ['model', 'optimizer', 'extra'],
[36m(TaskRunner pid=147334)[0m                            'save_contents': ['model', 'optimizer', 'extra']},
[36m(TaskRunner pid=147334)[0m             'cliprange_value': 0.5,
[36m(TaskRunner pid=147334)[0m             'data_loader_seed': 42,
[36m(TaskRunner pid=147334)[0m             'enable': None,
[36m(TaskRunner pid=147334)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=147334)[0m             'forward_micro_batch_size': None,
[36m(TaskRunner pid=147334)[0m             'forward_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=147334)[0m             'grad_clip': 1.0,
[36m(TaskRunner pid=147334)[0m             'loss_agg_mode': 'token-mean',
[36m(TaskRunner pid=147334)[0m             'model': {'_target_': 'verl.workers.config.FSDPCriticModelCfg',
[36m(TaskRunner pid=147334)[0m                       'enable_activation_offload': False,
[36m(TaskRunner pid=147334)[0m                       'enable_gradient_checkpointing': True,
[36m(TaskRunner pid=147334)[0m                       'external_lib': None,
[36m(TaskRunner pid=147334)[0m                       'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig',
[36m(TaskRunner pid=147334)[0m                                       'dtype': 'bfloat16',
[36m(TaskRunner pid=147334)[0m                                       'entropy_checkpointing': False,
[36m(TaskRunner pid=147334)[0m                                       'entropy_from_logits_with_chunking': False,
[36m(TaskRunner pid=147334)[0m                                       'forward_only': False,
[36m(TaskRunner pid=147334)[0m                                       'forward_prefetch': False,
[36m(TaskRunner pid=147334)[0m                                       'fsdp_size': -1,
[36m(TaskRunner pid=147334)[0m                                       'full_determinism': False,
[36m(TaskRunner pid=147334)[0m                                       'model_dtype': 'fp32',
[36m(TaskRunner pid=147334)[0m                                       'offload_policy': False,
[36m(TaskRunner pid=147334)[0m                                       'optimizer_offload': False,
[36m(TaskRunner pid=147334)[0m                                       'param_offload': False,
[36m(TaskRunner pid=147334)[0m                                       'reshard_after_forward': True,
[36m(TaskRunner pid=147334)[0m                                       'seed': 42,
[36m(TaskRunner pid=147334)[0m                                       'strategy': 'fsdp',
[36m(TaskRunner pid=147334)[0m                                       'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=147334)[0m                                       'use_orig_params': False,
[36m(TaskRunner pid=147334)[0m                                       'use_torch_compile': True,
[36m(TaskRunner pid=147334)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=147334)[0m                       'lora_alpha': 16,
[36m(TaskRunner pid=147334)[0m                       'lora_rank': 0,
[36m(TaskRunner pid=147334)[0m                       'override_config': {},
[36m(TaskRunner pid=147334)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(TaskRunner pid=147334)[0m                       'target_modules': 'all-linear',
[36m(TaskRunner pid=147334)[0m                       'tiled_mlp': {'enabled': False, 'num_shards': 4},
[36m(TaskRunner pid=147334)[0m                       'tokenizer_path': '/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct',
[36m(TaskRunner pid=147334)[0m                       'trust_remote_code': False,
[36m(TaskRunner pid=147334)[0m                       'use_remove_padding': False,
[36m(TaskRunner pid=147334)[0m                       'use_shm': False},
[36m(TaskRunner pid=147334)[0m             'optim': {'_target_': 'verl.workers.config.FSDPOptimizerConfig',
[36m(TaskRunner pid=147334)[0m                       'betas': [0.9, 0.999],
[36m(TaskRunner pid=147334)[0m                       'clip_grad': 1.0,
[36m(TaskRunner pid=147334)[0m                       'lr': 1e-05,
[36m(TaskRunner pid=147334)[0m                       'lr_scheduler_type': 'constant',
[36m(TaskRunner pid=147334)[0m                       'lr_warmup_steps': -1,
[36m(TaskRunner pid=147334)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=147334)[0m                       'min_lr_ratio': 0.0,
[36m(TaskRunner pid=147334)[0m                       'num_cycles': 0.5,
[36m(TaskRunner pid=147334)[0m                       'optimizer': 'AdamW',
[36m(TaskRunner pid=147334)[0m                       'optimizer_impl': 'torch.optim',
[36m(TaskRunner pid=147334)[0m                       'override_optimizer_config': None,
[36m(TaskRunner pid=147334)[0m                       'total_training_steps': -1,
[36m(TaskRunner pid=147334)[0m                       'warmup_style': None,
[36m(TaskRunner pid=147334)[0m                       'weight_decay': 0.01},
[36m(TaskRunner pid=147334)[0m             'ppo_epochs': 1,
[36m(TaskRunner pid=147334)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=147334)[0m             'ppo_micro_batch_size': None,
[36m(TaskRunner pid=147334)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=147334)[0m             'ppo_mini_batch_size': 32,
[36m(TaskRunner pid=147334)[0m             'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=147334)[0m                          'all_ranks': False,
[36m(TaskRunner pid=147334)[0m                          'enable': False,
[36m(TaskRunner pid=147334)[0m                          'ranks': [],
[36m(TaskRunner pid=147334)[0m                          'save_path': 'outputs/profile',
[36m(TaskRunner pid=147334)[0m                          'tool': None,
[36m(TaskRunner pid=147334)[0m                          'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=147334)[0m                                                  'analysis': True,
[36m(TaskRunner pid=147334)[0m                                                  'contents': [],
[36m(TaskRunner pid=147334)[0m                                                  'discrete': False,
[36m(TaskRunner pid=147334)[0m                                                  'level': 'level0'},
[36m(TaskRunner pid=147334)[0m                                          'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=147334)[0m                                                   'discrete': False},
[36m(TaskRunner pid=147334)[0m                                          'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=147334)[0m                                                    'step_end': None,
[36m(TaskRunner pid=147334)[0m                                                    'step_start': 0},
[36m(TaskRunner pid=147334)[0m                                          'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=147334)[0m                                                           'stack_depth': 32,
[36m(TaskRunner pid=147334)[0m                                                           'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=147334)[0m             'rollout_n': 8,
[36m(TaskRunner pid=147334)[0m             'shuffle': False,
[36m(TaskRunner pid=147334)[0m             'strategy': 'fsdp',
[36m(TaskRunner pid=147334)[0m             'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=147334)[0m             'use_dynamic_bsz': False},
[36m(TaskRunner pid=147334)[0m  'custom_reward_function': {'name': 'compute_score', 'path': None},
[36m(TaskRunner pid=147334)[0m  'data': {'apply_chat_template_kwargs': {},
[36m(TaskRunner pid=147334)[0m           'custom_cls': {'name': None, 'path': None},
[36m(TaskRunner pid=147334)[0m           'datagen': {'name': None, 'path': None},
[36m(TaskRunner pid=147334)[0m           'dataloader_num_workers': 8,
[36m(TaskRunner pid=147334)[0m           'filter_overlong_prompts': False,
[36m(TaskRunner pid=147334)[0m           'filter_overlong_prompts_workers': 1,
[36m(TaskRunner pid=147334)[0m           'image_key': 'images',
[36m(TaskRunner pid=147334)[0m           'image_patch_size': 14,
[36m(TaskRunner pid=147334)[0m           'max_prompt_length': 4096,
[36m(TaskRunner pid=147334)[0m           'max_response_length': 2048,
[36m(TaskRunner pid=147334)[0m           'prompt_key': 'prompt',
[36m(TaskRunner pid=147334)[0m           'return_full_prompt': False,
[36m(TaskRunner pid=147334)[0m           'return_multi_modal_inputs': True,
[36m(TaskRunner pid=147334)[0m           'return_raw_chat': True,
[36m(TaskRunner pid=147334)[0m           'return_raw_input_ids': False,
[36m(TaskRunner pid=147334)[0m           'reward_fn_key': 'data_source',
[36m(TaskRunner pid=147334)[0m           'sampler': {'class_name': None, 'class_path': None},
[36m(TaskRunner pid=147334)[0m           'seed': None,
[36m(TaskRunner pid=147334)[0m           'shuffle': True,
[36m(TaskRunner pid=147334)[0m           'tokenizer': None,
[36m(TaskRunner pid=147334)[0m           'tool_config_path': None,
[36m(TaskRunner pid=147334)[0m           'train_batch_size': 64,
[36m(TaskRunner pid=147334)[0m           'train_files': '/workspace/agent-lightning-spider/spider_sql/train_spider.parquet',
[36m(TaskRunner pid=147334)[0m           'train_max_samples': -1,
[36m(TaskRunner pid=147334)[0m           'truncation': 'error',
[36m(TaskRunner pid=147334)[0m           'trust_remote_code': False,
[36m(TaskRunner pid=147334)[0m           'use_shm': False,
[36m(TaskRunner pid=147334)[0m           'val_batch_size': None,
[36m(TaskRunner pid=147334)[0m           'val_files': '/workspace/agent-lightning-spider/spider_sql/test_dev.parquet',
[36m(TaskRunner pid=147334)[0m           'val_max_samples': -1,
[36m(TaskRunner pid=147334)[0m           'validation_shuffle': False,
[36m(TaskRunner pid=147334)[0m           'video_key': 'videos'},
[36m(TaskRunner pid=147334)[0m  'global_profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=147334)[0m                      'global_tool_config': {'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=147334)[0m                                                      'controller_nsight_options': {'cuda-graph-trace': 'graph',
[36m(TaskRunner pid=147334)[0m                                                                                    'cuda-memory-usage': 'true',
[36m(TaskRunner pid=147334)[0m                                                                                    'trace': 'cuda,nvtx,cublas,ucx'},
[36m(TaskRunner pid=147334)[0m                                                      'discrete': False,
[36m(TaskRunner pid=147334)[0m                                                      'worker_nsight_options': {'capture-range': 'cudaProfilerApi',
[36m(TaskRunner pid=147334)[0m                                                                                'capture-range-end': None,
[36m(TaskRunner pid=147334)[0m                                                                                'cuda-graph-trace': 'graph',
[36m(TaskRunner pid=147334)[0m                                                                                'cuda-memory-usage': 'true',
[36m(TaskRunner pid=147334)[0m                                                                                'kill': 'none',
[36m(TaskRunner pid=147334)[0m                                                                                'trace': 'cuda,nvtx,cublas,ucx'}},
[36m(TaskRunner pid=147334)[0m                                             'torch_memory': {'context': 'all',
[36m(TaskRunner pid=147334)[0m                                                              'kw_args': {},
[36m(TaskRunner pid=147334)[0m                                                              'stack_depth': 32,
[36m(TaskRunner pid=147334)[0m                                                              'stacks': 'all',
[36m(TaskRunner pid=147334)[0m                                                              'trace_alloc_max_entries': 100000}},
[36m(TaskRunner pid=147334)[0m                      'profile_continuous_steps': False,
[36m(TaskRunner pid=147334)[0m                      'save_path': 'outputs/profile',
[36m(TaskRunner pid=147334)[0m                      'steps': None,
[36m(TaskRunner pid=147334)[0m                      'tool': None},
[36m(TaskRunner pid=147334)[0m  'ray_kwargs': {'ray_init': {'num_cpus': None}, 'timeline_json_file': None},
[36m(TaskRunner pid=147334)[0m  'reward_manager': {'_target_': 'verl.trainer.config.config.RewardManagerConfig',
[36m(TaskRunner pid=147334)[0m                     'module': {'_target_': 'verl.trainer.config.config.ModuleConfig',
[36m(TaskRunner pid=147334)[0m                                'name': 'custom_reward_manager',
[36m(TaskRunner pid=147334)[0m                                'path': None},
[36m(TaskRunner pid=147334)[0m                     'name': 'naive',
[36m(TaskRunner pid=147334)[0m                     'source': 'register'},
[36m(TaskRunner pid=147334)[0m  'reward_model': {'enable': False,
[36m(TaskRunner pid=147334)[0m                   'enable_resource_pool': False,
[36m(TaskRunner pid=147334)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=147334)[0m                   'launch_reward_fn_async': False,
[36m(TaskRunner pid=147334)[0m                   'max_length': None,
[36m(TaskRunner pid=147334)[0m                   'micro_batch_size': None,
[36m(TaskRunner pid=147334)[0m                   'micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=147334)[0m                   'model': {'external_lib': None,
[36m(TaskRunner pid=147334)[0m                             'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig',
[36m(TaskRunner pid=147334)[0m                                             'forward_prefetch': False,
[36m(TaskRunner pid=147334)[0m                                             'fsdp_size': -1,
[36m(TaskRunner pid=147334)[0m                                             'param_offload': False,
[36m(TaskRunner pid=147334)[0m                                             'reshard_after_forward': True,
[36m(TaskRunner pid=147334)[0m                                             'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=147334)[0m                             'input_tokenizer': '/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct',
[36m(TaskRunner pid=147334)[0m                             'override_config': {},
[36m(TaskRunner pid=147334)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(TaskRunner pid=147334)[0m                             'trust_remote_code': False,
[36m(TaskRunner pid=147334)[0m                             'use_fused_kernels': False,
[36m(TaskRunner pid=147334)[0m                             'use_remove_padding': False,
[36m(TaskRunner pid=147334)[0m                             'use_shm': False},
[36m(TaskRunner pid=147334)[0m                   'n_gpus_per_node': 8,
[36m(TaskRunner pid=147334)[0m                   'nnodes': 0,
[36m(TaskRunner pid=147334)[0m                   'num_workers': 1,
[36m(TaskRunner pid=147334)[0m                   'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=147334)[0m                                'all_ranks': False,
[36m(TaskRunner pid=147334)[0m                                'enable': False,
[36m(TaskRunner pid=147334)[0m                                'ranks': [],
[36m(TaskRunner pid=147334)[0m                                'save_path': 'outputs/profile',
[36m(TaskRunner pid=147334)[0m                                'tool': None,
[36m(TaskRunner pid=147334)[0m                                'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=147334)[0m                                                        'analysis': True,[36m(WorkerDict pid=152210)[0m /workspace/agent-lightning-spider/verl/verl/utils/tokenizer.py:107: UserWarning: Failed to create processor: Unsupported processor type: Qwen2TokenizerFast. This may affect multimodal processing
[36m(WorkerDict pid=152210)[0m   warnings.warn(f"Failed to create processor: {e}. This may affect multimodal processing", stacklevel=1)

[36m(TaskRunner pid=147334)[0m                                                        'contents': [],
[36m(TaskRunner pid=147334)[0m                                                        'discrete': False,
[36m(TaskRunner pid=147334)[0m                                                        'level': 'level0'},
[36m(TaskRunner pid=147334)[0m                                                'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=147334)[0m                                                         'discrete': False},
[36m(TaskRunner pid=147334)[0m                                                'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=147334)[0m                                                          'step_end': None,
[36m(TaskRunner pid=147334)[0m                                                          'step_start': 0},
[36m(TaskRunner pid=147334)[0m                                                'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=147334)[0m                                                                 'stack_depth': 32,
[36m(TaskRunner pid=147334)[0m                                                                 'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=147334)[0m                   'reward_loop_class_name': None,
[36m(TaskRunner pid=147334)[0m                   'reward_loop_module_path': None,
[36m(TaskRunner pid=147334)[0m                   'reward_loop_source': 'register',
[36m(TaskRunner pid=147334)[0m                   'reward_manager': 'naive',
[36m(TaskRunner pid=147334)[0m                   'rollout': {'_target_': 'verl.workers.config.RolloutConfig',
[36m(TaskRunner pid=147334)[0m                               'cudagraph_capture_sizes': None,
[36m(TaskRunner pid=147334)[0m                               'data_parallel_size': 1,
[36m(TaskRunner pid=147334)[0m                               'disable_log_stats': True,
[36m(TaskRunner pid=147334)[0m                               'dtype': 'bfloat16',
[36m(TaskRunner pid=147334)[0m                               'enable_chunked_prefill': True,
[36m(TaskRunner pid=147334)[0m                               'enable_prefix_caching': True,
[36m(TaskRunner pid=147334)[0m                               'enforce_eager': True,
[36m(TaskRunner pid=147334)[0m                               'engine_kwargs': {},
[36m(TaskRunner pid=147334)[0m                               'expert_parallel_size': 1,
[36m(TaskRunner pid=147334)[0m                               'free_cache_engine': True,
[36m(TaskRunner pid=147334)[0m                               'gpu_memory_utilization': 0.5,
[36m(TaskRunner pid=147334)[0m                               'limit_images': None,
[36m(TaskRunner pid=147334)[0m                               'load_format': 'auto',
[36m(TaskRunner pid=147334)[0m                               'max_model_len': None,
[36m(TaskRunner pid=147334)[0m                               'max_num_batched_tokens': 8192,
[36m(TaskRunner pid=147334)[0m                               'max_num_seqs': 1024,
[36m(TaskRunner pid=147334)[0m                               'name': '???',
[36m(TaskRunner pid=147334)[0m                               'prompt_length': 2048,
[36m(TaskRunner pid=147334)[0m                               'response_length': 2048,
[36m(TaskRunner pid=147334)[0m                               'skip_tokenizer_init': False,
[36m(TaskRunner pid=147334)[0m                               'tensor_model_parallel_size': 2},
[36m(TaskRunner pid=147334)[0m                   'sandbox_fusion': {'max_concurrent': 64,
[36m(TaskRunner pid=147334)[0m                                      'memory_limit_mb': 1024,
[36m(TaskRunner pid=147334)[0m                                      'url': None},
[36m(TaskRunner pid=147334)[0m                   'strategy': 'fsdp',
[36m(TaskRunner pid=147334)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=147334)[0m                   'use_dynamic_bsz': False,
[36m(TaskRunner pid=147334)[0m                   'use_reward_loop': True},
[36m(TaskRunner pid=147334)[0m  'trainer': {'balance_batch': True,
[36m(TaskRunner pid=147334)[0m              'critic_warmup': 0,
[36m(TaskRunner pid=147334)[0m              'default_hdfs_dir': None,
[36m(TaskRunner pid=147334)[0m              'default_local_dir': '/workspace/agent-lightning-spider/checkpoint',
[36m(TaskRunner pid=147334)[0m              'del_local_ckpt_after_load': False,
[36m(TaskRunner pid=147334)[0m              'device': 'cuda',
[36m(TaskRunner pid=147334)[0m              'esi_redundant_time': 0,
[36m(TaskRunner pid=147334)[0m              'experiment_name': 'spider-async-mix308-open-source-vllm0.11',
[36m(TaskRunner pid=147334)[0m              'log_val_generations': 0,
[36m(TaskRunner pid=147334)[0m              'logger': ['console', 'wandb'],
[36m(TaskRunner pid=147334)[0m              'max_actor_ckpt_to_keep': None,
[36m(TaskRunner pid=147334)[0m              'max_critic_ckpt_to_keep': None,
[36m(TaskRunner pid=147334)[0m              'n_gpus_per_node': 8,
[36m(TaskRunner pid=147334)[0m              'nnodes': 1,
[36m(TaskRunner pid=147334)[0m              'project_name': 'AgentLightning',
[36m(TaskRunner pid=147334)[0m              'ray_wait_register_center_timeout': 300,
[36m(TaskRunner pid=147334)[0m              'resume_from_path': None,
[36m(TaskRunner pid=147334)[0m              'resume_mode': 'auto',
[36m(TaskRunner pid=147334)[0m              'rollout_data_dir': '/workspace/agent-lightning-spider/checkpoint/rollout',
[36m(TaskRunner pid=147334)[0m              'save_freq': -1,
[36m(TaskRunner pid=147334)[0m              'test_freq': 10,
[36m(TaskRunner pid=147334)[0m              'total_epochs': 10000,
[36m(TaskRunner pid=147334)[0m              'total_training_steps': 100,
[36m(TaskRunner pid=147334)[0m              'use_legacy_worker_impl': 'auto',
[36m(TaskRunner pid=147334)[0m              'val_before_train': True,
[36m(TaskRunner pid=147334)[0m              'val_only': False,
[36m(TaskRunner pid=147334)[0m              'validation_data_dir': '/workspace/agent-lightning-spider/checkpoint/valid'},
[36m(TaskRunner pid=147334)[0m  'transfer_queue': {'enable': False}}
[36m(TaskRunner pid=147334)[0m tilearn roll packer disabled!!!
[36m(TaskRunner pid=147334)[0m Size of train dataloader: 109, Size of val dataloader: 1
[36m(TaskRunner pid=147334)[0m Total training steps: 100
[36m(TaskRunner pid=147334)[0m colocated worker base class <class 'verl.single_controller.base.worker.Worker'>
[36m(TaskRunner pid=147334)[0m bind role actor_rollout method chat_completion to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(TaskRunner pid=147334)[0m bind role actor_rollout method generate to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(TaskRunner pid=147334)[0m bind role actor_rollout method get_zeromq_address to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(TaskRunner pid=147334)[0m bind role actor_rollout method sleep to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(TaskRunner pid=147334)[0m bind role actor_rollout method wake_up to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(WorkerDict pid=152208)[0m [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[36m(WorkerDict pid=152205)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=152205)[0m   "architectures": [
[36m(WorkerDict pid=152205)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=152205)[0m   ],
[36m(WorkerDict pid=152205)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=152205)[0m   "dtype": "bfloat16",
[36m(WorkerDict pid=152205)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=152205)[0m   "hidden_act": "silu",[36m(WorkerDict pid=152205)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=152205)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=152205)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[36m(WorkerDict pid=152207)[0m /workspace/agent-lightning-spider/verl/verl/utils/tokenizer.py:107: UserWarning: Failed to create processor: Unsupported processor type: Qwen2TokenizerFast. This may affect multimodal processing[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=152207)[0m   warnings.warn(f"Failed to create processor: {e}. This may affect multimodal processing", stacklevel=1)[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152210)[0m libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 8 (supports 1 to 1) for device /sys/class/infiniband/bnxt_re7
[36m(WorkerDict pid=152210)[0m libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 8 (supports 1 to 1) for device /sys/class/infiniband/bnxt_re7
[36m(WorkerDict pid=152210)[0m libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 8 (supports 1 to 1) for device /sys/class/infiniband/bnxt_re6
[36m(WorkerDict pid=152210)[0m libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 8 (supports 1 to 1) for device /sys/class/infiniband/bnxt_re6
[36m(WorkerDict pid=152210)[0m libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 8 (supports 1 to 1) for device /sys/class/infiniband/bnxt_re5
[36m(WorkerDict pid=152210)[0m libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 8 (supports 1 to 1) for device /sys/class/infiniband/bnxt_re5
[36m(WorkerDict pid=152210)[0m libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 8 (supports 1 to 1) for device /sys/class/infiniband/bnxt_re4
[36m(WorkerDict pid=152210)[0m libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 8 (supports 1 to 1) for device /sys/class/infiniband/bnxt_re4
[36m(WorkerDict pid=152210)[0m libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 8 (supports 1 to 1) for device /sys/class/infiniband/bnxt_re3
[36m(WorkerDict pid=152210)[0m libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 8 (supports 1 to 1) for device /sys/class/infiniband/bnxt_re3
[36m(WorkerDict pid=152210)[0m libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 8 (supports 1 to 1) for device /sys/class/infiniband/bnxt_re2
[36m(WorkerDict pid=152210)[0m libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 8 (supports 1 to 1) for device /sys/class/infiniband/bnxt_re2
[36m(WorkerDict pid=152210)[0m libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 8 (supports 1 to 1) for device /sys/class/infiniband/bnxt_re1
[36m(WorkerDict pid=152210)[0m libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 8 (supports 1 to 1) for device /sys/class/infiniband/bnxt_re1
[36m(WorkerDict pid=152210)[0m libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 8 (supports 1 to 1) for device /sys/class/infiniband/bnxt_re0
[36m(WorkerDict pid=152210)[0m libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 8 (supports 1 to 1) for device /sys/class/infiniband/bnxt_re0
[36m(WorkerDict pid=152206)[0m `torch_dtype` is deprecated! Use `dtype` instead![32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152206)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`[32m [repeated 14x across cluster][0m

[36m(WorkerDict pid=152205)[0m   "hidden_size": 1536,
[36m(WorkerDict pid=152205)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=152205)[0m   "intermediate_size": 8960,
[36m(WorkerDict pid=152205)[0m   "layer_types": [
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention",
[36m(WorkerDict pid=152205)[0m     "full_attention"
[36m(WorkerDict pid=152205)[0m   ],
[36m(WorkerDict pid=152205)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=152205)[0m   "max_window_layers": 28,
[36m(WorkerDict pid=152205)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=152205)[0m   "num_attention_heads": 12,
[36m(WorkerDict pid=152205)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=152205)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=152205)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=152205)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=152205)[0m   "rope_scaling": null,
[36m(WorkerDict pid=152205)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=152205)[0m   "sliding_window": null,
[36m(WorkerDict pid=152205)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=152205)[0m   "transformers_version": "4.57.1",
[36m(WorkerDict pid=152205)[0m   "use_cache": true,
[36m(WorkerDict pid=152205)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=152205)[0m   "vocab_size": 151936
[36m(WorkerDict pid=152205)[0m }
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152208)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=152208)[0m Skipping monkey patch for Qwen2ForCausalLM as use_fused_kernels is False or fused_kernels_backend is torch
[36m(WorkerDict pid=152207)[0m [Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152205)[0m Qwen2ForCausalLM contains 1.54B parameters
[36m(WorkerDict pid=152205)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fff9d2e5260>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fff9d2e5120>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m [2026-01-11 13:37:47] mi308-ccs-aus-e07-03:152208:153118 [3] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could not find any local path from gpu 1 to net.
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m [2026-01-11 13:37:47] mi308-ccs-aus-e07-03:152208:153118 [3] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could not find any local path from gpu 1 to net.
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m [2026-01-11 13:37:47] mi308-ccs-aus-e07-03:152208:153118 [3] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could not find any local path from gpu 2 to net.
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m [2026-01-11 13:37:47] mi308-ccs-aus-e07-03:152208:153118 [3] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could not find any local path from gpu 2 to net.
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m [2026-01-11 13:37:47] mi308-ccs-aus-e07-03:152208:153118 [3] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could not find any local path from gpu 3 to net.
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m [2026-01-11 13:37:47] mi308-ccs-aus-e07-03:152208:153118 [3] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could not find any local path from gpu 3 to net.
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m [2026-01-11 13:37:47] mi308-ccs-aus-e07-03:152208:153118 [3] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could not find any local path from gpu 4 to net.
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m [2026-01-11 13:37:47] mi308-ccs-aus-e07-03:152208:153118 [3] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could not find any local path from gpu 6 to net.
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m [2026-01-11 13:37:47] mi308-ccs-aus-e07-03:152208:153118 [3] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could not find any local path from gpu 7 to net.
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m [2026-01-11 13:37:47] mi308-ccs-aus-e07-03:152208:153118 [3] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could not find any local path from gpu 5 to net.
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m [2026-01-11 13:37:47] mi308-ccs-aus-e07-03:152208:153118 [3] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could not find any local path from gpu 3 to net.
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m [2026-01-11 13:37:47] mi308-ccs-aus-e07-03:152208:153118 [3] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could not find any local path from gpu 4 to net.
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m [2026-01-11 13:37:47] mi308-ccs-aus-e07-03:152208:153118 [3] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could not find any local path from gpu 7 to net.
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m [2026-01-11 13:37:47] mi308-ccs-aus-e07-03:152208:153118 [3] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could not find any local path from gpu 2 to net.[36m(WorkerDict pid=152208)[0m /workspace/agent-lightning-spider/verl/verl/utils/profiler/config.py:52: UserWarning: Torch profiler tool config is not fully supported now.
[36m(WorkerDict pid=152208)[0m   warnings.warn("Torch profiler tool config is not fully supported now.", stacklevel=1)
[36m(WorkerDict pid=152208)[0m /workspace/agent-lightning-spider/verl/verl/utils/tokenizer.py:107: UserWarning: Failed to create processor: Unsupported processor type: Qwen2TokenizerFast. This may affect multimodal processing
[36m(WorkerDict pid=152208)[0m   warnings.warn(f"Failed to create processor: {e}. This may affect multimodal processing", stacklevel=1)
[36m(WorkerDict pid=152209)[0m libibverbs: Warning: Driver bnxt_re does not support the kernel ABI of 8 (supports 1 to 1) for device /sys/class/infiniband/bnxt_re0[32m [repeated 112x across cluster][0m

[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m [2026-01-11 13:37:47] mi308-ccs-aus-e07-03:152208:153118 [3] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could
[36m(WorkerDict pid=152206)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152206)[0m Skipping monkey patch for Qwen2ForCausalLM as use_fused_kernels is False or fused_kernels_backend is torch[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152205)[0m RCCL version : 2.26.6-HEAD:64f48b6
[36m(WorkerDict pid=152205)[0m HIP version  : 7.0.51831-7c9236b16
[36m(WorkerDict pid=152205)[0m ROCm version : 7.0.2.0-56-9428210
[36m(WorkerDict pid=152205)[0m Hostname     : mi308-ccs-aus-e07-03.prov.aus.ccs.cpe.ice.amd.com
[36m(WorkerDict pid=152205)[0m Librccl path : /opt/rocm/lib/librccl.so.1
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m [2026-01-11 13:37:47
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152205)[0m Total steps: 100, num_warmup_steps: 0
[36m(WorkerDict pid=152205)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=152205)[0m Actor use_fused_kernels=False
[36m(WorkerDict pid=152208)[0m  not find any local path from gpu 1 to net.
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m 
[36m(WorkerDict pid=152208)[0m [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[36m(WorkerDict pid=152208)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=152208)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152206)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152212)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152210)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152211)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152209)[0m 
[36m(WorkerDict pid=152205)[0m ] mi308-ccs-aus-e07-03:152205:153112 [0] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could not find any local path from gpu 1 to net.[36m(WorkerDict pid=152208)[0m /usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=152208)[0m   warnings.warn(

[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152205)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152207)[0m 
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:37:50 [plugins/__init__.py:32] No plugins for group vllm.platform_plugins found.
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:37:50 [platforms/__init__.py:36] Checking if TPU platform is available.
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:37:50 [platforms/__init__.py:55] TPU platform is not available because: No module named 'libtpu'
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:37:50 [platforms/__init__.py:61] Checking if CUDA platform is available.
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:37:50 [platforms/__init__.py:88] Exception happens when checking CUDA platform: NVML Shared Library Not Found
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:37:50 [platforms/__init__.py:105] CUDA platform is not available because: NVML Shared Library Not Found
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:37:50 [platforms/__init__.py:112] Checking if ROCm platform is available.
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:37:50 [platforms/__init__.py:120] Confirmed ROCm platform is available.
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:37:50 [platforms/__init__.py:133] Checking if XPU platform is available.
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:37:50 [platforms/__init__.py:153] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:37:50 [platforms/__init__.py:160] Checking if CPU platform is available.
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:37:51 [platforms/__init__.py:225] Automatically detected platform rocm.
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:37:52 [utils/flashinfer.py:55] FlashInfer unavailable since package was not found
[36m(WorkerDict pid=152207)[0m [2026-01-11 13:37:47] mi308-ccs-aus-e07-03:152207:153115 [2] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could not find any local path from gpu 2 to net.[32m [repeated 202x across cluster][0m
[36m(WorkerDict pid=152207)[0m [2026-01-11 13:37:47] mi308-ccs-aus-e07-03:152207:153115 [2] /longer_pathname_so_that_rpms_can_support_packaging_the_debug_info_for_all_os_profiles/src/out/ubuntu-22.04/22.04/build/rccl/hipify/src/graph/topo.cc:1550 NCCL WARN Could[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=152207)[0m  not find any local path from gpu 1 to net.[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=152207)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0[32m [repeated 21x across cluster][0m
[36m(pid=153931)[0m DEBUG 01-11 13:38:01 [plugins/__init__.py:32] No plugins for group vllm.platform_plugins found.[32m [repeated 9x across cluster][0m
[36m(pid=153931)[0m DEBUG 01-11 13:38:01 [platforms/__init__.py:36] Checking if TPU platform is available.[32m [repeated 9x across cluster][0m
[36m(pid=153931)[0m DEBUG 01-11 13:38:01 [platforms/__init__.py:55] TPU platform is not available because: No module named 'libtpu'[32m [repeated 9x across cluster][0m
[36m(pid=153931)[0m DEBUG 01-11 13:38:01 [platforms/__init__.py:61] Checking if CUDA platform is available.[32m [repeated 9x across cluster][0m
[36m(TaskRunner pid=147334)[0m DEBUG 01-11 13:37:53 [platforms/__init__.py:88] Exception happens when checking CUDA platform: NVML Shared Library Not Found[32m [repeated 8x across cluster][0m
[36m(TaskRunner pid=147334)[0m DEBUG 01-11 13:37:53 [platforms/__init__.py:105] CUDA platform is not available because: NVML Shared Library Not Found[32m [repeated 8x across cluster][0m
[36m(TaskRunner pid=147334)[0m DEBUG 01-11 13:37:53 [platforms/__init__.py:112] Checking if ROCm platform is available.[32m [repeated 17x across cluster][0m
[36m(TaskRunner pid=147334)[0m DEBUG 01-11 13:37:53 [platforms/__init__.py:120] Confirmed ROCm platform is available.[32m [repeated 17x across cluster][0m
[36m(TaskRunner pid=147334)[0m DEBUG 01-11 13:37:53 [platforms/__init__.py:133] Checking if XPU platform is available.[32m [repeated 8x across cluster][0m
[36m(TaskRunner pid=147334)[0m DEBUG 01-11 13:37:53 [platforms/__init__.py:153] XPU platform is not available because: No module named 'intel_extension_for_pytorch'[32m [repeated 8x across cluster][0m
[36m(TaskRunner pid=147334)[0m DEBUG 01-11 13:37:53 [platforms/__init__.py:160] Checking if CPU platform is available.[32m [repeated 8x across cluster][0m
[36m(TaskRunner pid=147334)[0m DEBUG 01-11 13:37:53 [platforms/__init__.py:225] Automatically detected platform rocm.[32m [repeated 8x across cluster][0m
[36m(TaskRunner pid=147334)[0m DEBUG 01-11 13:37:55 [utils/flashinfer.py:55] FlashInfer unavailable since package was not found[32m [repeated 8x across cluster][0m
[36m(vLLMHttpServer pid=153932)[0m DEBUG 01-11 13:38:06 [plugins/__init__.py:40] Available plugins for group vllm.general_plugins:
[36m(vLLMHttpServer pid=153932)[0m DEBUG 01-11 13:38:06 [plugins/__init__.py:42] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[36m(vLLMHttpServer pid=153932)[0m DEBUG 01-11 13:38:06 [plugins/__init__.py:45] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(vLLMHttpServer pid=153932)[0m INFO 01-11 13:38:06 [config/scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.
[36m(vLLMHttpServer pid=153932)[0m DEBUG 01-11 13:38:06 [model_executor/models/registry.py:607] Cached model info file for class vllm.model_executor.models.qwen2.Qwen2ForCausalLM is stale
[36m(vLLMHttpServer pid=153932)[0m DEBUG 01-11 13:38:06 [model_executor/models/registry.py:659] Cache model info for class vllm.model_executor.models.qwen2.Qwen2ForCausalLM miss. Loading model instead.
[36m(pid=153930)[0m DEBUG 01-11 13:38:01 [plugins/__init__.py:32] No plugins for group vllm.platform_plugins found.[32m [repeated 7x across cluster][0m
[36m(pid=153930)[0m DEBUG 01-11 13:38:01 [platforms/__init__.py:36] Checking if TPU platform is available.[32m [repeated 7x across cluster][0m
[36m(pid=153930)[0m DEBUG 01-11 13:38:01 [platforms/__init__.py:55] TPU platform is not available because: No module named 'libtpu'[32m [repeated 7x across cluster][0m
[36m(pid=153930)[0m DEBUG 01-11 13:38:01 [platforms/__init__.py:61] Checking if CUDA platform is available.[32m [repeated 7x across cluster][0m
[36m(pid=153930)[0m DEBUG 01-11 13:38:01 [platforms/__init__.py:88] Exception happens when checking CUDA platform: NVML Shared Library Not Found[32m [repeated 8x across cluster][0m
[36m(pid=153930)[0m DEBUG 01-11 13:38:01 [platforms/__init__.py:105] CUDA platform is not available because: NVML Shared Library Not Found[32m [repeated 8x across cluster][0m
[36m(pid=153926)[0m DEBUG 01-11 13:38:01 [platforms/__init__.py:112] Checking if ROCm platform is available.[32m [repeated 16x across cluster][0m
[36m(pid=153928)[0m DEBUG 01-11 13:38:01 [platforms/__init__.py:120] Confirmed ROCm platform is available.[32m [repeated 16x across cluster][0m
[36m(pid=153926)[0m DEBUG 01-11 13:38:01 [platforms/__init__.py:133] Checking if XPU platform is available.[32m [repeated 8x across cluster][0m
[36m(pid=153926)[0m DEBUG 01-11 13:38:01 [platforms/__init__.py:153] XPU platform is not available because: No module named 'intel_extension_for_pytorch'[32m [repeated 8x across cluster][0m[36m(vLLMHttpServer pid=153932)[0m INFO:2026-01-11 13:38:06,738:vLLMHttpServer, replica_rank: 2, master address: 10.235.192.105, master port: 34755, data parallel master port: 45609
[36m(vLLMHttpServer pid=153932)[0m INFO:2026-01-11 13:38:06,744:override_generation_config: {'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'repetition_penalty': 1.0, 'max_new_tokens': 2048}
[36m(vLLMHttpServer pid=153932)[0m INFO:2026-01-11 13:38:06,744:enable_sleep_mode: True
[36m(vLLMHttpServer pid=153932)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(vLLMHttpServer pid=153932)[0m INFO:2026-01-11 13:38:06,767:replica_rank=2, node_rank=0, nnodes=1, get worker zmq addresses: ['ipc:///tmp/verl_vllm_zmq_152207_root.ipc']
[36m(WorkerDict pid=152210)[0m /workspace/agent-lightning-spider/verl/verl/utils/profiler/config.py:52: UserWarning: Torch profiler tool config is not fully supported now.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152210)[0m   warnings.warn("Torch profiler tool config is not fully supported now.", stacklevel=1)[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153932)[0m /workspace/agent-lightning-spider/verl/verl/utils/tokenizer.py:107: UserWarning: Failed to create processor: Unsupported processor type: Qwen2TokenizerFast. This may affect multimodal processing[32m [repeated 8x across cluster][0m
[36m(vLLMHttpServer pid=153932)[0m   warnings.warn(f"Failed to create processor: {e}. This may affect multimodal processing", stacklevel=1)[32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=152210)[0m /usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152210)[0m   warnings.warn([32m [repeated 7x across cluster][0m

[36m(pid=153926)[0m DEBUG 01-11 13:38:01 [platforms/__init__.py:160] Checking if CPU platform is available.[32m [repeated 8x across cluster][0m
[36m(pid=153928)[0m DEBUG 01-11 13:38:01 [platforms/__init__.py:225] Automatically detected platform rocm.[32m [repeated 8x across cluster][0m
[36m(pid=153926)[0m DEBUG 01-11 13:38:04 [utils/flashinfer.py:55] FlashInfer unavailable since package was not found[32m [repeated 8x across cluster][0m
[36m(vLLMHttpServer pid=153931)[0m ['serve',
[36m(vLLMHttpServer pid=153931)[0m  '/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct',
[36m(vLLMHttpServer pid=153931)[0m  '--dtype',
[36m(vLLMHttpServer pid=153931)[0m  'bfloat16',
[36m(vLLMHttpServer pid=153931)[0m  '--load_format',
[36m(vLLMHttpServer pid=153931)[0m  'dummy',
[36m(vLLMHttpServer pid=153931)[0m  '--max_model_len',
[36m(vLLMHttpServer pid=153931)[0m  '32768',
[36m(vLLMHttpServer pid=153931)[0m  '--max_num_seqs',
[36m(vLLMHttpServer pid=153931)[0m  '1024',
[36m(vLLMHttpServer pid=153931)[0m  '--enable_chunked_prefill',
[36m(vLLMHttpServer pid=153931)[0m  '--max_num_batched_tokens',
[36m(vLLMHttpServer pid=153931)[0m  '8192',
[36m(vLLMHttpServer pid=153931)[0m  '--enable_prefix_caching',
[36m(vLLMHttpServer pid=153931)[0m  '--enable_sleep_mode',
[36m(vLLMHttpServer pid=153931)[0m  '--logprobs_mode',
[36m(vLLMHttpServer pid=153931)[0m  'processed_logprobs',
[36m(vLLMHttpServer pid=153931)[0m  '--disable_custom_all_reduce',
[36m(vLLMHttpServer pid=153931)[0m  '--gpu_memory_utilization',
[36m(vLLMHttpServer pid=153931)[0m  '0.8',
[36m(vLLMHttpServer pid=153931)[0m  '--disable_log_stats',
[36m(vLLMHttpServer pid=153931)[0m  '--tensor_parallel_size',
[36m(vLLMHttpServer pid=153931)[0m  '1',
[36m(vLLMHttpServer pid=153931)[0m  '--seed',
[36m(vLLMHttpServer pid=153931)[0m  '0',
[36m(vLLMHttpServer pid=153931)[0m  '--override_generation_config',
[36m(vLLMHttpServer pid=153931)[0m  '{"temperature": 1.0, "top_k": -1, "top_p": 1, "repetition_penalty": 1.0, '
[36m(vLLMHttpServer pid=153931)[0m  '"max_new_tokens": 2048}',
[36m(vLLMHttpServer pid=153931)[0m  '--hf_overrides',
[36m(vLLMHttpServer pid=153931)[0m  '{}']
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:13 [model_executor/models/registry.py:669] Loaded model info for class vllm.model_executor.models.qwen2.Qwen2ForCausalLM
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:13 [logging_utils/log_time.py:29] Registry inspect model class: Elapsed time 6.1948454 secs
[36m(vLLMHttpServer pid=153927)[0m INFO 01-11 13:38:13 [config/model.py:631] Resolved architecture: Qwen2ForCausalLM
[36m(vLLMHttpServer pid=153927)[0m INFO 01-11 13:38:13 [config/model.py:1745] Using max model len 32768
[36m(vLLMHttpServer pid=153927)[0m INFO 01-11 13:38:13 [engine/arg_utils.py:1443] Using ray runtime env (env vars redacted): {'env_vars': {'NCCL_DEBUG': '***', 'TOKENIZERS_PARALLELISM': '***', 'VLLM_LOGGING_LEVEL': '***'}}
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:13 [plugins/__init__.py:32] No plugins for group vllm.stat_logger_plugins found.
[36m(vLLMHttpServer pid=153926)[0m DEBUG 01-11 13:38:07 [plugins/__init__.py:40] Available plugins for group vllm.general_plugins:[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153926)[0m DEBUG 01-11 13:38:07 [plugins/__init__.py:42] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153926)[0m DEBUG 01-11 13:38:07 [plugins/__init__.py:45] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153927)[0m INFO 01-11 13:38:13 [config/scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.[32m [repeated 8x across cluster][0m
[36m(vLLMHttpServer pid=153926)[0m DEBUG 01-11 13:38:07 [model_executor/models/registry.py:607] Cached model info file for class vllm.model_executor.models.qwen2.Qwen2ForCausalLM is stale[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153926)[0m DEBUG 01-11 13:38:07 [model_executor/models/registry.py:659] Cache model info for class vllm.model_executor.models.qwen2.Qwen2ForCausalLM miss. Loading model instead.[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153925)[0m DEBUG 01-11 13:38:13 [plugins/io_processors/__init__.py:33] No IOProcessor plugins requested by the model
[36m(vLLMHttpServer pid=153925)[0m WARNING 01-11 13:38:13 [utils/system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: In a Ray actor and can only be spawned; CUDA is initialized
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:15 [plugins/__init__.py:32] No plugins for group vllm.platform_plugins found.
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:15 [platforms/__init__.py:36] Checking if TPU platform is available.
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:15 [platforms/__init__.py:55] TPU platform is not available because: No module named 'libtpu'
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:15 [platforms/__init__.py:61] Checking if CUDA platform is available.
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:15 [platforms/__init__.py:88] Exception happens when checking CUDA platform: NVML Shared Library Not Found
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:15 [platforms/__init__.py:105] CUDA platform is not available because: NVML Shared Library Not Found
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:15 [platforms/__init__.py:112] Checking if ROCm platform is available.
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:15 [platforms/__init__.py:120] Confirmed ROCm platform is available.
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:15 [platforms/__init__.py:133] Checking if XPU platform is available.
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:15 [platforms/__init__.py:153] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:15 [platforms/__init__.py:160] Checking if CPU platform is available.
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:16 [platforms/__init__.py:225] Automatically detected platform rocm.
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:18 [utils/flashinfer.py:55] FlashInfer unavailable since package was not found
[36m(vLLMHttpServer pid=153930)[0m DEBUG 01-11 13:38:13 [model_executor/models/registry.py:669] Loaded model info for class vllm.model_executor.models.qwen2.Qwen2ForCausalLM[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m DEBUG 01-11 13:38:13 [logging_utils/log_time.py:29] Registry inspect model class: Elapsed time 6.7749101 secs[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m INFO 01-11 13:38:13 [config/model.py:631] Resolved architecture: Qwen2ForCausalLM[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m INFO 01-11 13:38:13 [config/model.py:1745] Using max model len 32768[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m INFO 01-11 13:38:13 [engine/arg_utils.py:1443] Using ray runtime env (env vars redacted): {'env_vars': {'NCCL_DEBUG': '***', 'TOKENIZERS_PARALLELISM': '***', 'VLLM_LOGGING_LEVEL': '***'}}[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m DEBUG 01-11 13:38:13 [plugins/__init__.py:32] No plugins for group vllm.stat_logger_plugins found.[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m INFO 01-11 13:38:13 [config/scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m DEBUG 01-11 13:38:13 [plugins/io_processors/__init__.py:33] No IOProcessor plugins requested by the model[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m WARNING 01-11 13:38:13 [utils/system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: In a Ray actor and can only be spawned; CUDA is initialized[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153927)[0m [1;36m(EngineCore_DP0 pid=154827)[0;0m DEBUG 01-11 13:38:20 [v1/engine/core.py:780] Waiting for init message from front-end.
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:20 [v1/engine/utils.py:1067] HELLO from local core engine process 0.
[36m(vLLMHttpServer pid=153927)[0m [1;36m(EngineCore_DP0 pid=154827)[0;0m DEBUG 01-11 13:38:20 [v1/engine/core.py:791] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/3220b990-b606-4836-8639-482b40348f28'], outputs=['ipc:///tmp/cd1a16fd-2478-49cd-b5d8-a80f122b3ac8'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 32775, '_data_parallel_master_port_list': [], 'data_parallel_size': 1}, parallel_config_hash=None)
[36m(vLLMHttpServer pid=153927)[0m [1;36m(EngineCore_DP0 pid=154827)[0;0m DEBUG 01-11 13:38:20 [v1/engine/core.py:593] Has DP Coordinator: False, stats publish address: None
[36m(vLLMHttpServer pid=153927)[0m [1;36m(EngineCore_DP0 pid=154827)[0;0m DEBUG 01-11 13:38:20 [plugins/__init__.py:40] Available plugins for group vllm.general_plugins:
[36m(vLLMHttpServer pid=153927)[0m [1;36m(EngineCore_DP0 pid=154827)[0;0m DEBUG 01-11 13:38:20 [plugins/__init__.py:42] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
[36m(vLLMHttpServer pid=153927)[0m [1;36m(EngineCore_DP0 pid=154827)[0;0m DEBUG 01-11 13:38:20 [plugins/__init__.py:45] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
[36m(vLLMHttpServer pid=153927)[0m [1;36m(EngineCore_DP0 pid=154827)[0;0m INFO 01-11 13:38:20 [v1/engine/core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', speculative_config=None, tokenizer='/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=dummy, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[36m(vLLMHttpServer pid=153930)[0m DEBUG 01-11 13:38:16 [plugins/__init__.py:32] No plugins for group vllm.platform_plugins found.[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m DEBUG 01-11 13:38:16 [platforms/__init__.py:36] Checking if TPU platform is available.[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m DEBUG 01-11 13:38:16 [platforms/__init__.py:55] TPU platform is not available because: No module named 'libtpu'[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m DEBUG 01-11 13:38:16 [platforms/__init__.py:61] Checking if CUDA platform is available.[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m DEBUG 01-11 13:38:16 [platforms/__init__.py:88] Exception happens when checking CUDA platform: NVML Shared Library Not Found[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m DEBUG 01-11 13:38:16 [platforms/__init__.py:105] CUDA platform is not available because: NVML Shared Library Not Found[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m DEBUG 01-11 13:38:16 [platforms/__init__.py:112] Checking if ROCm platform is available.[32m [repeated 15x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m DEBUG 01-11 13:38:16 [platforms/__init__.py:120] Confirmed ROCm platform is available.[32m [repeated 15x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m DEBUG 01-11 13:38:16 [platforms/__init__.py:133] Checking if XPU platform is available.[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m DEBUG 01-11 13:38:16 [platforms/__init__.py:153] XPU platform is not available because: No module named 'intel_extension_for_pytorch'[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153930)[0m DEBUG 01-11 13:38:16 [platforms/__init__.py:160] Checking if CPU platform is available.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152210)[0m DEBUG 01-11 13:38:20 [compilation/decorators.py:184] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[36m(WorkerDict pid=152210)[0m DEBUG 01-11 13:38:20 [compilation/decorators.py:184] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[36m(WorkerDict pid=152210)[0m DEBUG 01-11 13:38:20 [compilation/decorators.py:184] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states', 'input_embeds']
[36m(WorkerDict pid=152210)[0m WARNING 01-11 13:38:21 [v1/worker/worker_base.py:301] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.
[36m(WorkerDict pid=152210)[0m DEBUG 01-11 13:38:21 [distributed/parallel_state.py:1169] world_size=1 rank=5 local_rank=5 distributed_init_method=env:// backend=nccl
[36m(vLLMHttpServer pid=153930)[0m DEBUG 01-11 13:38:16 [platforms/__init__.py:225] Automatically detected platform rocm.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152208)[0m [Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:38:21 [distributed/parallel_state.py:1252] Detected 1 nodes in the distributed environment
[36m(WorkerDict pid=152208)[0m INFO 01-11 13:38:21 [distributed/parallel_state.py:1394] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[36m(WorkerDict pid=152212)[0m DEBUG 01-11 13:38:22 [v1/sample/logits_processor/__init__.py:63] No logitsprocs plugins installed (group vllm.logits_processors).
[36m(WorkerDict pid=152205)[0m INFO 01-11 13:38:22 [v1/worker/gpu_model_runner.py:3259] Starting to load model /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct...
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:22 [device_allocator/cumem.py:178] Allocated 467664896 bytes for weights with address 140364228853760 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:22 [device_allocator/cumem.py:178] Allocated 20971520 bytes for weights with address 140732834775040 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:22 [device_allocator/cumem.py:178] Allocated 2097152 bytes for weights with address 140733612818432 from cumem allocator
[36m(WorkerDict pid=152205)[0m INFO 01-11 13:38:22 [platforms/rocm.py:274] Using Triton Attention backend.
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:22 [compilation/backends.py:55] Using InductorStandaloneAdaptor
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:22 [config/compilation.py:882] enabled custom ops: Counter()
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:22 [config/compilation.py:883] disabled custom ops: Counter({'rms_norm': 57, 'column_parallel_linear': 56, 'row_parallel_linear': 56, 'silu_and_mul': 28, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'logits_processor': 1})
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:22 [model_executor/model_loader/base_loader.py:53] Loading weights on cuda ...
[36m(WorkerDict pid=152205)[0m INFO 01-11 13:38:23 [v1/worker/gpu_model_runner.py:3338] Model loading took 3.0605 GiB memory and 0.285702 seconds
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:24 [compilation/decorators.py:409] Start compiling function <code object forward at 0x7fab88aea430, file "/workspace/adhoc/vllm/vllm/model_executor/models/qwen2.py", line 361>
[36m(vLLMHttpServer pid=153930)[0m DEBUG 01-11 13:38:19 [utils/flashinfer.py:55] FlashInfer unavailable since package was not found[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] Traced files (to be considered for compilation cache):
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/model_executor/layers/layernorm.py
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/attention/layer.py
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/distributed/communication_op.py
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/model_executor/layers/utils.py
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/model_executor/layers/rotary_embedding/base.py
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/model_executor/layers/rotary_embedding/common.py
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/model_executor/custom_op.py
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/model_executor/layers/activation.py
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/distributed/parallel_state.py
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/model_executor/layers/linear.py
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/model_executor/models/qwen2.py
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] /usr/local/lib/python3.12/dist-packages/torch/_dynamo/polyfills/builtins.py
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] /usr/local/lib/python3.12/dist-packages/torch/_dynamo/polyfills/itertools.py
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] /usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/platforms/interface.py
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:26 [compilation/caching.py:167] /usr/local/lib/python3.12/dist-packages/torch/_dynamo/polyfills/__init__.py
[36m(vLLMHttpServer pid=153932)[0m [1;36m(EngineCore_DP0 pid=154861)[0;0m DEBUG 01-11 13:38:21 [v1/engine/core.py:780] Waiting for init message from front-end.[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153932)[0m DEBUG 01-11 13:38:21 [v1/engine/utils.py:1067] HELLO from local core engine process 0.[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153932)[0m [1;36m(EngineCore_DP0 pid=154861)[0;0m DEBUG 01-11 13:38:21 [v1/engine/core.py:791] Received init message: EngineHandshakeMetadata(addresses=EngineZmqAddresses(inputs=['ipc:///tmp/5e69f8e0-4554-4321-929b-3fbb77d174da'], outputs=['ipc:///tmp/d3d4b2ff-fca4-459f-a98c-c60c90087a5f'], coordinator_input=None, coordinator_output=None, frontend_stats_publish_address=None), parallel_config={'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 45609, '_data_parallel_master_port_list': [], 'data_parallel_size': 1}, parallel_config_hash=None)[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153932)[0m [1;36m(EngineCore_DP0 pid=154861)[0;0m DEBUG 01-11 13:38:21 [v1/engine/core.py:593] Has DP Coordinator: False, stats publish address: None[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152207)[0m DEBUG 01-11 13:38:21 [plugins/__init__.py:40] Available plugins for group vllm.general_plugins:[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=152207)[0m DEBUG 01-11 13:38:21 [plugins/__init__.py:42] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=152207)[0m DEBUG 01-11 13:38:21 [plugins/__init__.py:45] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.[32m [repeated 15x across cluster][0m
[36m(vLLMHttpServer pid=153932)[0m [1;36m(EngineCore_DP0 pid=154861)[0;0m INFO 01-11 13:38:21 [v1/engine/core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', speculative_config=None, tokenizer='/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=dummy, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152207)[0m DEBUG 01-11 13:38:21 [compilation/decorators.py:184] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.deepseek_v2.DeepseekV2Model'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds'][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152207)[0m DEBUG 01-11 13:38:21 [compilation/decorators.py:184] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama.LlamaModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds'][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152207)[0m DEBUG 01-11 13:38:21 [compilation/decorators.py:184] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.llama_eagle3.LlamaModel'>: ['input_ids', 'positions', 'hidden_states', 'input_embeds'][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152207)[0m WARNING 01-11 13:38:21 [v1/worker/worker_base.py:301] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152207)[0m DEBUG 01-11 13:38:21 [distributed/parallel_state.py:1169] world_size=1 rank=2 local_rank=2 distributed_init_method=env:// backend=nccl[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152207)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0[32m [repeated 47x across cluster][0m
[36m(WorkerDict pid=152207)[0m DEBUG 01-11 13:38:21 [distributed/parallel_state.py:1252] Detected 1 nodes in the distributed environment[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152207)[0m INFO 01-11 13:38:21 [distributed/parallel_state.py:1394] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152205)[0m INFO 01-11 13:38:27 [compilation/backends.py:631] Using cache directory: /apps/mingjiel/.cache/vllm/torch_compile_cache/1532008d72/rank_0_0/backbone for vLLM's torch.compile
[36m(WorkerDict pid=152205)[0m INFO 01-11 13:38:27 [compilation/backends.py:647] Dynamo bytecode transform time: 3.05 s
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:22 [v1/sample/logits_processor/__init__.py:63] No logitsprocs plugins installed (group vllm.logits_processors).[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:27 [compilation/backends.py:165] Directly load the 0-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_0', '/apps/mingjiel/.cache/vllm/torch_compile_cache/1532008d72/rank_0_0/backbone/artifact_shape_None_subgraph_0')
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:23 [device_allocator/cumem.py:178] Allocated 29360128 bytes for weights with address 140363056545792 from cumem allocator[32m [repeated 605x across cluster][0m
[36m(WorkerDict pid=152209)[0m INFO 01-11 13:38:23 [platforms/rocm.py:274] Using Triton Attention backend.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:23 [compilation/backends.py:55] Using InductorStandaloneAdaptor[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:23 [config/compilation.py:882] enabled custom ops: Counter()[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:23 [config/compilation.py:883] disabled custom ops: Counter({'rms_norm': 57, 'column_parallel_linear': 56, 'row_parallel_linear': 56, 'silu_and_mul': 28, 'vocab_parallel_embedding': 1, 'rotary_embedding': 1, 'logits_processor': 1})[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:23 [model_executor/model_loader/base_loader.py:53] Loading weights on cuda ...[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:24 [compilation/decorators.py:409] Start compiling function <code object forward at 0x7fab9ed88f20, file "/workspace/adhoc/vllm/vllm/model_executor/models/qwen2.py", line 361>[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152205)[0m INFO 01-11 13:38:29 [compilation/backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.077 s
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:30 [v1/engine/utils.py:954] Waiting for 1 local, 0 remote core engine proc(s) to start.
[36m(WorkerDict pid=152205)[0m INFO 01-11 13:38:32 [compilation/monitor.py:34] torch.compile takes 5.13 s in total
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:27 [compilation/caching.py:167] Traced files (to be considered for compilation cache):[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:27 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/model_executor/layers/layernorm.py[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:27 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/attention/layer.py[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:27 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:27 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/distributed/communication_op.py[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:27 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/model_executor/layers/utils.py[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:27 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/model_executor/layers/rotary_embedding/base.py[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:27 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/model_executor/layers/rotary_embedding/common.py[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:27 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/model_executor/custom_op.py[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:27 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/model_executor/layers/activation.py[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:27 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/distributed/parallel_state.py[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:27 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/model_executor/layers/linear.py[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:27 [compilation/caching.py:167] /usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py[32m [repeated 35x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:27 [compilation/caching.py:167] /workspace/adhoc/vllm/vllm/platforms/interface.py[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [v1/worker/gpu_worker.py:346] Initial free memory: 189.52 GiB; Requested memory: 0.80 (util), 153.59 GiB
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [v1/worker/gpu_worker.py:352] Free memory after profiling: 186.05 GiB (total), 150.12 GiB (within requested)
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [v1/worker/gpu_worker.py:358] Memory profiling takes 9.37 seconds. Total non KV cache memory: 9.00GiB; torch peak memory increase: 5.61GiB; non-torch forward increase memory: 0.33GiB; weights memory: 3.06GiB.
[36m(WorkerDict pid=152205)[0m INFO 01-11 13:38:33 [v1/worker/gpu_worker.py:359] Available KV cache memory: 144.59 GiB
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:30 [compilation/backends.py:165] Directly load the 28-th graph for dynamic shape from inductor_standalone via handle ('artifact_shape_None_subgraph_28', '/apps/mingjiel/.cache/vllm/torch_compile_cache/1532008d72/rank_4_0/backbone/artifact_shape_None_subgraph_28')[32m [repeated 231x across cluster][0m[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
[36m(vLLMHttpServer pid=153926)[0m INFO:2026-01-11 13:38:07,036:vLLMHttpServer, replica_rank: 3, master address: 10.235.192.105, master port: 40437, data parallel master port: 45927[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153926)[0m INFO:2026-01-11 13:38:07,040:override_generation_config: {'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'repetition_penalty': 1.0, 'max_new_tokens': 2048}[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153926)[0m INFO:2026-01-11 13:38:07,040:enable_sleep_mode: True[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153926)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153926)[0m INFO:2026-01-11 13:38:07,063:replica_rank=3, node_rank=0, nnodes=1, get worker zmq addresses: ['ipc:///tmp/verl_vllm_zmq_152208_root.ipc'][32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153926)[0m /workspace/agent-lightning-spider/verl/verl/utils/tokenizer.py:107: UserWarning: Failed to create processor: Unsupported processor type: Qwen2TokenizerFast. This may affect multimodal processing[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153926)[0m   warnings.warn(f"Failed to create processor: {e}. This may affect multimodal processing", stacklevel=1)[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:01, 30.84it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:01, 32.94it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:00<00:01, 33.49it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:00<00:01, 33.70it/s]

[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140356848975872 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140349957734400 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140344410767360 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140338863800320 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140333316833280 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140327769866240 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140322222899200 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140316675932160 from cumem allocator
[36m(vLLMHttpServer pid=153931)[0m [1;36m(EngineCore_DP0 pid=154835)[0;0m INFO 01-11 13:38:33 [v1/core/kv_cache_utils.py:1229] GPU KV cache size: 5,414,688 tokens
[36m(vLLMHttpServer pid=153931)[0m [1;36m(EngineCore_DP0 pid=154835)[0;0m INFO 01-11 13:38:33 [v1/core/kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 165.24x
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140311128965120 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140305581998080 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140300035031040 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140294488064000 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140288941096960 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140283394129920 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140277847162880 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140272300195840 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140266753228800 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140261206261760 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140255659294720 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140250112327680 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140244565360640 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140239018393600 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140233471426560 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140227924459520 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140222377492480 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140216830525440 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140211283558400 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:33 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140205736591360 from cumem allocator
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:34 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=512, uniform_decode=False, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:34 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=496, uniform_decode=False, has_lora=False))
[36m(WorkerDict pid=152209)[0m INFO 01-11 13:38:30 [compilation/backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.095 s[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:34 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=480, uniform_decode=False, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:34 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=464, uniform_decode=False, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:34 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=448, uniform_decode=False, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:34 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=432, uniform_decode=False, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:34 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=416, uniform_decode=False, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:34 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=400, uniform_decode=False, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:34 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=384, uniform_decode=False, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:34 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=368, uniform_decode=False, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:34 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=352, uniform_decode=False, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:34 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=336, uniform_decode=False, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:34 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=320, uniform_decode=False, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:34 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=304, uniform_decode=False, has_lora=False))
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:00<00:00, 33.43it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:00<00:00, 33.26it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:00<00:00, 32.96it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:00<00:00, 32.80it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:01<00:00, 32.73it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:01<00:00, 32.39it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:01<00:00, 32.30it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:01<00:00, 32.28it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:01<00:00, 32.62it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:16,  3.04it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:06,  7.35it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:04,  9.88it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:03, 11.47it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:00<00:03, 12.50it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:01<00:03, 13.14it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:01<00:02, 13.55it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:01<00:02, 13.83it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:01<00:02, 14.01it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:01<00:03, 10.46it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:01<00:02, 11.37it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:02<00:02, 12.16it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:02<00:02, 12.78it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:02<00:01, 13.25it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:02<00:01, 13.62it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:02<00:01, 13.84it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:02<00:01, 14.04it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:02<00:01, 14.18it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:02<00:00, 14.26it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:03<00:00, 14.32it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:03<00:00, 14.35it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:03<00:00, 14.36it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:04<00:00,  6.84it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:04<00:00,  7.89it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:04<00:00,  8.97it/s]
[36m(WorkerDict pid=152205)[0m Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:04<00:00,  7.39it/s]Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:04<00:00, 10.77it/s]
[36m(vLLMHttpServer pid=153931)[0m INFO:2026-01-11 13:38:43,086:Initializing a V1 LLM engine with config: model='/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', speculative_config=None, tokenizer='/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=dummy, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:34 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=288, uniform_decode=False, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:34 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=272, uniform_decode=False, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:34 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=256, uniform_decode=False, has_lora=False))
[36m(vLLMHttpServer pid=153932)[0m DEBUG 01-11 13:38:31 [v1/engine/utils.py:954] Waiting for 1 local, 0 remote core engine proc(s) to start.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:36 [compilation/cuda_graph.py:142] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=512, uniform_decode=True, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:36 [compilation/cuda_graph.py:142] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=496, uniform_decode=True, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:36 [compilation/cuda_graph.py:142] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=480, uniform_decode=True, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:36 [compilation/cuda_graph.py:142] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=464, uniform_decode=True, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:36 [compilation/cuda_graph.py:142] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=448, uniform_decode=True, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:36 [compilation/cuda_graph.py:142] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=432, uniform_decode=True, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:36 [compilation/cuda_graph.py:142] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=416, uniform_decode=True, has_lora=False))
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:36 [compilation/cuda_graph.py:142] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=400, uniform_decode=True, has_lora=False))
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:38:34 [v1/worker/gpu_worker.py:346] Initial free memory: 189.33 GiB; Requested memory: 0.80 (util), 153.59 GiB[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:38:34 [v1/worker/gpu_worker.py:352] Free memory after profiling: 185.86 GiB (total), 150.12 GiB (within requested)[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:38:34 [v1/worker/gpu_worker.py:358] Memory profiling takes 9.91 seconds. Total non KV cache memory: 9.00GiB; torch peak memory increase: 5.61GiB; non-torch forward increase memory: 0.33GiB; weights memory: 3.06GiB.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:38:35 [device_allocator/cumem.py:178] Allocated 5544869888 bytes for kv_cache with address 140205401047040 from cumem allocator[32m [repeated 196x across cluster][0m
[36m(vLLMHttpServer pid=153926)[0m [1;36m(EngineCore_DP0 pid=154842)[0;0m INFO 01-11 13:38:35 [v1/core/kv_cache_utils.py:1229] GPU KV cache size: 5,414,688 tokens[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153926)[0m [1;36m(EngineCore_DP0 pid=154842)[0;0m INFO 01-11 13:38:35 [v1/core/kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 165.24x[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:37 [compilation/cuda_graph.py:142] Capturing a cudagraph on (PIECEWISE,BatchDescriptor(num_tokens=1, uniform_decode=False, has_lora=False))[32m [repeated 391x across cluster][0m
[36m(vLLMHttpServer pid=153927)[0m DEBUG 01-11 13:38:40 [v1/engine/utils.py:954] Waiting for 1 local, 0 remote core engine proc(s) to start.
[36m(vLLMHttpServer pid=153926)[0m DEBUG 01-11 13:38:40 [v1/engine/utils.py:954] Waiting for 1 local, 0 remote core engine proc(s) to start.
[36m(WorkerDict pid=152205)[0m INFO 01-11 13:38:41 [v1/worker/gpu_model_runner.py:4244] Graph capturing finished in 7 secs, took 0.48 GiB
[36m(WorkerDict pid=152205)[0m DEBUG 01-11 13:38:41 [v1/worker/gpu_worker.py:486] Free memory on device (189.52/191.98 GiB) on startup. Desired GPU memory utilization is (0.8, 153.59 GiB). Actual usage is 3.06 GiB for weight, 5.61 GiB for peak activation, 0.33 GiB for non-torch memory, and 0.48 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=154575628185` (143.96 GiB) to fit into requested memory, or `--kv-cache-memory=193157352960` (179.89 GiB) to fully utilize gpu memory. Current kv cache memory in use is 144.59 GiB.
[36m(vLLMHttpServer pid=153931)[0m [1;36m(EngineCore_DP0 pid=154835)[0;0m INFO 01-11 13:38:41 [v1/engine/core.py:250] init engine (profile, create kv cache, warmup model) took 17.40 seconds
[36m(WorkerDict pid=152206)[0m DEBUG 01-11 13:38:41 [compilation/cuda_graph.py:142] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True, has_lora=False))[32m [repeated 374x across cluster][0m
[36m(vLLMHttpServer pid=153931)[0m DEBUG 01-11 13:38:42 [v1/engine/utils.py:1067] READY from local core engine process 0.
[36m(vLLMHttpServer pid=153931)[0m [1;36m(EngineCore_DP0 pid=154835)[0;0m DEBUG 01-11 13:38:42 [utils/gc_utils.py:40] GC Debug Config. enabled:False,top_objects:-1
[36m(vLLMHttpServer pid=153931)[0m [1;36m(EngineCore_DP0 pid=154835)[0;0m DEBUG 01-11 13:38:42 [v1/engine/core.py:875] EngineCore waiting for work.
[36m(vLLMHttpServer pid=153931)[0m [1;36m(EngineCore_DP0 pid=154835)[0;0m DEBUG 01-11 13:38:42 [v1/engine/core.py:875] EngineCore waiting for work.
[36m(vLLMHttpServer pid=153931)[0m INFO 01-11 13:38:43 [entrypoints/openai/api_server.py:1725] Supported tasks: ['generate']
[36m(vLLMHttpServer pid=153931)[0m WARNING 01-11 13:38:43 [config/model.py:1568] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[36m(vLLMHttpServer pid=153931)[0m INFO 01-11 13:38:43 [entrypoints/openai/serving_responses.py:154] Using default chat sampling params from model: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'max_tokens': 2048}
[36m(vLLMHttpServer pid=153931)[0m INFO 01-11 13:38:43 [entrypoints/openai/serving_chat.py:131] Using default chat sampling params from model: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'max_tokens': 2048}
[36m(vLLMHttpServer pid=153931)[0m INFO 01-11 13:38:43 [entrypoints/openai/serving_completion.py:73] Using default completion sampling params from model: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'max_tokens': 2048}
[36m(vLLMHttpServer pid=153931)[0m INFO 01-11 13:38:43 [entrypoints/openai/serving_chat.py:131] Using default chat sampling params from model: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'max_tokens': 2048}
[36m(TaskRunner pid=147334)[0m AgentLoopManager: ['10.235.192.105:39193', '10.235.192.105:40293', '10.235.192.105:40629', '10.235.192.105:37331', '10.235.192.105:39293', '10.235.192.105:42515', '10.235.192.105:40751', '10.235.192.105:39385']
[36m(vLLMHttpServer pid=153929)[0m INFO 01-11 13:38:45 [v1/engine/async_llm.py:729] Engines are idle, requests have been drained
[36m(vLLMHttpServer pid=153929)[0m [1;36m(EngineCore_DP0 pid=154865)[0;0m INFO 01-11 13:38:45 [v1/core/block_pool.py:390] Successfully reset prefix cache
[36m(WorkerDict pid=152208)[0m INFO 01-11 13:38:45 [device_allocator/cumem.py:239] CuMemAllocator: sleep freed 147.62 GiB memory in total, of which 0.00 GiB is backed up in CPU and the rest 147.62 GiB is discarded directly.
[36m(WorkerDict pid=152206)[0m INFO 01-11 13:38:45 [v1/worker/gpu_worker.py:142] Sleep mode freed 153.21 GiB memory, 3.59 GiB memory is still in use.
[36m(AgentLoopWorker pid=155575)[0m DEBUG 01-11 13:38:50 [plugins/__init__.py:32] No plugins for group vllm.platform_plugins found.[36m(AgentLoopWorker pid=155574)[0m /workspace/agent-lightning-spider/verl/verl/utils/tokenizer.py:107: UserWarning: Failed to create processor: Unsupported processor type: Qwen2TokenizerFast. This may affect multimodal processing
[36m(AgentLoopWorker pid=155574)[0m   warnings.warn(f"Failed to create processor: {e}. This may affect multimodal processing", stacklevel=1)
[36m(TaskRunner pid=147334)[0m wandb: Tracking run with wandb version 0.22.3
[36m(TaskRunner pid=147334)[0m wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[36m(TaskRunner pid=147334)[0m wandb: Run data is saved locally in /workspace/agent-lightning-spider/agent-lightning/examples/spider_async/wandb/offline-run-20260111_133853-heihfqgj
[36m(TaskRunner pid=147334)[0m wandb: Detected [litellm, mcp, openai] in use.
[36m(TaskRunner pid=147334)[0m wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[36m(TaskRunner pid=147334)[0m wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/

[36m(AgentLoopWorker pid=155575)[0m DEBUG 01-11 13:38:50 [platforms/__init__.py:36] Checking if TPU platform is available.
[36m(AgentLoopWorker pid=155575)[0m DEBUG 01-11 13:38:50 [platforms/__init__.py:55] TPU platform is not available because: No module named 'libtpu'
[36m(AgentLoopWorker pid=155575)[0m DEBUG 01-11 13:38:50 [platforms/__init__.py:61] Checking if CUDA platform is available.
[36m(AgentLoopWorker pid=155575)[0m DEBUG 01-11 13:38:50 [platforms/__init__.py:88] Exception happens when checking CUDA platform: NVML Shared Library Not Found
[36m(vLLMHttpServer pid=153932)[0m DEBUG 01-11 13:38:41 [v1/engine/utils.py:954] Waiting for 1 local, 0 remote core engine proc(s) to start.[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:42 [v1/worker/gpu_worker.py:486] Free memory on device (189.35/191.98 GiB) on startup. Desired GPU memory utilization is (0.8, 153.59 GiB). Actual usage is 3.06 GiB for weight, 5.61 GiB for peak activation, 0.33 GiB for non-torch memory, and 0.48 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=154575628185` (143.96 GiB) to fit into requested memory, or `--kv-cache-memory=192970706432` (179.72 GiB) to fully utilize gpu memory. Current kv cache memory in use is 144.59 GiB.[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153926)[0m [1;36m(EngineCore_DP0 pid=154842)[0;0m INFO 01-11 13:38:43 [v1/engine/core.py:250] init engine (profile, create kv cache, warmup model) took 18.64 seconds[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=152209)[0m DEBUG 01-11 13:38:42 [compilation/cuda_graph.py:142] Capturing a cudagraph on (FULL,BatchDescriptor(num_tokens=1, uniform_decode=True, has_lora=False))[32m [repeated 26x across cluster][0m
[36m(vLLMHttpServer pid=153925)[0m DEBUG 01-11 13:38:44 [v1/engine/utils.py:1067] READY from local core engine process 0.[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m DEBUG 01-11 13:38:44 [utils/gc_utils.py:40] GC Debug Config. enabled:False,top_objects:-1[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153928)[0m [1;36m(EngineCore_DP0 pid=154831)[0;0m DEBUG 01-11 13:38:45 [v1/engine/core.py:875] EngineCore waiting for work.[32m [repeated 30x across cluster][0m
[36m(vLLMHttpServer pid=153925)[0m INFO 01-11 13:38:44 [entrypoints/openai/api_server.py:1725] Supported tasks: ['generate'][32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153925)[0m WARNING 01-11 13:38:44 [config/model.py:1568] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153925)[0m INFO 01-11 13:38:44 [entrypoints/openai/serving_chat.py:131] Using default chat sampling params from model: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'max_tokens': 2048}[32m [repeated 21x across cluster][0m
[36m(vLLMHttpServer pid=153925)[0m INFO 01-11 13:38:44 [entrypoints/openai/serving_completion.py:73] Using default completion sampling params from model: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'max_tokens': 2048}[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153928)[0m INFO 01-11 13:38:45 [v1/engine/async_llm.py:729] Engines are idle, requests have been drained[32m [repeated 7x across cluster][0m
[36m(vLLMHttpServer pid=153928)[0m [1;36m(EngineCore_DP0 pid=154831)[0;0m INFO 01-11 13:38:45 [v1/core/block_pool.py:390] Successfully reset prefix cache[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=155573)[0m DEBUG 01-11 13:38:50 [platforms/__init__.py:105] CUDA platform is not available because: NVML Shared Library Not Found
[36m(AgentLoopWorker pid=155573)[0m DEBUG 01-11 13:38:50 [platforms/__init__.py:112] Checking if ROCm platform is available.
[36m(WorkerDict pid=152207)[0m INFO 01-11 13:38:45 [device_allocator/cumem.py:239] CuMemAllocator: sleep freed 147.62 GiB memory in total, of which 0.00 GiB is backed up in CPU and the rest 147.62 GiB is discarded directly.[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=155575)[0m DEBUG 01-11 13:38:50 [platforms/__init__.py:120] Confirmed ROCm platform is available.
[36m(AgentLoopWorker pid=155575)[0m DEBUG 01-11 13:38:50 [platforms/__init__.py:133] Checking if XPU platform is available.
[36m(AgentLoopWorker pid=155575)[0m DEBUG 01-11 13:38:50 [platforms/__init__.py:153] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
[36m(AgentLoopWorker pid=155575)[0m DEBUG 01-11 13:38:50 [platforms/__init__.py:160] Checking if CPU platform is available.
[36m(WorkerDict pid=152207)[0m INFO 01-11 13:38:45 [v1/worker/gpu_worker.py:142] Sleep mode freed 153.21 GiB memory, 3.59 GiB memory is still in use.[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=155575)[0m DEBUG 01-11 13:38:50 [platforms/__init__.py:225] Automatically detected platform rocm.
[36m(AgentLoopWorker pid=155575)[0m DEBUG 01-11 13:38:53 [utils/flashinfer.py:55] FlashInfer unavailable since package was not found
[36m(AgentLoopWorker pid=155575)[0m Using dataset class: RLHFDataset
[36m(TaskRunner pid=147334)[0m Checkpoint tracker file does not exist: /workspace/agent-lightning-spider/checkpoint/latest_checkpointed_iteration.txt
[36m(TaskRunner pid=147334)[0m Training from scratch
[36m(TaskRunner pid=147334)[0m Use Custom Scheduler: False
[36m(TaskRunner pid=147334)[0m LiteLLM config: num_retries=3, allowed_fails=99999999
[36m(TaskRunner pid=147334)[0m [llm_proxy.py] litellm_config: {'general_settings': {'forward_client_headers_to_llm_api': True}, 'litellm_settings': {'num_retries': 3}, 'router_settings': {'allowed_fails': 99999999}}
[36m(AgentLoopWorker pid=155581)[0m DEBUG 01-11 13:38:50 [plugins/__init__.py:32] No plugins for group vllm.platform_plugins found.[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=155581)[0m DEBUG 01-11 13:38:50 [platforms/__init__.py:36] Checking if TPU platform is available.[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=155581)[0m DEBUG 01-11 13:38:50 [platforms/__init__.py:55] TPU platform is not available because: No module named 'libtpu'[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=155581)[0m DEBUG 01-11 13:38:50 [platforms/__init__.py:61] Checking if CUDA platform is available.[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=155581)[0m DEBUG 01-11 13:38:50 [platforms/__init__.py:88] Exception happens when checking CUDA platform: NVML Shared Library Not Found[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=155581)[0m DEBUG 01-11 13:38:50 [platforms/__init__.py:105] CUDA platform is not available because: NVML Shared Library Not Found[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=155581)[0m DEBUG 01-11 13:38:51 [platforms/__init__.py:112] Checking if ROCm platform is available.[32m [repeated 15x across cluster][0m
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:38:56 [compilation/decorators.py:184] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.mixtral.MixtralModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:38:56 [compilation/decorators.py:184] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.qwen2_moe.Qwen2MoeModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[36m(WorkerDict pid=152208)[0m DEBUG 01-11 13:38:56 [compilation/decorators.py:184] Inferred dynamic dimensions for forward method of <class 'vllm.model_executor.models.qwen3_moe.Qwen3MoeModel'>: ['input_ids', 'positions', 'intermediate_tensors', 'inputs_embeds']
[36m(AgentLoopWorker pid=155581)[0m DEBUG 01-11 13:38:51 [platforms/__init__.py:120] Confirmed ROCm platform is available.[32m [repeated 15x across cluster][0m[36m(TaskRunner pid=147334)[0m 2026-01-11 13:38:57,761 [INFO] (Process-147334 agentlightning.llm_proxy)   Updating LLMProxy model list to: [{'model_name': '/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', 'litellm_params': {'model': 'hosted_vllm//apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', 'api_base': 'http://10.235.192.105:39193/v1/'}}, {'model_name': '/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', 'litellm_params': {'model': 'hosted_vllm//apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', 'api_base': 'http://10.235.192.105:40293/v1/'}}, {'model_name': '/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', 'litellm_params': {'model': 'hosted_vllm//apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', 'api_base': 'http://10.235.192.105:40629/v1/'}}, {'model_name': '/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', 'litellm_params': {'model': 'hosted_vllm//apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', 'api_base': 'http://10.235.192.105:37331/v1/'}}, {'model_name': '/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', 'litellm_params': {'model': 'hosted_vllm//apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', 'api_base': 'http://10.235.192.105:39293/v1/'}}, {'model_name': '/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', 'litellm_params': {'model': 'hosted_vllm//apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', 'api_base': 'http://10.235.192.105:42515/v1/'}}, {'model_name': '/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', 'litellm_params': {'model': 'hosted_vllm//apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', 'api_base': 'http://10.235.192.105:40751/v1/'}}, {'model_name': '/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', 'litellm_params': {'model': 'hosted_vllm//apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct', 'api_base': 'http://10.235.192.105:39385/v1/'}}]
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:38:57,761 [INFO] (Process-147334 agentlightning.llm_proxy)   Restarting LLMProxy server...
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:38:57,762 [INFO] (Process-147334 agentlightning.llm_proxy)   Starting LLMProxy server in mp mode with store capabilities: {'thread_safe': True, 'async_safe': True, 'zero_copy': True}
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:38:57,762 [INFO] (Process-147334 agentlightning.utils.server_launcher)   Starting server fastapi.applications:app...
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:38:57,762 [WARNING] (Process-147334 agentlightning.utils.server_launcher)   No host provided, using 0.0.0.0.
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:38:57,763 [INFO] (Process-147334 agentlightning.utils.server_launcher)   Starting Gunicorn server...
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:38:57,765 [WARNING] (Process-147334 agentlightning.utils.server_launcher)   No access host provided, using default outbound IPv4 address for this machine.
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:38:57,799 [INFO] (Process-157289 agentlightning.llm_proxy)   Adding a new middleware to the FastAPI app.
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:38:57,828 [INFO] (Process-157289 agentlightning.llm_proxy)   LLMProxy preparation is done. Will start the server.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:57 +0000] [157289] [INFO] Starting gunicorn 23.0.0
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:57 +0000] [157289] [INFO] Listening at: http://0.0.0.0:57051 (157289)
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:57 +0000] [157289] [INFO] Using worker: uvicorn_worker.UvicornWorker
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:57 +0000] [157299] [INFO] Booting worker with pid: 157299
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:57 +0000] [157299] [INFO] Started server process [157299]
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:57 +0000] [157299] [INFO] Waiting for application startup.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:57 +0000] [157299] [INFO] Application startup complete.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:57 +0000] [157309] [INFO] Booting worker with pid: 157309
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:57 +0000] [157309] [INFO] Started server process [157309]
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:57 +0000] [157309] [INFO] Waiting for application startup.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:57 +0000] [157309] [INFO] Application startup complete.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:57 +0000] [157326] [INFO] Booting worker with pid: 157326

[36m(AgentLoopWorker pid=155581)[0m DEBUG 01-11 13:38:51 [platforms/__init__.py:133] Checking if XPU platform is available.[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=155581)[0m DEBUG 01-11 13:38:51 [platforms/__init__.py:153] XPU platform is not available because: No module named 'intel_extension_for_pytorch'[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=155581)[0m DEBUG 01-11 13:38:51 [platforms/__init__.py:160] Checking if CPU platform is available.[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=155581)[0m DEBUG 01-11 13:38:51 [platforms/__init__.py:225] Automatically detected platform rocm.[32m [repeated 7x across cluster][0m
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m    ██╗     ██╗████████╗███████╗██╗     ██╗     ███╗   ███╗
[36m(TaskRunner pid=147334)[0m    ██║     ██║╚══██╔══╝██╔════╝██║     ██║     ████╗ ████║
[36m(TaskRunner pid=147334)[0m    ██║     ██║   ██║   █████╗  ██║     ██║     ██╔████╔██║
[36m(TaskRunner pid=147334)[0m    ██║     ██║   ██║   ██╔══╝  ██║     ██║     ██║╚██╔╝██║
[36m(TaskRunner pid=147334)[0m    ███████╗██║   ██║   ███████╗███████╗███████╗██║ ╚═╝ ██║
[36m(TaskRunner pid=147334)[0m    ╚══════╝╚═╝   ╚═╝   ╚══════╝╚══════╝╚══════╝╚═╝     ╚═╝
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m [1;37m#------------------------------------------------------------#[0m
[36m(TaskRunner pid=147334)[0m [1;37m#                                                            #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#               'A feature I really want is...'               #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#        https://github.com/BerriAI/litellm/issues/new        #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#                                                            #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#------------------------------------------------------------#[0m
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m  Thank you for using LiteLLM! - Krrish & Ishaan
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m [1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m [32mLiteLLM: Proxy initialized with Config, Set models:[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m    ██╗     ██╗████████╗███████╗██╗     ██╗     ███╗   ███╗
[36m(TaskRunner pid=147334)[0m    ██║     ██║╚══██╔══╝██╔════╝██║     ██║     ████╗ ████║
[36m(TaskRunner pid=147334)[0m    ██║     ██║   ██║   █████╗  ██║     ██║     ██╔████╔██║
[36m(TaskRunner pid=147334)[0m    ██║     ██║   ██║   ██╔══╝  ██║     ██║     ██║╚██╔╝██║
[36m(TaskRunner pid=147334)[0m    ███████╗██║   ██║   ███████╗███████╗███████╗██║ ╚═╝ ██║
[36m(TaskRunner pid=147334)[0m    ╚══════╝╚═╝   ╚═╝   ╚══════╝╚══════╝╚══════╝╚═╝     ╚═╝
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m [1;37m#------------------------------------------------------------#[0m
[36m(TaskRunner pid=147334)[0m [1;37m#                                                            #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#              'I don't like how this works...'               #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#        https://github.com/BerriAI/litellm/issues/new        #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#                                                            #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#------------------------------------------------------------#[0m
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m  Thank you for using LiteLLM! - Krrish & Ishaan
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m [1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m [32mLiteLLM: Proxy initialized with Config, Set models:[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m    ██╗     ██╗████████╗███████╗██╗     ██╗     ███╗   ███╗
[36m(TaskRunner pid=147334)[0m    ██║     ██║╚══██╔══╝██╔════╝██║     ██║     ████╗ ████║
[36m(TaskRunner pid=147334)[0m    ██║     ██║   ██║   █████╗  ██║     ██║     ██╔████╔██║
[36m(TaskRunner pid=147334)[0m    ██║     ██║   ██║   ██╔══╝  ██║     ██║     ██║╚██╔╝██║
[36m(TaskRunner pid=147334)[0m    ███████╗██║   ██║   ███████╗███████╗███████╗██║ ╚═╝ ██║
[36m(TaskRunner pid=147334)[0m    ╚══════╝╚═╝   ╚═╝   ╚══════╝╚══════╝╚══════╝╚═╝     ╚═╝
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m [1;37m#------------------------------------------------------------#[0m
[36m(TaskRunner pid=147334)[0m [1;37m#                                                            #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#            'The thing I wish you improved is...'            #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#        https://github.com/BerriAI/litellm/issues/new        #[0m[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:58 +0000] [157326] [INFO] Started server process [157326]
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:58 +0000] [157326] [INFO] Waiting for application startup.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:58 +0000] [157330] [INFO] Booting worker with pid: 157330
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:58 +0000] [157326] [INFO] Application startup complete.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:58 +0000] [157330] [INFO] Started server process [157330]
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:58 +0000] [157330] [INFO] Waiting for application startup.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:58 +0000] [157336] [INFO] Booting worker with pid: 157336
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:58 +0000] [157330] [INFO] Application startup complete.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:58 +0000] [157336] [INFO] Started server process [157336]
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:58 +0000] [157336] [INFO] Waiting for application startup.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:38:58 +0000] [157336] [INFO] Application startup complete.

[36m(TaskRunner pid=147334)[0m [1;37m#                                                            #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#------------------------------------------------------------#[0m
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m  Thank you for using LiteLLM! - Krrish & Ishaan
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m [1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m [32mLiteLLM: Proxy initialized with Config, Set models:[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m    ██╗     ██╗████████╗███████╗██╗     ██╗     ███╗   ███╗
[36m(TaskRunner pid=147334)[0m    ██║     ██║╚══██╔══╝██╔════╝██║     ██║     ████╗ ████║
[36m(TaskRunner pid=147334)[0m    ██║     ██║   ██║   █████╗  ██║     ██║     ██╔████╔██║
[36m(TaskRunner pid=147334)[0m    ██║     ██║   ██║   ██╔══╝  ██║     ██║     ██║╚██╔╝██║
[36m(TaskRunner pid=147334)[0m    ███████╗██║   ██║   ███████╗███████╗███████╗██║ ╚═╝ ██║
[36m(TaskRunner pid=147334)[0m    ╚══════╝╚═╝   ╚═╝   ╚══════╝╚══════╝╚══════╝╚═╝     ╚═╝
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m [1;37m#------------------------------------------------------------#[0m
[36m(TaskRunner pid=147334)[0m [1;37m#                                                            #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#            'The thing I wish you improved is...'            #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#        https://github.com/BerriAI/litellm/issues/new        #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#                                                            #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#------------------------------------------------------------#[0m
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m  Thank you for using LiteLLM! - Krrish & Ishaan
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m [1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m [32mLiteLLM: Proxy initialized with Config, Set models:[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m    ██╗     ██╗████████╗███████╗██╗     ██╗     ███╗   ███╗
[36m(TaskRunner pid=147334)[0m    ██║     ██║╚══██╔══╝██╔════╝██║     ██║     ████╗ ████║
[36m(TaskRunner pid=147334)[0m    ██║     ██║   ██║   █████╗  ██║     ██║     ██╔████╔██║
[36m(TaskRunner pid=147334)[0m    ██║     ██║   ██║   ██╔══╝  ██║     ██║     ██║╚██╔╝██║
[36m(TaskRunner pid=147334)[0m    ███████╗██║   ██║   ███████╗███████╗███████╗██║ ╚═╝ ██║
[36m(TaskRunner pid=147334)[0m    ╚══════╝╚═╝   ╚═╝   ╚══════╝╚══════╝╚══════╝╚═╝     ╚═╝
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m [1;37m#------------------------------------------------------------#[0m
[36m(TaskRunner pid=147334)[0m [1;37m#                                                            #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#           'I get frustrated when the product...'            #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#        https://github.com/BerriAI/litellm/issues/new        #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#                                                            #[0m
[36m(TaskRunner pid=147334)[0m [1;37m#------------------------------------------------------------#[0m
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m  Thank you for using LiteLLM! - Krrish & Ishaan
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m [1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m [32mLiteLLM: Proxy initialized with Config, Set models:[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(TaskRunner pid=147334)[0m [32m    /apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct[0m
[36m(vLLMHttpServer pid=153927)[0m INFO 01-11 13:38:58 [entrypoints/chat_utils.py:557] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[36m(vLLMHttpServer pid=153925)[0m DEBUG 01-11 13:38:58 [v1/sample/logits_processor/__init__.py:63] No logitsprocs plugins installed (group vllm.logits_processors).
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m DEBUG 01-11 13:38:58 [v1/engine/core.py:881] EngineCore loop active.
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m **************************************************************************************************** None SchedulerOutput(scheduled_new_reqs=[NewRequestData(req_id=chatcmpl-59217c09b1e445a99b3dfe7ae1e5b0a7,prompt_token_ids=[151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 18665, 1246, 594, 432, 2087, 30, 151645, 198, 151644, 77091, 198],mm_features=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[151643], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=2048, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, structured_outputs=None, extra_args=None),block_ids=([1, 2, 3],),num_computed_tokens=0,lora_request=None,prompt_embeds_shape=None)], scheduled_cached_reqs=CachedRequestData(req_ids=[], resumed_req_ids=set(), new_token_ids=[], all_token_ids={}, new_block_ids=[], num_computed_tokens=[], num_output_tokens=[]), num_scheduled_tokens={'chatcmpl-59217c09b1e445a99b3dfe7ae1e5b0a7': 35}, total_num_scheduled_tokens=35, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[3], finished_req_ids=set(), free_encoder_mm_hashes=[], pending_structured_output_tokens=False, kv_connector_metadata=None, ec_connector_metadata=None)[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m Process EngineCore_DP0:
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m Traceback (most recent call last):
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m     self.run()
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m     self._target(*self._args, **self._kwargs)
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m   File "/workspace/adhoc/vllm/vllm/v1/engine/core.py", line 847, in run_engine_core
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m     raise e
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m   File "/workspace/adhoc/vllm/vllm/v1/engine/core.py", line 836, in run_engine_core
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m     engine_core.run_busy_loop()
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m   File "/workspace/adhoc/vllm/vllm/v1/engine/core.py", line 863, in run_busy_loop
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m     self._process_engine_step()
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m   File "/workspace/adhoc/vllm/vllm/v1/engine/core.py", line 892, in _process_engine_step
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m     outputs, model_executed = self.step_fn()
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m                               ^^^^^^^^^^^^^^
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m   File "/workspace/adhoc/vllm/vllm/v1/engine/core.py", line 343, in step
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m     model_output = future.result()
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m                    ^^^^^^^^^^^^^
[36m(vLLMHttpServer pid=153925)[0m [1;36m(EngineCore_DP0 pid=154823)[0;0m AttributeError: 'NoneType' object has no attribute 'result'
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:39:01,588 [INFO] (Process-147334 agentlightning.utils.server_launcher)   Subprocess server started successfully.
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:39:01,589 [INFO] (Process-147334 agentlightning.utils.server_launcher)   Server fastapi.applications:app started at http://0.0.0.0:57051
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:39:01,589 [WARNING] (Process-147334 agentlightning.utils.server_launcher)   No access host provided, using default outbound IPv4 address for this machine.
[36m(AgentLoopWorker pid=155576)[0m /workspace/agent-lightning-spider/verl/verl/utils/tokenizer.py:107: UserWarning: Failed to create processor: Unsupported processor type: Qwen2TokenizerFast. This may affect multimodal processing[32m [repeated 7x across cluster][0m
[36m(AgentLoopWorker pid=155576)[0m   warnings.warn(f"Failed to create processor: {e}. This may affect multimodal processing", stacklevel=1)[32m [repeated 7x across cluster][0m
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:39:01,787 [WARNING] (Process-157326 agentlightning.llm_proxy)   Loop and lock are not owned by the current process. Clearing them.
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:39:01,787 [WARNING] (Process-157336 agentlightning.llm_proxy)   Loop and lock are not owned by the current process. Clearing them.
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:39:01,820 [WARNING] (Process-157309 agentlightning.llm_proxy)   Loop and lock are not owned by the current process. Clearing them.
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:39:01,848 [WARNING] (Process-157299 agentlightning.llm_proxy)   Loop and lock are not owned by the current process. Clearing them.
[36m(TaskRunner pid=147334)[0m [92m13:39:01 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:39:01,893:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:01,896:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:01,896:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:39:01,909 [WARNING] (Process-157330 agentlightning.llm_proxy)   Loop and lock are not owned by the current process. Clearing them.
[36m(TaskRunner pid=147334)[0m [92m13:39:01 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:39:01,954:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:01,956:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:01,957:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:39:01 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:39:01,956:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:01,959:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:01,959:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:39:01 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:39:01,990:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:01,992:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:01,992:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:39:01 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:39:01,993:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:01,995:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:01,995:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:39:02 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:39:02,005:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:02,007:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:02,007:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:39:02 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:39:02,005:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:02,008:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:02,008:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:39:02 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:39:02,021:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:02,024:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:02,024:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:39:02 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:39:02,024:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:02,026:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:02,026:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:39:02 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:39:02,036:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:02,038:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:39:02,038:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:39:02 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:39:02,036:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,673:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,676:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,677:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,681:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,684:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,684:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,710:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,712:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,712:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,713:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,715:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,715:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,714:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,716:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,716:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,715:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,718:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,718:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,717:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,720:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,721:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,731:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,734:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,734:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,741:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,744:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,744:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,757:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,759:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,759:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,762:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,764:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,764:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,768:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,772:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,772:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,773:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,775:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,775:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,775:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,778:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,778:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,781:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,783:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,781:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,784:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,784:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,784:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,794:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,797:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,797:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,799:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,802:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,802:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,803:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,806:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,806:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,806:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,809:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,809:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,814:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,816:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,817:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,819:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,821:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,821:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,828:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,831:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,831:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,832:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,834:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,835:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,841:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,844:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,844:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,845:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,847:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,847:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,860:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,863:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,864:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,864:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,866:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,867:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,873:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,875:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,875:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,886:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,889:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,889:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,889:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,892:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,892:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,893:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,896:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,896:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,897:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,900:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,900:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,906:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,909:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,910:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,918:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,921:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,921:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,936:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,939:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,939:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,939:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,941:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,942:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,962:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,964:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,965:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,968:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,970:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,970:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,970:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,973:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,973:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:29 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:29,994:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,997:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:29,997:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,002:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,005:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,005:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,014:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,017:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,017:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,014:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,018:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,018:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,019:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,021:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,021:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,029:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,032:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,032:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,037:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,040:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,040:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,040:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,043:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,043:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,043:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,045:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,045:Calling end() on an ended span.

[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-a440e87fed13 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-7cb3088cdd11, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-7cb3088cdd11 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-6851d588340e, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-6851d588340e contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-d516d46330c6, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-d516d46330c6 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-6c21c51b64a2, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-6c21c51b64a2 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-dd3cc109b17d, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-dd3cc109b17d contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-3caf970674e8, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-3caf970674e8 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-b7dfd20797db, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-b7dfd20797db contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-397f2bc7d641, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-397f2bc7d641 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-a6f391cf9242, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-a6f391cf9242 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-f29b6206bac6, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-f29b6206bac6 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-5deae7a660eb, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-5deae7a660eb contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Completed 2828/3000 tasks...
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-6f14b541a7b5, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-6f14b541a7b5 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-57002e2b4bfd, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-57002e2b4bfd contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-898e1ba8fdac, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-898e1ba8fdac contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-d0c2f23024d1, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-d0c2f23024d1 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-9b3a0178a5b0, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-9b3a0178a5b0 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-8b85c17e2fe7, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-8b85c17e2fe7 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})][36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,054:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,057:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,057:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,058:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,060:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,061:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,114:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,116:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,116:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,116:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,119:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,119:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,116:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,120:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,120:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,121:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,123:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,123:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,130:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,133:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,133:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,142:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,145:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,145:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,160:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,162:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,162:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,161:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,164:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,164:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,163:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,166:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,166:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,187:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,190:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,190:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,189:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,191:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,191:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,194:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,197:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,197:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,200:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,202:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,202:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,206:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,208:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,208:Calling end() on an ended span.

[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-9709caddb93b, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-9709caddb93b contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-904af220e125, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-904af220e125 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-355fafd0fa29, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-355fafd0fa29 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-fac878ab0719, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-fac878ab0719 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-c2a3d247a760, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-c2a3d247a760 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-87e56b658f4f, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-87e56b658f4f contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-501f23cc3b42, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-501f23cc3b42 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-4fa10ebec88f, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-4fa10ebec88f contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-82d92d955c0a, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-82d92d955c0a contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-4f0920ec1ba6, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-4f0920ec1ba6 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-9adfaf80dffd, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-9adfaf80dffd contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-f7bf53aa902d, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-f7bf53aa902d contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-907b8b2e93be, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-907b8b2e93be contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-5631b3cdfb74, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-5631b3cdfb74 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-0e024e694702, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-0e024e694702 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-4fec9ed7588b, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-4fec9ed7588b contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-f9c7bfa9075d, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-f9c7bfa9075d contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-13b3279357b0, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-13b3279357b0 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-bae6b749d82b, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-bae6b749d82b contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-885ab2508714, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-885ab2508714 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-ed10f90830b7, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-ed10f90830b7 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-460b80f7e95c, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-460b80f7e95c contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-caf24e91ca84, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-caf24e91ca84 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-4f0f914349e9, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-4f0f914349e9 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-cb583591d8b8, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-cb583591d8b8 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-262501d59c39, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-262501d59c39 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-e8e8336cf012, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-e8e8336cf012 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-7ed411abf3a2, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-7ed411abf3a2 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-58a3d3d8933d, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-58a3d3d8933d contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-d3d44259c7ce, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-d3d44259c7ce contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-7fd3e3421ddf, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-7fd3e3421ddf contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-c146133ec59c, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-c146133ec59c contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-a9a596544ccc, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-a9a596544ccc contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-229a1c63ea51, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-229a1c63ea51 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-5f237f616a04, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-5f237f616a04 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})][36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,236:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,239:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,239:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,237:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,239:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,240:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,245:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,247:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,247:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,254:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,256:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,256:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,257:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,259:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,260:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,260:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,262:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,262:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,270:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,273:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,273:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,278:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,280:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,280:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,281:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,283:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,283:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,294:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,296:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,297:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,303:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,305:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,306:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,309:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,312:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,312:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,323:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,326:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,326:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,325:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,328:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,326:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,328:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,328:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,328:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,326:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,328:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,328:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,328:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,330:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,331:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,331:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,334:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,334:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,343:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,347:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,347:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,348:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,351:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,351:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,351:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,354:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,354:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,355:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,357:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,357:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,364:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,367:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,367:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,371:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,374:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,374:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,375:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,378:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,378:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,395:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,398:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,398:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,397:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,400:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,400:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,399:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,402:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,402:Calling end() on an ended span.

[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-6f674d7bedae, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-6f674d7bedae contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-7a2895b292c5, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-7a2895b292c5 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-26cb35b4992d, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-26cb35b4992d contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-0bf5b29deed8, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-0bf5b29deed8 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-c7fa4da4647d, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-c7fa4da4647d contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-0afc2b2a3d3e, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-0afc2b2a3d3e contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-627783fe3cf9, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-627783fe3cf9 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-1e2082945ae8, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-1e2082945ae8 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-c700ccc15ef5, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-c700ccc15ef5 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-6183232faf88, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-6183232faf88 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-78117ae90ee7, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-78117ae90ee7 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-d07137980fae, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-d07137980fae contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-ea44e7ce80c3, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-ea44e7ce80c3 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-f6360212e584, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-f6360212e584 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-1811d5d10e8e, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-1811d5d10e8e contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-916ec328483d, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-916ec328483d contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-4159d6c82182, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-4159d6c82182 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-a73e9e84b350, will be auto-set to 0.0.[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,432:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,435:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,435:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,433:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,437:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,437:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,436:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,439:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,439:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,444:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,446:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,446:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,451:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,454:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,454:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,454:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,456:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,457:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,473:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,477:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,477:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,477:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,479:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,479:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,488:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,491:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,491:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,512:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,515:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,515:Calling end() on an ended span.

[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-a73e9e84b350 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-565c5ca1a4f6, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-565c5ca1a4f6 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-0a5dc45c6bd7, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-0a5dc45c6bd7 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-02eef50a526e, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-02eef50a526e contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-de2a5db3c967, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-de2a5db3c967 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-35fd0244338c, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-35fd0244338c contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-5cb5243a1e50, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-5cb5243a1e50 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-4114ff119434, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-4114ff119434 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-ae41c151db0e, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-ae41c151db0e contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-44e5b7b0359a, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-44e5b7b0359a contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-6b240804e6aa, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-6b240804e6aa contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-f9bf38c0a134, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-f9bf38c0a134 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-bb3468fe5541, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-bb3468fe5541 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-37d97844d2d9, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-37d97844d2d9 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-309cc5c0594c, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-309cc5c0594c contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-4928b63e6bfd, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-4928b63e6bfd contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-8c3dd82cba4f, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-8c3dd82cba4f contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-50c353ce1b84, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-50c353ce1b84 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})][36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,537:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,539:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,540:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,540:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,543:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,543:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,547:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,550:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,550:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,551:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,553:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,553:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,554:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,557:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,557:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,556:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,559:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,559:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,563:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,566:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,566:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,567:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,570:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,570:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,570:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,573:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,573:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,588:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,590:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,590:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,602:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,604:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,604:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,603:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,606:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,606:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,607:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,610:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,610:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,610:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,612:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,613:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,611:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,614:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,615:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,626:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,628:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,628:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,626:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,629:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,629:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,629:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,632:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,632:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,633:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,636:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,636:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,636:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,640:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,640:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,640:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,642:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,642:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,643:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,645:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,645:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:30 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:30,653:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,656:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:30,656:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,503:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,505:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,506:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,506:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,509:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,509:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,508:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,511:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,511:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,511:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,514:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,514:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,513:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,516:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,516:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,519:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,521:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,522:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,532:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,534:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,534:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,537:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,540:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,540:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,569:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,572:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,572:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,577:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,579:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,579:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,585:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,587:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,587:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,658:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,660:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,660:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,667:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,669:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,669:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,710:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,713:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,713:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,720:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,723:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,723:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,723:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,726:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,726:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,736:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,739:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,739:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,744:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,747:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,748:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,780:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,783:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,783:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,808:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,811:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,811:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,822:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,824:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,825:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,830:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,833:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,833:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,833:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,835:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,835:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,861:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,863:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,863:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,864:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,866:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,866:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,866:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,868:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,869:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,869:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,871:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,871:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,877:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,879:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,880:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,911:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,913:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,913:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,918:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,920:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,920:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,975:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,977:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,977:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,979:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,981:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,982:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,982:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,984:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:31,984:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:31 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:31,999:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:32,001:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:32,001:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:32 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:32,001:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:32,004:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:32,004:Calling end() on an ended span.
[36m(TaskRunner pid=147334)[0m [92m13:40:32 - LiteLLM Proxy:ERROR[0m: common_request_processing.py:796 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m ERROR:2026-01-11 13:40:32,021:litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 840, in acompletion
[36m(TaskRunner pid=147334)[0m     headers, response = await self.make_openai_chat_completion_request(
[36m(TaskRunner pid=147334)[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/logging_utils.py", line 190, in async_wrapper
[36m(TaskRunner pid=147334)[0m     result = await func(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 460, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 437, in make_openai_chat_completion_request
[36m(TaskRunner pid=147334)[0m     await openai_aclient.chat.completions.with_raw_response.create(
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py", line 381, in wrapped
[36m(TaskRunner pid=147334)[0m     return cast(LegacyAPIResponse[R], await func(*args, **kwargs))
[36m(TaskRunner pid=147334)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py", line 2678, in create
[36m(TaskRunner pid=147334)[0m     return await self._post(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1797, in post
[36m(TaskRunner pid=147334)[0m     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/openai/_base_client.py", line 1597, in request
[36m(TaskRunner pid=147334)[0m     raise self._make_status_error_from_response(err.response) from None
[36m(TaskRunner pid=147334)[0m openai.InternalServerError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 609, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await init_response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/llms/openai/openai.py", line 887, in acompletion
[36m(TaskRunner pid=147334)[0m     raise OpenAIError(
[36m(TaskRunner pid=147334)[0m litellm.llms.openai.common_utils.OpenAIError: Error code: 500 - {'error': {'message': 'EngineCore encountered an issue. See stack trace (above) for the root cause.', 'type': 'Internal Server Error', 'param': None, 'code': 500}}
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m During handling of the above exception, another exception occurred:
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/proxy_server.py", line 5069, in chat_completion
[36m(TaskRunner pid=147334)[0m     result = await base_llm_response_processor.base_process_llm_request(
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/proxy/common_request_processing.py", line 570, in base_process_llm_request
[36m(TaskRunner pid=147334)[0m     responses = await llm_responses
[36m(TaskRunner pid=147334)[0m                 ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1365, in acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1341, in acompletion
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_fallbacks(**kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4539, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     return await self.async_function_with_fallbacks_common_utils(
[36m(TaskRunner pid=147334)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4497, in async_function_with_fallbacks_common_utils
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4531, in async_function_with_fallbacks
[36m(TaskRunner pid=147334)[0m     response = await self.async_function_with_retries(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4744, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     raise original_exception
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4627, in async_function_with_retries
[36m(TaskRunner pid=147334)[0m     response = await self.make_call(original_function, *args, **kwargs)
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 4755, in make_call
[36m(TaskRunner pid=147334)[0m     response = await response
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1647, in _acompletion
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/router.py", line 1599, in _acompletion
[36m(TaskRunner pid=147334)[0m     response = await _response
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1917, in wrapper_async
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/utils.py", line 1761, in wrapper_async
[36m(TaskRunner pid=147334)[0m     result = await original_function(*args, **kwargs)
[36m(TaskRunner pid=147334)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/main.py", line 628, in acompletion
[36m(TaskRunner pid=147334)[0m     raise exception_type(
[36m(TaskRunner pid=147334)[0m           ^^^^^^^^^^^^^^^
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2346, in exception_type
[36m(TaskRunner pid=147334)[0m   File "/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 537, in exception_type
[36m(TaskRunner pid=147334)[0m     raise InternalServerError(
[36m(TaskRunner pid=147334)[0m litellm.exceptions.InternalServerError: litellm.InternalServerError: InternalServerError: Hosted_vllmException - EngineCore encountered an issue. See stack trace (above) for the root cause.. Received Model Group=/apps/mingjiel/Qwen/Qwen2.5-Coder-1.5B-Instruct
[36m(TaskRunner pid=147334)[0m Available Model Group Fallbacks=None LiteLLM Retried: 3 times, LiteLLM Max Retries: 3
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:32,024:Tried calling set_status on an ended span.
[36m(TaskRunner pid=147334)[0m WARNING:2026-01-11 13:40:32,024:Calling end() on an ended span.

[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-4f633b645eb5, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-4f633b645eb5 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-5a2738564469, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-5a2738564469 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-5db5edbb35c9, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-5db5edbb35c9 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-fafe6bbab360, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-fafe6bbab360 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Completed 2908/3000 tasks...
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-79e06fbe9adb, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-79e06fbe9adb contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-6f7dc2853d5b, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-6f7dc2853d5b contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-c2b921d43499, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-c2b921d43499 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-302ddc75f296, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-302ddc75f296 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-323a81b75bbd, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-323a81b75bbd contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-a672fd904166, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-a672fd904166 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-cccee88cecb3, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-cccee88cecb3 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-c19e16cb0447, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-c19e16cb0447 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-7ea2c9fb9715, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-7ea2c9fb9715 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-2676d21eb5b8, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-2676d21eb5b8 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-5df81c145e81, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-5df81c145e81 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-814dd1839eb9, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-814dd1839eb9 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-374c93f1e3ce, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-374c93f1e3ce contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-e6f003a0b8be, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-e6f003a0b8be contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-054c8c30fc40, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-054c8c30fc40 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-fc528ef4e1dc, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-fc528ef4e1dc contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-f0f7007e1fe9, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-f0f7007e1fe9 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-d03ad5d460c4, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-d03ad5d460c4 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-a184f4e68428, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-a184f4e68428 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-a8f9a8c36e5e, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-a8f9a8c36e5e contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-5167c53b0de8, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-5167c53b0de8 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-c8848b650179, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-c8848b650179 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-f9bb0b4fe3f4, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-f9bb0b4fe3f4 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-c9598b8fa7f3, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-c9598b8fa7f3 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-7d589631fcec, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-7d589631fcec contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-72756709aa5d, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-72756709aa5d contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-da4ff5956f04, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-da4ff5956f04 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-8e8d086fb56e, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-8e8d086fb56e contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-5f9cd13cc70c, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-5f9cd13cc70c contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-21b5de98bad8, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-21b5de98bad8 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-ff1cf3e518be, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-ff1cf3e518be contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-fcb5b01ec0fa, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-fcb5b01ec0fa contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-0b8ce880a9f9, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-0b8ce880a9f9 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-e8ab7340e177, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-e8ab7340e177 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-1d1f5729a97b, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-1d1f5729a97b contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-77a1927887e4, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-77a1927887e4 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-9001c178ac97, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-9001c178ac97 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-f64e2e03f623, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-f64e2e03f623 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-220a27cbb2fd, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-220a27cbb2fd contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-d11b2ea7376d, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-d11b2ea7376d contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-f98b4b73f82c, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-f98b4b73f82c contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-cd48d86269ca, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-cd48d86269ca contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-b92b518e101b, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-b92b518e101b contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-e9a952fc1830, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-e9a952fc1830 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-336d8ec5f777, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-336d8ec5f777 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-b8d8af3f978b, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-b8d8af3f978b contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-6bf37559b57f, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-6bf37559b57f contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-c39c61e6c668, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-c39c61e6c668 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-d3014ee79c7f, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-d3014ee79c7f contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-2fc09b944ec4, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-2fc09b944ec4 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-2a92d246e570, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-2a92d246e570 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-dc0c6593ad58, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-dc0c6593ad58 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-c3396dfa5d3f, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-c3396dfa5d3f contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-c058a9d323d4, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-c058a9d323d4 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Completed 2962/3000 tasks...
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-dc5396c7b0c7, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-dc5396c7b0c7 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-fcc40536e816, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-fcc40536e816 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-d45137f31fef, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-d45137f31fef contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-6268fd18a1d9, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-6268fd18a1d9 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-b1ad157cc85a, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-b1ad157cc85a contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-44f65bf01081, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-44f65bf01081 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-7957b120a065, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-7957b120a065 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-4e976c2b1733, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-4e976c2b1733 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-522e9dae458e, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-522e9dae458e contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-704082ac5cca, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-704082ac5cca contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-6891d7b3b11e, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-6891d7b3b11e contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-65dac6292b1f, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-65dac6292b1f contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-58590c99afc0, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-58590c99afc0 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-3a8d0ff3d7f9, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-3a8d0ff3d7f9 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-105c37e87ba1, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-105c37e87ba1 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-c9e7c50bad2f, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-c9e7c50bad2f contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-d08d7ab13f7e, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-d08d7ab13f7e contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-65f6ff4063e3, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-65f6ff4063e3 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-6edd00828968, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-6edd00828968 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-5f3f0f5042a9, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-5f3f0f5042a9 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-8b6fea4eea5c, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-8b6fea4eea5c contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-a10526a06018, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-a10526a06018 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-b3bccca66f58, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-b3bccca66f58 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-62b0b6ec62b6, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-62b0b6ec62b6 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-7e0c0c02fa47, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-7e0c0c02fa47 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-885a5586d3ed, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-885a5586d3ed contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-d9a8a1146549, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-d9a8a1146549 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-1e3a2345f890, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-1e3a2345f890 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-b83f89c0264e, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-b83f89c0264e contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-be2dca3b247b, will be auto-set to 0.0.[36m(TaskRunner pid=147334)[0m 2026-01-11 13:40:34,931 [INFO] (Process-147334 agentlightning.utils.server_launcher)   Stopping server fastapi.applications:app...
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:40:34,931 [INFO] (Process-147334 agentlightning.utils.server_launcher)   Stopping subprocess server...
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:34 +0000] [157289] [INFO] Handling signal: term
[36m(TaskRunner pid=147334)[0m Traceback (most recent call last):[32m [repeated 1218x across cluster][0m
[36m(TaskRunner pid=147334)[0m     raise e[32m [repeated 2030x across cluster][0m
[36m(TaskRunner pid=147334)[0m                ^^^^^^^^^^^^^^[32m [repeated 406x across cluster][0m
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:34 +0000] [165751] [INFO] Shutting down
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:34 +0000] [165751] [INFO] Error while closing socket [Errno 9] Bad file descriptor
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165980] [INFO] Shutting down
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165980] [INFO] Error while closing socket [Errno 9] Bad file descriptor
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165837] [INFO] Shutting down
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165837] [INFO] Error while closing socket [Errno 9] Bad file descriptor
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165690] [INFO] Shutting down
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165690] [INFO] Error while closing socket [Errno 9] Bad file descriptor
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [166476] [INFO] Shutting down
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [166476] [INFO] Error while closing socket [Errno 9] Bad file descriptor
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165751] [INFO] Waiting for application shutdown.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165751] [INFO] Application shutdown complete.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165751] [INFO] Finished server process [165751]
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165980] [INFO] Waiting for application shutdown.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165980] [INFO] Application shutdown complete.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165980] [INFO] Finished server process [165980]
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165837] [INFO] Waiting for application shutdown.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165837] [INFO] Application shutdown complete.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165837] [INFO] Finished server process [165837]
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165690] [INFO] Waiting for application shutdown.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165690] [INFO] Application shutdown complete.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [165690] [INFO] Finished server process [165690]
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [166476] [INFO] Waiting for application shutdown.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [166476] [INFO] Application shutdown complete.
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [166476] [INFO] Finished server process [166476]
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [157289] [ERROR] Worker (pid:165751) was sent SIGTERM!
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [157289] [ERROR] Worker (pid:166476) was sent SIGTERM!
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [157289] [ERROR] Worker (pid:165690) was sent SIGTERM!
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [157289] [ERROR] Worker (pid:165837) was sent SIGTERM!
[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [157289] [ERROR] Worker (pid:165980) was sent SIGTERM!

[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-be2dca3b247b contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-dd3c383703ac, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-dd3c383703ac contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-8dbe35cc1957, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-8dbe35cc1957 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-f19295eb1742, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-f19295eb1742 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-7840e2a5f0fb, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-7840e2a5f0fb contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-86e633f9bbd6, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-86e633f9bbd6 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-ed570d9a9040, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-ed570d9a9040 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-e0c36d181f50, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-e0c36d181f50 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Warning: Reward is None for rollout ro-7a1a36b05d49, will be auto-set to 0.0.
[36m(TaskRunner pid=147334)[0m Warning: Rollout ro-7a1a36b05d49 contains empty response: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': None, 'agent_name': 'write_query'})]
[36m(TaskRunner pid=147334)[0m Completed 3000/3000 tasks...
[36m(TaskRunner pid=147334)[0m All tasks finished.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-815d8a7537ca, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-41be421653a1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4fa74f90a523, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1343e45acb22, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c03039dfcd4c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0c7b8ed21249, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-24cb79bd0330, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-310474caf476, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4fcbfd0600d4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-38f9cba0c3f4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f7d73098bf61, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-14e6e9d164c9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-13e7d04d2da1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e21686b922b3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-95932c3c0f58, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-91270a44a1ff, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eb02252dcb15, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c5652d232d4a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-621432408ff6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b0ca99523feb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dd5ece4e0039, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-43c12d21fe53, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-19b8648a7c6d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-76399912952c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2ba66291ef33, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-85f697c7aad8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-26ed728b1926, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a488591a04c9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cacf3d3facfc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-201416e61879, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-51db4aea735d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-67466a484dee, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3a636ee792ec, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-980873f0ea52, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4119a43a7a32, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1dd420fe9463, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-153084fc1d1e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1fb2023076b2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-215276c463e4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a3d0ed4e5cb4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a638297420bc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7e29cb461e3d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-da32c8f18b95, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f0691e0e3b9d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0853316e83aa, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-09a9b7f29ae2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eda33908aa1a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8381f9fb3ce6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-81f7fb24cef6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dbaa49d79faa, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5c3bd4255ff5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1e602096d5c5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-37b7a6195e66, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7980a9e5f373, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e7119b307134, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-50c9dce0c332, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3d85c0817db5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6fcf8a5682cb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-36bc7b63e049, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e7228281614a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-672f947046b6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8b8063a4535f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a685a22c3155, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6668074b86f0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2c9672bda7fb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-01fb993efa4f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2c700fe5ff35, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c6c530d368e7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-58fb1ba32304, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4377aea20b3c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8c713aa83aea, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a5711aaa6789, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-086148859750, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-229cc4e81641, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d1ec062b8e73, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2843c87d6015, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5ce576628cf9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-54a3541b740c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c46966a72f9a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d51268b98d9c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-57f8ee0ea310, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2eb019f55bc6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0ae2b723c123, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8a7801ae403c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6bf0135a39ac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0830f2420c58, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9b9b280d5d6b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bc65942a528f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d0507a16eba0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-be1237199cda, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f8fb79095d30, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-045883e54a40, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-30d7e323ec69, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0ececc03a452, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-61d88195c632, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5db3a6c9589e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b8bb900158cf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a23ea7bf6354, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-be0fc931a9ad, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6cce0f0d59ba, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-db4c392e82a9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1330f0a611fb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5fd718994bce, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8d55a769cbf1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-df385db82c87, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-207392b51b1c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8f273ada761e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f3152aa1cac3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1009e4c5c14c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2a6373cc1da7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d43d144f78ac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c18842a8e05c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-82774ab38da5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-10e0c6df7ea5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-88d8b5eb2a7b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-784a7cc6f9c6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fada36f3d743, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4153b3a11991, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3c9dd50685d5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c0c020579464, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5834fe229b3e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4a46371e838b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0d1e5a3f75b1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-83495813db06, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a978ae260f12, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c8745f6a019d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d6b19a0b480d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-998482b19a11, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5f60fdcd3c92, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c72149012cd4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ace53dafd946, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-af75f8d03bd1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e658a3bc69c8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2d27cbc7fd27, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-02d89d700826, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fbaff73ae713, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-520e15e69042, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7ed97a089671, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6f02cfa172a0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-89b145674a2c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2ae9e11655ac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-451c0fd43cf5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eede6f5f390e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fb2a158d8dc0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-620c5ca0091d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0f931d6eaa44, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8ed1053bcbd8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-05242acfb2bb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e6b0b0961f14, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e276df88e8cc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d7a2b04e8cb0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dfa6f35e8324, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-54204d30a5b1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cf515753a33d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ce6ba2044bac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fee8374c4258, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2877c30ab10e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ae408d4aba4b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-de2d6a966734, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-75e10b791397, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-57442e64924b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-864a86438ca0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d750d3a34637, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8f91a6641bf2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-52655e61cf7a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4d380851c036, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-921a2cc2db3a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fa6c852e91b7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ed3e1e917d55, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-90abcdd3b284, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-91ec88884769, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9473d45480a7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f8d681e4731a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-18dbaaf35c5f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f8f896328d4b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-080e35b903e8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-14e6eef68e2b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-98d7ba68fddf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-78d04f2cbe5e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-41b28d65f23e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1c2dbbf1f28c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eafff2528ee7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e5e81919ebc9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b058141c631a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3230b509b766, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bf664f509207, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f65de75dc377, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1c380e1cfe1d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b19a09e79a43, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bb3f78430f68, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-40ee442e4594, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9c531731f0b9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c7b7b1480300, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-902018177e80, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-58ececba2197, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-21afb25c181b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-db2ad7b29cb8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-659ab1b732bd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7cad9a504469, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-182e92e1c6a8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-143f98dfd83e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-026a384880cf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-23825a33db91, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7469bfded714, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6056e3320fde, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-254f28c2bfe2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3830f9a1ff90, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c84c69db37a9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-521a0c9c439e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9a5ccdfadfd7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0333a50fd63a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-df49c22ad106, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-361ea7f7738b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6d705efe8748, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-00afab572c0f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6659b6b55331, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c94c562246e0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e2e689125066, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e8a13e300225, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f1a5cd5f30ca, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-56e347b12880, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-26e0708b829a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1b6df3b9601a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f6b390a07902, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2063c8a2340c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-516a7abdb358, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7398f2de2458, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-066e33c524d7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-93bf34d5479b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a8b4601919da, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3a40697171c1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-076532f7fa2c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c4af7ec803c4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-971e5259065d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c5683c9b72c0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4694129ac675, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2f26fd99cd37, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-efb898a8d63c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-27500ee3d6df, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fb24efd835dd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6cddaff1ecab, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2487f7b9bdb6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f3891d49f700, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-37d1ca627d40, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9f1007aa8cea, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2ebc87c0ca03, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8932ae2a407e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8793c1d523e5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-56628835e2cd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-99dcab45d274, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f2f23e89f719, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-75b017f02841, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-834569549c1a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1eb4d718805e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-abaee3f8c19e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ff89bbff47f1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2a7936cb815d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eca2806f26ac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-867fe5e8c6f8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-95990c2eae81, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-258fe614b208, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8e0b91614150, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6abf0a729a5e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-56c435b045c9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1253c9fbbdc8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cf97acfad92b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5ba9b79c6b61, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e49365e6984e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3be1d9068723, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9c47ad5350dc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-66a768cc7348, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-83c20a40c631, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1f249912bea2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a600fa2cc205, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-32f47d52c35f, please check the acc function.[36m(TaskRunner pid=147334)[0m [2026-01-11 13:40:35 +0000] [157289] [INFO] Shutting down: Master
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:40:35,261 [INFO] (Process-147334 agentlightning.utils.server_launcher)   Subprocess server stopped
[36m(TaskRunner pid=147334)[0m 2026-01-11 13:40:35,261 [INFO] (Process-147334 agentlightning.utils.server_launcher)   Server fastapi.applications:app stopped

[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7e0afc717b8b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-58164dd0c2e8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d8c8573df780, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ee53753df600, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-476066f83dd5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9e91e9f4d855, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-71ae0a2f6c24, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-171bc46aa227, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3ab9a788b66c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2957b0e06139, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c7f020d50878, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-04a1bb6878b9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0cfc208b9bd3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3fbcb9e6f52c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a7d04a8bf27b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f7668bc3a47a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-778730b3ad70, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-756292dbd0a2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f630470ea0a9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2c688aa1dbe5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8c55759a1934, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b1cd4a8195f0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-972cb5491ae4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a41599c0d576, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a97948cf1f60, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a10c0516c03e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e2104748c2f2, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0336693a2675, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bf5444b96586, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bb99ad6cb181, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-be583712e549, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6219209039a6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8ef7a05331be, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-08a8f0e327d8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c2344aeb00f6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-84c2ed0886f0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1f92bce6e00c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d697d93a0eb8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-77e197f97dae, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b77423f10012, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a6073fc52459, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-43d06c2bffa7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-47670eb1bb00, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e93f2ac704a4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e16fdd752356, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5228922d4bed, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-570beca6eec8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-875ee9e3c142, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-274ff07f7013, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5f78b519ff3c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-37ab601bbaf5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-63fb6d723e8f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-83ae4899c48b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e538a474ade5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-76a96431ce17, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d03ea55924d4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d202d938d262, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-90d14d526298, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6b918390aec9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-519f372e6055, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b0afd29bfd2c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9fb31e30d508, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3a39c58e3277, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-48a1c3cb39f5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e9b7d49f6f4c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2d0479616654, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a29f31a727a4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1e533edd3e98, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2ef26fb2a8ef, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-59f54c11d100, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-478aca1bda9a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8f8d020aa960, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1ee089cdbc54, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-46dbe7cb6961, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d56f0c9fa6e0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-264d44aa3ecf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1c398c73d7a1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9cb3000b4325, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-161a50305230, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6ee8040d8615, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-097bc3104484, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3af511afa87f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-89fe0da49806, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a0482978579a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-54f07e80e0b4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c867e2b2af55, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f2e1bd8b2538, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-76865da850ed, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-64152f7a017b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a511765f031b, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-98ff8599051c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fab82ab1f323, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d60352a6e0a4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f045ba556716, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2d8f3f2ce8cf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ee99bd499bcc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3e0e15e54f9e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d3e10b16f0a5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-47e5164dedac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-117bd4fa39f8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-145f9d4feebe, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-987d29eee6f0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-815a206c9383, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b625e08f568b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bacead42433a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ecb4a4ad6b46, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5b622844f033, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3781285366d9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-23e5cac671ac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3f47695e2137, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5e36b7e5d1bc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6f6e32bb9d33, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9c34c9fdb905, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2ccb7ce7c435, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6ca4cc1c5b19, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f9f4cc5e9bf7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e4625b1a0143, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7d4a3584cdd6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ce416e5f4761, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-af4869351446, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5ab82df79a0a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2539739e0b51, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-95efdafbe49a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-93f3fe52dcac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b96c75123a4a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4306570b9a09, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-07082b920f37, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-97c69b4d7527, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d0ec05085116, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a0321985f9d0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-31d3113d7154, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0e01f191f3f6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2194200edd74, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5df9386f792f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b16168e9fc08, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e24558391153, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b4b9bffd820b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-28706bd6213b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-460df9bbebaa, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-304e042b503a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b48a02dfee62, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-307ce8df4e88, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b53773c899da, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-48eeda1ce8cd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3d02a98b1023, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f83a0fbcc87e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bf8f606ccab1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a3f99c35b890, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-111f87bc1b75, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-31603b25ddcf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6742f4e67200, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-48f2e71522bb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-11a5325b29dc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-60209aa6df2e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-79675104a095, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7b32e73b16c8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-69669598e95f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6d704fe1027b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b40cddc1773b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c94a84f2339d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-936d1ccd2743, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4aea6e0e6ca8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7d400a373ecf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6ac4ebe4fa3c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1569caedad72, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5d49fa23a21d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9330cb286b6c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d9739967654e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-036aa490c180, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c766aa51d979, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4688d5d75930, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f2266254d7cb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-81816a10c563, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e5b924ee8a42, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7c248f3a6cc5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2fd0b4582924, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5dddc33d4bf3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-47a7c551621a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c75f599f0c6a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-98320b49b741, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0efbf70e22c8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e1028b2c46e4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-85b2296c4414, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-332153f1fab6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9564e15cbca8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2772539ee0d4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-02211fdb39fa, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d89886151ed2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1c26bded711b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0821952e5290, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f9b6502653ac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-283490932ff1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ddc5bc662ea2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6a11441ad9f7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b417210a7fbe, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bf8c93ed256d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6e25bc768ab0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0c7b00bb058e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bec12f90992d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-17d220d2f8bc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-917d6d1cb0c2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-641f7c7f43e9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-212d7c962295, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7ab2edc2412f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-74f8d6a53870, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b88c7726be8d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-265d230a5fe6, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b05d2e3bbe43, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-faaa1933e72f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0ae2f85f7694, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a632319aa636, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-08cba07ce407, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a98b492e9125, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a364a766062f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d1fa87010bed, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-acc4553c256a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9fc958d2d7e2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-61736e94293a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-22760cfbcf04, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a379ec9b7ff1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-23185c4a2bfc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b3146892d16c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1a7e2958a626, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-24e4ab0a6434, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-92d0b2a6b75f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aa05586ffffb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2e8e884736d3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-17a0cceaa11a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cc9e4a29c853, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bd4481d7aab1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e2ecb5ee860f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a3d367389e16, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2023b7f33a24, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8f4cd85d7c9e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-07fced41f436, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6f45c66bd73f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-54fe04080505, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5ea6dc3e2b71, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-631a66602623, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4f465621b6bb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5cd5bde011b0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-83a86969cb48, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0b31826dfacd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dcbd22675ec5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2acf4406ddc8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-585d3a570223, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-515742e44947, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-28ccba734de8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-381c07d8df1f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-22cdf3d0a647, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-70905f484f83, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e0156a5732b1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b0fe0cd44fbe, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6ffb24b24f0e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-99af2c56e1ed, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4ce6e818d10c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a95bdf906ba9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a2d56052ebd6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f2e99deae856, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-af16de820416, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5cd43388e200, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-97f3824d7c10, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-04fc0e99697b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eb70f803a5af, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-205c432e4a31, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6334415cf9ef, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bb357257821a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1e5bb031f856, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c8dc569db202, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-95677497419c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-910991f283ce, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-71b528290a83, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-094a0bda2889, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-116f21e6b28a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6356bcc3d5fb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6093a97c9807, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c23567836f6f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6466aaa0e492, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f181fe01eb57, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2315955d4e7b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aa9ec70b5d8a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-64c3abc53ae7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5e47cf7f1ded, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-626b4ec1fc7a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8a8478930fd4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0b5bbd20d2f9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-80a2c0db8925, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3b330c3a2278, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-05d1358c85b7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-29530cca7a77, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ff7544615770, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f8421f33fc8b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ae6d79db816c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9548b8d00402, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e3b6b0dc129e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2e27041de1ae, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ae3a4d928079, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9fa5c5f73a0d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5c96a4e92dcc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2aa28e840546, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-324e5c875030, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dae9476f2fd9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7a533705da42, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-886c1941ad76, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-402f8a323f55, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5732baf06b0a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-542be65955ce, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-38d109040de6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-519aae0e5553, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-59239de73f12, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-48f524c7039c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2fad6e2b8275, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b451892e523e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-01930fe0651e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-42b1c35485ae, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4fda86d31f75, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cc044d5a5b69, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d10fa25db255, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-53c5326be84b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-422ae7363887, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c31c548182b1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-291bd03ba576, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1977467bb3cb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-84c653d761d5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-80483deb2e28, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f91e63fc5f0c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-05b8aa8dcc5a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5686a0fd2b41, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-61c5b82c7bb7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aa430ca379d1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e58103658c31, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0fcd27458576, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-470b916f165f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a907d2a25736, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-725855d93f2e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-83faa6cb539f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3eea3580bb6d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-43d0acfe06ba, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-25cf182ab6c4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-19e3d04abe49, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8f1ab09dd946, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6bf5626410b7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-51aa2eb9de7c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4a4a5d4f5d02, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-44661a12ee54, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8bbfe61d5d72, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e47be26950b9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9f08250a0c35, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-74adf1878864, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a4dc92077d55, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-488a214908ac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-63b67ecf9b22, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-419966112a88, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-850b316aee84, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b024531b38cc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dc3e3d44a5f5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-33269a75b6b5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bb60dbb2a91c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d750d3367b65, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bf22626cebfe, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-03c0be04dfe6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5e4824af47cc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4d83e6a591b0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3a3b32a59cad, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d937aa51c2c1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6ea57abb8290, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9f820808db80, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-96b04784eaae, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5777e0d745ed, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cccb094a2975, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bcb60a5603e5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c8f72a96d3bc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4659bc2364e2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e79cf0498504, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f7c44656efe4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e5b951ef4c0c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7c7663e526e6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b0fad63f5fc0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ec8e3927b175, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-02d1654521db, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c7ef0c15c861, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2cf389496cbb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-27351ffb55d5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4b4f33d94351, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-493109704430, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ee345bdf5fb2, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4c0dbae929d9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ff438da769ff, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c0f5008118ec, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a6e82908ce1a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-af86351195cb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0c9fadb08eea, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3b8e7892c418, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9addb59e4526, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bf12780e1987, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e9fbc0e0d781, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3d5280beeb0c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-06420f060355, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-95b6ab2f7804, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f7583219bf32, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-61876dfc74a2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6d3ea254acc6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4fda3cec01be, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-24c3c50bc811, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c4629df98ad0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-15d75e2de4cd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1219f5e1eba3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dc546883608d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ce094ee8aed6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0af0a2f0534b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-569cbfe07a93, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-52db3c00d20c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-854ae3979b7e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a2171251d794, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cfbf6ff1fe05, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-29048ceb24ef, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2583d7e86eee, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6f8f48cc5909, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-42301dcd1739, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-896a626d935d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3aa642fd6c59, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-88b756762496, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-be8792c77887, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2c9ff70521f6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7bea3c7dbd06, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-695d6e3dbe97, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3e5641941f9b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-129599649010, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-823020c63279, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f70978e6e455, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c6dc0001b3bb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0d28d09b6182, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5b0b95079e03, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4b5ca3ddbfb2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cc2301e1698a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eb86e52122ee, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6fe8244c6d1a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e99d931cb254, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6a4dea09e0ad, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3763206fb0d4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6d13034ebcc2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a5422acdf1f6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-55ce6cdff146, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-622a9ee4f984, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d6ba473dbde4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d430903261a0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-81192089e6be, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-976ec868975d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a114cdede1da, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f47568523d49, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-85f97cf8d7c7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bdedafa92e78, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0f667585be13, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b8b0833e6a16, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f4630d080a0f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0b439200b9a6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-de299a96b7e3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-caf5320ab6cd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1cfc9dace70f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5d50540c7ad4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d06847e3b80d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d70d3b66c1e3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c4093ca6acb7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-60b20eedc755, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-18a44d6c5452, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-52e26d16e83b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8c0b32009645, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5aacd7aabf7d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0a12e1443837, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a1dfb17f7a86, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c84e6480135e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7c02c61e938e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-69e6ea1ccbda, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0b96c8b9bdbc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-65e57a3262a8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-05be3bb8e719, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ead5c999456f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dc018aef1a71, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ed7a8fa238e7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-06ef5e184d3e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6cf18ded7942, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fbff1d6aa29c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-360b3095e282, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4085ad10c1dd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e7feb76886dc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a7a724740eab, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9e5756209baf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9f0254990c1b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ab14ec106dbc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c537a06322ad, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f527f59019fe, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7387215e43fe, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9a911627fa06, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1cdee33f7c44, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-22159cd01ca4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-88450c4ca173, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5e43e5c6a8b4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e054b034f831, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6b4aa5819b10, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3e515427c25f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-29b6ddbb755c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b9cd7702b58b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c52829aa0b8f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c910c2cfc73e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-68fcec9955ee, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6cbae8595d3b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5dad2ab48671, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ffe9c52a9d02, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a99bb5600c90, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0d9b1ac66883, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a96992e548d9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4911bdfc9c35, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f3a70f418c4e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fc3ae9be5066, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ddd2f534fc46, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aab8762f414a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4e80a33177c6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c9e26d9cd5a4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1d77c2d97a14, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b91ec91eb366, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4b5a3fc94b57, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f80b3731450c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-617384692f77, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e14aeab93d78, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a94bdbd58fcc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cd64f5cde984, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-077d96f54e56, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0d88e294bdbe, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bcee70ef1d4e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aef978caba5b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4ea3fcb14f07, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7bacf689c00a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8526a6d62e55, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c4366386616c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1a23d5c9eb27, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2b9ca9583dfc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d0c195acbb4a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b90be4899c41, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-551a1a1f6906, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f919e0b78045, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5fe63911f920, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ecfdb48a2d10, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f67718f74b83, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eefa39348513, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4e86ff016a91, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d162d40555a7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1d6ea558c99e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7185d63827bb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-41e5a534ce4a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b0238d4dc2b5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-50f0b6ee4e30, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-167379d7e927, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1fc0615015cd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e5800d3817e6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-56377499ffb2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d113226ca177, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0784d3ca9b11, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3f69a5c69d5c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-87de0bd17961, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9684ff068247, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-74cadf3acaeb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-13c22ec62cb2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0239d3f13c6f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ebdf2d6b3d4f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7edf163462d7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e528aa328c68, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3969c80938d5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bb45a8deff94, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7a10ca99958e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-824c6b025d1b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b229d16eeba8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-df21d9a059fb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-00e9e1c0fe1a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8837b94095c1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c951c265f652, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-490f65f54a47, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aef04bd5870d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9bd2558a5f60, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e70a0814d2e1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a9c041178b8d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b454a1a3188c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-44e10a93c2f0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dbca740dec88, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c225ac503aa0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d6eb3700d152, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b8fdf2676458, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-efb684d11f56, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-137d0e3e31b0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e8ef424c463f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0636dd464ff8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b3f48c3d1ad0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f3be56f73209, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f6ae70e34290, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-97b05220ed8b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-30dd20a24e09, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c13e480f777b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-269c90e846e0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-953a917c2eb3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bb792124ed6c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3bc891cc7e1c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-555225126642, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8cf10262c07d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cba58c5a72d7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eda4daeb716c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7e05c3612b28, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9fd526cd5cfa, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7c3390f94665, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4822a0739c91, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2f38cf455247, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e13e10e08fee, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a36be88b7a1c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d4e01960fdf5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-68dc5615cef4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b8730bb349b0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0e81a91bcd77, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6cca975e7c92, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-93d116be7901, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-59986405d0e8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-73457e6d23d0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f4044da759bb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7ba639c0b964, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d6dc3cdc21e2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2ab91154dbf0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0d8093b6ce00, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1f0abdf726f2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e20752c87d40, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d4b4048705a3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f2a1b316ceae, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d9a016e5d7fb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ab195deee5c0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1fb56b0a16f8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-565944cd5bcf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5a1e5bbaaab3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2f27b33b3001, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1060ecd50196, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4abec6263e5c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aa42ff4546e9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f291725eb076, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-999601f5a817, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-883757fe6492, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-08b91a35098c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-090097fea350, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9a146bf7fea1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b32ffe38541b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1d616f9c4a89, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a5ff3234e44c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0577098a6529, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6942421d7aef, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-94371c5f9881, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e5c8f0bd0610, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1667e22c1d4f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f59f6b9ce77a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ddac02dd121b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-859462def64e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b655d4cf1d51, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8a47f9dea63c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e7ed957f2ec4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0b47b1f180a5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3b7a0d659e90, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5a54c937251b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d389d637889e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1a42a31ff878, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-819fa33de635, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-94cfe5b672c5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-49a61aecf6e1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ca3144d28748, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ffea9b8ad3ad, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a1ae552442e0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8d230049d7ce, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5639d51af090, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c421a2d2a80c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7f9002018775, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-949db7e38a17, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7f91572d4ba8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0220a8d01cf0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8791a4d8a91d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3ec6525f4772, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3082f7763732, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-edb6cd34253f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0cb00e91f417, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e0dd97bc0ec4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6867be1c4316, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-22a69aa66107, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-89644705b611, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1e785b64e59e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7090aad2801e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e28297c47747, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d775f9247f7d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7a14c5d68732, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5039154a1c97, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f26c409daae2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9b67bc0535af, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c2d1aaf12134, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-12f7163edaf0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-89af844625da, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c9a7d871e8fd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c75a334432b0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a438892426d6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9f76d5ea2d44, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d7c3dcdb9bca, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ae70cae56f0b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ae88d4ffbd99, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-639adf16ff18, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d2cee22bc92d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b397639567c4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-057c317707b7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cd92f8431555, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-673d54895902, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9674169d714f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a356e3b3ecb6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6f44eb3d6a23, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f02ca88b457e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-13289fcaefea, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0593f5bd2d0b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-75f3277d345d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d076c4ef72d1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-10221deed391, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-918bb494af49, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-113420b9a164, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ccf76ceaf809, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0ebab615e56c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-16ac4a52f001, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a0e79a471c87, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b8c1bfeecce6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e1436598d7ef, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-124ce4a03d59, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-72bc2d6e11ab, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d57a8d614382, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-35f2895fef84, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e55bb63b2030, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8a9bd2db29e7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-df605c93874f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cc6524ce4b45, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-66055f51fe54, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8bc8f859ddc2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-746993016929, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a9657c29c8d0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5aee7f994853, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f8fd67f2c38a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-84c212e2d087, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d7e76fd827ca, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f451d1e7182a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4837aa4e8164, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e91228460b03, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b63a39982482, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-45c7729332ec, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ebc88804a77b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a9bfe47fff7f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-93a9ffb39cbc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-060345205fc7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9f0f01320625, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-010d3a58a9bc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2c47a106d6ad, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-700a988e4b17, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c69e64fc30b9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-123fb611e87d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2d165d065d11, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-afb50a78fae7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aae78a7e3cca, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2d0217bcf036, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2759df603b64, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4a5102e6c611, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ffb1ff891bc0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-384550d39a60, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a6f8883f6189, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5d4a528b6592, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-50dcea6f06b1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b1cd2f06b60b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3fc43fffa2cb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9f8856ca733d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-96eac5b53a9e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-66ea05450d68, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-682206f07450, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7272dd39f7c4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6ca089c25b73, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e706bab18c27, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ca036848827d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-edaafcc1e4e2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-79319b01c80f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-773383ab70d2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bad9463b10a2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-86844806e9ec, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fe10c23d59e4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f2ca652c4b45, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c0c4ca888af7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-202b655870be, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a4495fd3d7e6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e2e64350c95d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-879119c6da91, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-220997adba31, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f46ef6384b21, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-585bb4e84d23, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c144de4ec9bf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c27cc5095db3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-701b11738b8f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-65b2645319de, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8dd9410d14df, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dc46171ecad6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-22b63eec5ab7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-65c2241cea04, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1f785e07bc2d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fa5b307d2f0a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4561ad5457ca, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a61e36eaa9c9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-210752b12ae3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b973969ecd1c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-186b7b764e0a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6893ae609cf6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f96aaa26e93e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a23b655f4af4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eb0191296f82, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c453128b0c0d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bcfe1adb368f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b1941f7089a8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0909c41ada24, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-91dc113eaf75, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-31368c86067c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f8b334260041, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bad8e3706d1c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cdb0e56e8c1d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4ecade110eec, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a13de44ca4c8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1eec0ef07ca5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f78ac58e7e50, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-de4f08928ddd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e69fc44900cb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e3f6011c7a39, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fc57a5ded28f, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5bcf6a613db1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cc56da3bde33, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3525be503047, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4c96dd22c1f2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2e38bbfd8d35, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c6a526f85ca8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1d8f4d96208f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1f99b652c490, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-22a7f05fc524, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-66e40a79db55, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c6a5fab416c8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b4e9ff658e55, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3fe211b0656c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fa6535ff3963, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-43c0d1869e49, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-75c02c4bd313, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4af77acc70e5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4707aa98032a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fa2575bf0c1a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ed9846be3486, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-621c64e13ede, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aaf05a0b3d86, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-34678b98f37d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8dcd165ace98, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0a0741cdbf9b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f40807ca44db, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5eec9760e589, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d1726d6bf4f7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-771643be3226, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f75042f902b7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-240ee4ca7391, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4404797de429, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f2db96f898c3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-158239c24fe3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-612effab9d1c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-407ac5113675, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5d21626e67f3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-23c5677c0517, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-72e5471aa84d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-80c2f08a06d3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1e41169b22eb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b2623e6fafa3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-88c367f52ff8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1ac3d442c5bd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f5f12103b665, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cf209aa85ac9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d7398272bdf5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-756e07e60637, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-55503bb360f8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e138b3e9dfff, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-357840325087, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-70e7df0618ed, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ad20831208cd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-561383d1eec7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4a622e5e4c2d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3582c89770d2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ef71822e6767, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fd2b7f8ca034, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a2e77b813dca, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e2f84afbd439, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-797932818097, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-01970a229f89, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bf529131f98e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-82a72da0c09d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3fd32e1a405b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-958e27395eb7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d6c1fc3355b3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7b4049af2800, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-64a3c1ae16b2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f52ff3151ca9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-56d288bc2eb1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eb986ad64491, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c733ae8f441e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aef05794a849, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7be24bc49f9d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3c05e0c05436, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6b599f8ff19e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dead8787ac85, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f4dd8417ddc5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9afa10aae6c9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-43d4fa776397, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3a948d36b748, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b39239e7c8e3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b717e1e24881, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5e802727bd5c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-26f9e043a83a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c0edfb65eb55, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d8588a907a74, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e668a07d7970, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eaed7569e91c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-54905d8c04cc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3463a4afa3d1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fdc8385f4287, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7cee43739973, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0587e9c23f58, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-41534e5a5aee, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fac58904b307, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8fe088bf2647, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f3df271bff33, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-40a3a2a82828, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a1f3a437484c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3930ce90cbb1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-120a0210db8d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b84a37319e97, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-672fa5c6c6bc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a4f9ba58887b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ae020d2cd663, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ce5ec43d9084, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1b8dc88a0124, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b78e846fd095, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f036f8a0ed7f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-045552a5ef19, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0ff70d0953bb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-05c2d3c50628, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-67bfa3c44cc8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-971796719666, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2d0ce83145f8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-45d52f3a73f0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d746575a01b6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-add41201b944, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a8ff6d5915d9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a084dff200a9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8098017b8fea, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d12527226d8c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9ac08d631cd3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-abe79463a89c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3687c9d34b8f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aac1c5511600, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3b91691a2d36, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e4fdf07578bc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a877298efba4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fe19aff50384, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1dca7aeeb1ac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-063d77b8d2a9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4cb6fad29028, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2e394b8f14f8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d7be21673a2f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8f7f4ac45431, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-744b8e8cf864, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ff2710ac4a32, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-992c7f7d4630, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8d4633796ecc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-edfa3edaaa96, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-02544800355d, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e4c4a07a6bb4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c1eaaaf0ba59, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6e2052584afd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-145e4570a2dc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3a148d17679a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-348111275bdc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-74012d623477, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d781690d17e7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-af2b802157bc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a5e16012e7eb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f0d3cd31ae15, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-40204d7c0256, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c37a30c35807, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-378263c54251, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b10338bbb765, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ad1b652a82ec, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6c20d8a485b8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5f209f2d1e65, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5c6ab545b8f2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3ab8722dcd49, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f1ec6ce5ce16, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2d4860ce16c3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-854e196a1760, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b50c12637b88, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fc748a05a5ff, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c1de50ae8971, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-06e2436630ac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8b4fa0937422, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2e70278fd7f4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-96153915a2dc, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-370f84c30616, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aca55ca49600, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f3f36cbe7d7f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-466d1675e941, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d051a3c9fc75, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-43a776e3f554, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-01dc88b96b7b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a208707a83d5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-baa258b16e97, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-74535137b377, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1ada2e440dca, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b70b4c8090be, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-06add2cc0bca, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-273ca1ce0d1f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-33fd96c8f32a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0261f9ac1aea, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fe252a5722da, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-52216292ef3a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c972d30673e0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bd4475967767, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8d89649940ee, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-86898acbecb7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-292770d46a1b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dec3a1706342, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-203431634044, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d06a7006eef3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-278298c355b7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f0f1451d75dd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-04cf59d9f41a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-753448981fb5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b0f341109d62, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2dcf9c7f0c63, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3424c10625e5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-71027b3ca3e2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f39737fe4b78, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e84b5a954f82, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3bcd563133e2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-06a5871c8635, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-74ba5afcefe5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-df3144b4d95f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-76467450efbe, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-850d2eb04dfe, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f8abf393f51d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4bd2bc438b2a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-723411ec88d9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-009985cf7269, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a6a284eb997e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e8d27319d570, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-461ffc3da727, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-57dc74512564, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fc6ea2f1aade, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9b96af88eae7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-458834e0af70, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dbce6b89a0b4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c22ba1544c2a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8b9a149f93bf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f97eba1756a2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-858a56f48592, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8a2604537414, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-09b217a0943b, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8c744313b769, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2e0930327afb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-15c62a714802, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-340befdb3b2d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7b16e2014df3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dc3fc61e3e81, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7e3ea011a85f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cda557158b2c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4ad081f838ab, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e59db5fc7c5d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-85fdf3b1dd9b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7c07ba8f5595, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-92b1139772f1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-27af2440cd33, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1b8f70736f9c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f7079c164420, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-61bb1b296b97, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0572527be1f0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3cc4e6c77794, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8bacb491f75c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-22ac6eb95b7a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-93f32c3f5b91, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a3068a98c0b9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c3261c5d86e6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-00692a51338d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fee79e607816, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d068d70fbbe8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5fe2411d59db, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6d7edd36ef18, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a68353976ff2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1f7f01034926, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eb41ef02ff45, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1506953db13c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-941774b071f2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9a870d5662bc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bb529e03bed8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7cbeeb31619e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2c832d1faeb1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e7823726d3e7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d7e91a0626f8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-36da2dc8a198, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ec343a591612, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-28241ca09e82, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2ba182a65034, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b41e269d403c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-51d48434b0b3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3a36c2ad8c62, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2f043ff3f16d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b215eb0976af, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1a083d950635, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4e89ebec1552, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-54addbd4a7a9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8d180caf36cf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c01f7093fcba, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9a6e8fe470ca, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b6aa6dbb7b34, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-65dd881b228c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bd033533abd5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c5566abc96e4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b72665507584, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-15f32080d9fd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-74ba0c811218, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0ec5a845435e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-82f58a53f6c3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5bd326105e1e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f577cb34188e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0af647a7943f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-817de1f29359, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-17e33025ee0e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7cad319d2cea, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-597aee68a209, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5a69771be5bc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bb1e1e254d4d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-80f622e8ffe9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4e9bdc32c30e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8e840bbd9709, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-50b36e31e54f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-813b3c8ffdf3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8fd540ad0ba4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c40c7b56c8b1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d1a8ee172d63, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a95fd658206d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c3846b55a529, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8d73d99f808d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7151036b5c41, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5832e6dd1627, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1fae4aec1aac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a4bd7813e234, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a6103be94f00, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e6e10f5d7552, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7b97d2041390, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8248d53ed4fd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c96bc8c3824f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-38d0fb874ae5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9ec83ffca977, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a0732da2d74e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-477e5dd5a029, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-38eae5300321, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-87a6c8862494, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-03e8d7b41e72, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-55e6c1a75597, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0a92bd53fd47, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f17d9d81dc64, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e07a8a65f9af, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-13ee753eb2f1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-114f425b9ba7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-80590f0ff8bf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fd7d4be7fa6c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a56d1ef2de0b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c7ba770d5157, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d41f3b0d64a1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4c10a794b905, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aa55e9ff4382, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4b0c8bce071b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1f2c87ea3773, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bc9442ee8eae, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-320e09314a47, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-819974966b07, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2ad9084e2b19, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-115abf371cdb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-313c98b79b56, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5cdb1de42ede, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-824ebf0d2816, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5dce7e53ada5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a5dc46da76f4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3276cf39274d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-97cf91025129, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-16ba3a067015, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d9b04624cc17, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-73849fff1934, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-375043ed4cda, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-abb0753e8979, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-42f148a42b3c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2d474a8632e9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6456b6d7000c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-76e7855ae718, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aa9e74b840ec, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f2728ed11771, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-94e35942f653, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c2d9e0753c67, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-09511909181a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-15b25edf76e6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0d1f33fa3d8a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-48ff0805ddfc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eaba79542192, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bb6c76d2923b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bcca496104c1, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6c7ed500c75b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-75b51e739403, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4dc23a7b692f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4e6cb7b8b009, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fe68dc332e30, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-65ec97e634c8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b9e4c9fe5eda, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-07373c6196ab, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-14dc5cef9726, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-541f6dac063c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-76600f5d8a71, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-370a4b3f74ee, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-84f5acbc2ac2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f0e5843affa2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f020007a72a4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-84662113f244, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8e48701819b6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-db138fbd157d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2a3486a8a8a6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ba5c479d4413, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d1e333e9ec56, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-169963f35e7d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3a04d833288f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b9f81b538668, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-474f846df6cd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b91c771ee8e3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-36030585e66d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b8b591f38e48, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-087d9ed0af87, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-40563e5d9b70, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4820b8939119, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-59b1125b4a60, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b56070dd4b26, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c2842083bf6a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9bc8b4b2049d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8c5af308928f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a1e759929bbb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-29d08a1e1613, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ca617c896c4b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-de85b2ad8817, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-96159a5442d7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4862dcdeadd9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-35c6093badbc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-07975688c217, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b237acd74323, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b6f604e8281f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bf1a7b8f5837, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-60d30d5636de, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3344e09c50f0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2f90fb5106ac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-760781b7144c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2a65c42d96f3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d700e45c74c1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7830f7ae9431, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dab039e31d66, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fdbedb9fb415, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3efda0968bfc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9cb7fb087b7e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-17de93dc29fb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-977ba7956c4c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ca8ab786c575, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-14d346799cc3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9cf9cf6194d3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b9bfd30f17e5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6e5714c9cc80, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c114df01d4e2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8a929714392c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bf545fe0b6e3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ad5d7328de83, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-23d01e970e65, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d41e7eefecd0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3f9f0a92292a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-22fea2c6914e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-08fbe9397488, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-99078641feb6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1a52bb09230f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-16f64e235191, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6382c26105e0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cd8f09660354, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bf626a3ff498, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-029e8363e52d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b913cce7cc5a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3a78b37ddfb9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-90d7dcaabb04, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b3bcedf379f2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8672ee1157cf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-68f920839483, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-651a08071f66, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d808767fca8c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6ac820f7e853, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0f8229ac3cca, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f265fc9d2928, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8993c66e1377, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cd179117d3bd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cc35ec7dd75e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a995e064e1db, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4b438f52cf47, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-63a725232548, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4d91640397e0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ee0d478c7197, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-22eb3ba7b1bc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9ba054056d51, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-05117bb13314, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-148982333f53, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-323164fd4d9f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-528928aa727c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a9e40a1968fc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d1c84deb0bd4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-532d5218ff8b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-06439c91f345, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-166b69ee2051, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f9fda6196b94, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a83cd65b886c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-47b501fa57f5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d412b9c7fe82, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3754cb9af58d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b98f32d01f13, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2eaac1592c1c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-be7fd1d6f5d9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-84df4d78ae5a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c8979b6a66b8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a80e0fddb988, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8a78b3676d27, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6010540cac51, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1fac44a104b1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-66dd2a2ea071, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5847071cff35, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1b9cf286a338, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-29c4d3084295, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b95850e327b2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bab36ba68e92, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-32dbc9836f27, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-38192b19fed3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-842a530f75da, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e749dab7146c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2d90fb3da12f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3b1595555c1e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-afde2cbd5f79, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c4cb8f24c786, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-efbbf9a979fc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e9058edbbfc9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c7798a474c12, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-33a39e543065, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2d19bea14bc3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c8325449cf7a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4ac911a8178c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-57c2b9667496, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-04f460278d32, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0260a007e0aa, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-36488bbf38c3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-31b6190f0141, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8375d1a51107, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fb8bdb90f814, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-39353425eddf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9640aa9e88da, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f83f7731cc94, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-724ab2931fb0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cbbe205d1a72, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6881d213ef77, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-33cdac1d2209, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1c10be27c676, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-235b20739806, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cc579e854d42, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6721785a11c0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a4d7576b6931, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-defc2dcca5ae, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f6be5fa38354, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bce802093b0d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d0a5c508d113, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7947f1306114, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aa4af9426b0c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9f0149ddfc77, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cfb64a698437, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-99a17b13600d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-af903bceb94b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ed9b7f828d04, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9fc8c8e9932a, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3873e30868f4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-49db8766616b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-db467ae498e1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c5e29a5a2b83, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5096dfb3ca3a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fa0a9623d8a4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7a6870a43a5b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-456905ce93ec, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-10e5f6dadd3d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-85d6825d6b0f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b2e27980d56a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-88e9af3343cc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-58da3aaf4087, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4fb5209fc6cb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-239ff7b94a08, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-98b3037892d3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-07715feaa7c5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9597caabb8f2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e6a718e5b9f7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eeb22a5103a0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c95d451db929, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-50031a4346c1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f49d67fb171a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d0d16c669055, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fe21786e231a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-164ffc666a1d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b7a2f04b2762, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e3f5b1730683, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1b2ef57f2819, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4d9edb9b4528, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-269375e270ec, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-70fe565e9ae4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-77d36f995797, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2360a4065d7e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9bdc8ae1ed2f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-224ec5abb85d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fccbaca7cdee, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7d8a4a23e1a9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-75949cf0c7e5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d86e655c4555, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a9737cca2b37, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6e6c4454f59e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-33c064e48bb1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9b3f1f69f38f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b033a2abbe47, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-806abe2a82a0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9fed847d6239, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0107d0090f7b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7f48dc5bd914, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dfb4bf5b5859, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6615aa0c9554, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ffa6af81c0cb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aaa105bf5680, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-388e1179053d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f160600ea0d3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-60b47bde3efb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e0ee546e2932, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-36a0ccbcba92, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-84d047a287e7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-44bd0be45978, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7a3b3496231a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e4f4c9e464d1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-51e142496d66, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3e6dc4244d8e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-98755274aac4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-332006635198, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6787dfca4e02, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-521b264a951d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-74b188b213bc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5728ead6a9fc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a1042ae21514, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-68f15e514a66, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dbbf2df2971c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9f91767409b8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c8696c3406d7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-213376f9a9b0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a1ec145a58ec, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-05f3bb8eb7c6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cbf57b1cfd94, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ebddde2165ff, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-efea423833a4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1c90262f593a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1046bdc4bd88, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1f86667b2750, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6269e7afe71e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b6833cf796b1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-22f1421a6b12, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8a8a343e9238, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8fdd004f8475, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b3b261b283e7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-29fc150a4b95, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ae3351ddd3f6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-968ddf8222fa, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1ca73114c868, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5d0738dd8361, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e7ee0e094864, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-673c2ace2524, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cb7f9d45cbd9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4c7886146ec7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2495e63b4e50, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0298195bbbf1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-32214de599a9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4e8b61969d20, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2670468b78bb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fdae75b36585, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2d8c362d74fe, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b41be7e2e22f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f9e01a5b89e0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-03ec04fdd548, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f203f244647e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0d363b450226, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aa80354bb304, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-57ed98bf1026, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6af8b75544e5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-778f9ed32323, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2aee6291831d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3b386c9597db, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c63aba9c55be, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bbedaecec3c2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d1099d3f844a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b2c7d4d8e4d9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c9938af75bb3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3e880cb9fbb6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9ada1b8fa50e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d221102a0901, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2f75ed9dbc4c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9d9ca9992b6b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-978b485ebf3c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d341465011b8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c82dd9b2f005, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-22c17d964399, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ded528de9fb5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c26d0848ce00, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-091b94473acf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-01ee6290df8d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-da96e6f433b7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eee0a917ce8f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cda87edda7ec, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8318cdbb5148, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b00772cf2cf8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-46b6d39179c7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1c7215681153, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2821f4799ed5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7c83ab8ad259, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d546206b4bfc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-470442a685cf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d78827284854, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-19c9fe4f8f38, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9f5f22a9c6cb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-609c60f7fb0d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1cd857409d4d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-07b11620cb53, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1a1ade04e5e3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6f0bd228b707, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c26a51302799, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-10fd1301ea63, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e886485a036d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1430053fa564, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c8fe0097ba5e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cdbda65a5157, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c11b98f90d43, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c34edabb5d13, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cc52cbf5b860, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ad4e68512336, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2b7d314358e0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5061e2bbe3f7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aeb07afef0c3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-587be21f7ce1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-709f2e6ccb3c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2fc5c9fc6168, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7d1482125e8e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-02c2abcbae89, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-daff14b89bcb, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fe7a06f74fe4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-11d39405da6c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bd4e553d477a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-835a9b49bdbb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4097a8e66bc4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cd3a9153fb82, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-52bd568e2b77, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6adc9a1e5c2e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a36d25d7b7a7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e828f3bd0091, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5522e3ff151c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6505be847330, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bb883d3ecb09, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3a7075db29fa, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-45f1542ae92c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7af1d3613fb0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a717d8194834, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5ec74637b74e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-75c98fa8115f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5f815a5d1ef7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3a7b1ede3710, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-80b6633b018e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-92358f51dac3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-576e8caac449, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-892ec56cd4e3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-060f3183d617, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4b6971c4e93d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a7a099f8c2b4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f80827a4972f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-086437b472a8, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-08cccecbdbd7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fc8d442bb373, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9bb46d7ad076, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e8b2804c9803, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-02242d7fbef4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-989658bea32b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-39ecefb11588, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5c8540727dd8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0c3418d3f05e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bd8268ca6c70, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-47096ab90d1b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3cbcbbb4ee5f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c12df0bc69fd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-93c5b5efad66, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4845556d401f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-70ec54357822, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-327632c7daad, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-86ae5dcfaa65, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-588e50a6dc3a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7553698dbeec, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-311f8af5c059, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7607e237069b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-27f83b865253, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-63847965e8fd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-50981aacc3f4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a736bf199c12, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1e476c1f206a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-846d7e095c14, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1c1434801c6f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f76f06bc6651, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-99596937f544, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7ea72fe57bfe, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-839161c53b1d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e83173d18360, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c255627a0b95, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-543acdb425c2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f77a39310cff, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cf8651232b2a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-63c64fd7f776, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2f5cce2dbb31, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d0cc96e6e1d7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bb7b37bb7fbd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bd6efe706a4d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7a17d5a88749, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-99646eba050e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-270b98a1d75c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2ccd6664033c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b8fb8456f5ba, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-96c366a5afaf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-296ca27fbff5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7d7a297d22fe, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-119d00ccad7c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ba982dc08006, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-28347c574b09, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-686dcd35478b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7c515cab8a3f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-efd123779844, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2224ea8e3d7d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f3c97aa087c1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-40ab7bbb45d2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a84b15986920, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a43060c6f45d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bd70e2d73456, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0b9a1c018de5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9cc0ab810a4a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1576e0505262, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bffb5a5e4bd8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5bfdedd5cfe7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4ddd3494170e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c328ca9330b1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6a55c705ee06, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6e6ed4176ced, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7e259419982c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-59a3a0006755, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-96a165e1346d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f352e50625cf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0e9dab0b7c37, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-97cb3e9897dd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0c93840fa44e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9c3a0a846c27, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a3db8b3173a3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c75ade4fdc23, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-746918d18a0b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-28deddde4a55, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6cc7e5d1a3cd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-330a778ae17a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-825b841fc459, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e84f917109d5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9d7eb962c77c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eb8348b8fb8f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-705cd3996f6d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7af57e5451ce, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-02c0035ff629, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f4bb103513b2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2bdd53444825, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aeda5acdc71c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0a0cfd5e511e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e8fe347ce529, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6bdb8da01307, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1a0a85666d7a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ee2c498ecc77, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7464600b05ed, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1f6674b97db7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7d6f6e7221e8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a5c01f083776, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-970e46a5015d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5aaeaa6aa6fc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5d88bf6155fe, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8031ef80e62d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b736a974a088, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a9f2e254272a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ff7fccabbd82, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-17fad15f12ab, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ecc30341ae1e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-886b4946f178, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1347a2bac691, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eeb75251dc3e, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3b0cfaf4536a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6a0b567b14b0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-77b8fe91e6e9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4c349dcde93a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5a4c17bab811, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9c1adb5a6fc1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9007abd555c7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-091cb57e20af, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-303f3bdf9ef3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c766a8ae3f71, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8272ae3274e2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-466897a19188, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4e412eccc1df, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3e4d911f0902, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1b7ee0ac8f05, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2cad160a64b2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e231537fe8ac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-960f52edb3b5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aac129797a3d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bd6b79655fc2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bd6a2a97e2be, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-00ab83abc090, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f4d819404033, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-496da0dbc8e1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fa3d9b41a0db, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5fe100f41b1d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c3177f27ea74, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a81f62e0a34f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-62f404ec05d2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-566caa775c89, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2ef076eb5846, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9a8162f4ae4f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-573896d85475, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f50b1d0a8bc1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4f76781f803b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7d5ffcab89f6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-934f5cf539a6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-906128f22bd0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eea2181986af, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6ad079bb432a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-77e114836fe8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b76f86ac4496, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ea05127ff1c5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-66717b40bf5b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a80b8300e333, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1557b2161a55, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f17e7c3053a3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-12d92308bab5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e65e8ec9a1d8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c740add1f679, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4fec5e565138, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-34d98de29b84, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3a04ad147993, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-403f0b43138b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bffec3cbf610, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d5a2c65b7f55, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1b9845af9285, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-847dff84cfd8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-80180706ddf8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ec4e75d84357, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3fceff1f900c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3eefe440c00f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fb83cae51800, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d1176b72bca5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-47af6c78bee3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9854731275e2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d899421d0521, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d543437c119c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bfef5f270b7f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5713ba97c2e5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f67b9deef485, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e75b2be80249, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-81f5f05ac0e5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2cadd64a9dbf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e29d01b278c8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ea53389efaee, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7c97a0b169ab, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-31a3dd997a1c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e048dd86c265, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c57d213652aa, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6d4bafcd1070, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f987ccb74b23, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5cb2f97d9a29, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8671a6ab537d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-907b18428846, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a98117f9b4a8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-01bc83abdd6c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e8deae59d517, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fb9fd0f27ddb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8e0b0ef2d75e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7d3525348412, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8327b2777a8a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c11cf4021b40, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c6bbd9725dde, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6e7643f9c488, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-38684b1d8f99, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0d369f584863, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f9209705242f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c451c984fa14, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-542a197d10d2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-10e0bbf3d597, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a2e3bb44546a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cc1c7094619f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-484f8ca1fc01, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3da2186ce7c7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6e33ca1d954b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7849286e097c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-41fc469b1aba, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-32ced8efc3f1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-36879d2a54c6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f3a636e06c50, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e4288b6087fd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-022019bd8619, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3c3ee03682db, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a2697230de88, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ba5dad87c8fd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-05f34c22e1e4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c8386f328e8e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ee366db39192, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cce7951a7d3b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cbded282ab71, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fc0e2f05aa57, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-56ec71622009, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6f5ef31b6c2b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ab89cd9b3308, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a921ab65458b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d28c8723b799, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-60566507f46b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7baefe50276f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-95944571c5fd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d3212555d016, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b0517dec8e0f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2061905abdd1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fda65558c935, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c5eb50030933, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-362298c61df7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5f57aac61d25, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0a813060a9c5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-178a928b6c6d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c7f8203f9421, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e303d47be8cf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-480b1e7d3beb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1c7ef013d574, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-324dc5e78c94, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-524ebe82b8ab, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fb6c2d33d92f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b1151227c4f5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a0073fba09ed, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-edaf499a5e98, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-07aa7645d7b4, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fa0dcf72536e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-324f857e0983, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-12520e979d7e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6e8618f4e5b0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9df056d937da, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2dab58ef6e2e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-865665b27240, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ff15db9e71fc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9ab08f07a89d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-92d942f26ca3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bbb93d5a7e27, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9d2bc87c83ba, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-648eb50ce261, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-93fca46a2d93, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4ad04f72335f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6d9ced334eb9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9fa826f7200b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-05e2f75efffb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-05d88ac93702, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e251249dde30, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c7fec0290d77, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f611b3baaaa2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7f3d89d07788, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-277a91c0f24d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a6e94d6273f2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-33e58750dac8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cc3e8c23395c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-287911e42300, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ad23249ddbc3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0f4a1d286a8c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-056d02eeb6b5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-47f98c8b47a0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a646b157d482, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b6a55abc8f4e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4f21375bb2a9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7373da012cd0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bf682807b122, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-94d5d94e8ed3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c5dc6a82149e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8509a9d4a32d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-53bc33d8c568, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c7c268358b58, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-89d94ed59864, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f4b90f26211c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fb9f4d9e0901, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6669ef732971, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-73a563f5740b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-13aaa85fda2e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f2b9393c8405, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d29392c4928a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ce8653909c08, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b01fcd23b803, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-47caf236bbc7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-64901b0a1dc7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-92ba79b00e91, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e811c1303c1f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-18866edb002f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6249d61798ac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1b9d5b4bc13a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-30666bdfdf02, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9e38429b5924, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bb4ba34e9cf7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-97ed5b107759, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c1dc84a06df1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1cf6fcbe1f66, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3ed566fe8b16, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ec482618823a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-12dcbdcfee97, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-269da3f63683, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a850d6693194, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d500c7498b0b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-09ac4b3e9704, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4a909ca0f6e7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e65135aa6f5b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5e76e6cea4f9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e4a449091994, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5bc1c15e964e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fd2704fbf7ea, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1a23ae6a876d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fe6d9e3aa26b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-578bead95635, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fc28dfb3595e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-186a5c0e7a87, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-253faf4b759d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6a188ce5c455, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f059abdfa245, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1960669a8500, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4540f65c1435, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-712fc4cd08b7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-18081ab01ad1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d11e02ddbb2a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-953d4d9641f0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-21534bea4c8f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-673f181b25cb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a780bfc5db87, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-36a1fe7bee7e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-11734c020bb0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ce43eeff692f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-172c453d18be, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-96853d6dfd2a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-889ac2f8b017, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9462d7f77ef6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0ed88340d120, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b507c85dbbca, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a06f09c383b7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b4e9e9f618a6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fd8a4f6d5e78, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7b235a4f3e30, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ebd27caf9fe2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b83fda2eae2c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-53ed25b956aa, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-866f2f0fd3af, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-322e1a68fd9f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b9b31592f6ed, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-43b79633dcd7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-de7beb6895b1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-00b80a1e132b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9aed6456df26, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-82d520240cff, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2e3522db4bbc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-925200c57184, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cd128998af36, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ffbfecfdd533, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e790b5afecc3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-470fdeafbc65, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-be9b9f9a4210, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-abbb9f198c14, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2b7f3c88decd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-29f8d187d1c1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-69252c6c2918, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-30551bd147a3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1f53849973f9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dda0d679d08b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-841e6c81830f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2e8812c89f2b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2a303fe7863f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2b38faf8eaed, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d4d08e8c0704, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0d3262b04f03, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c31348623f8f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9f62a459bd04, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4960da1bac1d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1cdab79e4505, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0fb630692d87, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d8ad797752ef, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ff78080b7a53, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-954db165b578, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9db7726a00a8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cfc45b780a5b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6b29b9a07ec9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8bb5fd50b6ca, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c1175eaed007, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-08a3d9698ec9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d203b298fbef, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3fac4fc6ba02, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-73313f965523, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4a7253cebffd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5cc3fe4dc73f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5fa8f55b82e2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a0aa68d7a790, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5252b684d23c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c6a41faf1a05, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4ef0366e9072, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1673b211bda6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cac1c4eb03ae, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e13656643fac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7c375be54a5d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1ab0abe1ffd9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a028628750a3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b2bbb400ef74, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-81c3f96f7a06, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2da4cd5adec1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d5f6885d7d09, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-52b732da47ec, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c50ac5ae8599, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a48ccb7594ce, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d981da754ce3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b30191f667a2, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1342789710dd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2bf320632c6c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1bc527e37f10, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ff98de8c1269, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-80330049b4aa, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a6e83d899fc8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-28791bc39110, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0cfdd0afdee7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c0c91ff0be13, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6d86bc701dd5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3c416d81fd56, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e1da9138c0ca, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9d40e92c56d9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-064f8b5db052, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e258ae5d8138, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1cf50528ec11, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a3bf1f597230, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5f6b40adbf33, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3ca3bd927329, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7475d4f5f94b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e839d5db041a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d901b6070cea, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-22ea5b5949fa, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-107b5b4c9845, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a21eb50ceb5a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6bff0a51a60a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-caf3380da607, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6241e8309207, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-03e0bc132d25, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d32c04881a94, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-692e16c5cb1f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-86f06a308964, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9d4c7945e35d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-440019a0e9a9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e89cfeb7f71b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b4871932b912, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4a6749c5c2b4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f44f0eb4205d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ccb8d276b52a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b7815d679d5c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c1a5c14e3869, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-df49240bd473, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f130d9f45cad, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c21cbc19f83f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-106236952001, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-db36181d736b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1ccda2f89bcd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-14df44ae0499, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d4b2976b19bd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8aae45c700e2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b17d8bd46f2b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fa5f7b78de7b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8fdff33eb40d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9f8cc6204819, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-09a1f68ce4db, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d95e2bbde2f3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7261eb8eb683, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8057d038b342, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b0bc2e139cca, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4448019c81db, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2dfa1fb69e8b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e79012750f17, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-16c2bc082689, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-287dc4cba6a9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d822f75cfa8c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-baf95fd96dc3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-46fb52a3224f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-66f58c8ee01f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f2223facf747, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5bafcb156c76, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0694176bdc49, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-80857aa85b1e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-062fa6490c24, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-588e7d531b45, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b20c082c6f94, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4c03be247ce5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-456d77d96cc7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2ee7214ffc91, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-82cbe70f665b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f809c06e1e67, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9f5bf9441a48, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1bf8f858078e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a544ee41a002, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-154db27f4c24, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8c8df5a2abb8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b8b979dc5e88, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-285fec29ebf8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-30a7ee6b08c1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c991e30ad8d3, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-485f65ab5db8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ae7f96470d63, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-88283310f026, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c5db9681d4b6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-30d1ab2a459f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3656c1d438d2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-20b042ac7d79, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6a6a4cead55e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7ede128d6510, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b22970ba03fc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ca48f02379ea, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-558cee8503f7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4b51910bc23a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-19bfc7477fad, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e076def10086, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-35fb978a9195, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3abff6778205, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-12d82c17f23c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4fc4bbd79832, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7391a1d967ad, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1e20d92911bb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d7267c84de42, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9f3065aa464c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0ab029a2f821, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fb92d4818ef5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5ea68acbce61, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7ed08ebe1db0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4ee59205785a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-169bb25bb12b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8e05a0d105e7, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-af6d0b88206c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-09703f1c96e7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-99cfa42fe9eb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aeb73cc23c25, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8db377a1a49e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8ed893696afe, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-01ac52ab9985, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-98ce84b05018, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4cf08a534843, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2e0c0f36039a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4eb13b94a75d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1477ca355576, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-47aa7058a787, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bf4198686764, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-54d74958d047, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-87c8e50da099, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-53b91b186fd6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4e5451cfddad, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-33fd35d12b4d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-054dfed8fcd8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-deae5cab8dff, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d72264171df5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-638f5b7ce58b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-16d0046449e0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-157f6774cca1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-75554098a97c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c8e4234c6e2f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f5962cd3c1f6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6c2183271a08, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-80a6ed6c2962, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5263809f8799, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-231c20cb5b24, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-89bc3858e9bc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-02a3fa30fd8f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-14aba50a6418, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ddfd4a09c8a0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-365d4ee63b93, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5952f8115280, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5d136c639a08, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-61e3abcc0e1a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0fa5ea783bdf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f0c3e70204cb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8f43c58ec47b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-59105890a10e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-20b99e541f92, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6b92a1ba0b71, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-79d4407ad360, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-52d151826d85, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d37ba8ceac74, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-120b4ff42a9f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e2ba6dabdf09, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0708fbadb193, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ddb9447d01d6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b321b3c863d0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e9dcaed7f50c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-905f73eb5bee, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-557a8781ed70, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d663429cf2a3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-45687b082199, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-17d6d63da49f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4ddb22214711, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8136ae233a98, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3d4a8e736a44, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-61a25a30d96d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a1fa40f36674, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2e4476788cc7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ca11ce1c0a1d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-539ec95d88ce, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-231a6d975779, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6e9a418fa041, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b877d109b7a5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2dac5a43a096, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4a9ac240a881, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-841356f6c8ff, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-44119e762f83, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e7838cd67ade, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b78035caf9da, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0a012f1d68df, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ef13ac74f427, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-55c312c0afa7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-31ab2db811e6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f39825f72356, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-40c409185fda, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-21e33f28d636, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-39d3bf85268d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f4ea4acc7398, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b3bd680f2cda, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0ba1c767aaf1, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7c6472ffa5a4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-18d7157a6e46, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f7ff4817c848, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-919efb299b19, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8c917ce7b8d7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d151865dce22, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-21e5431e98cd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6a74c2a0081e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-06210b28b7b3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e7229daa438e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-51d926b3441f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5ea7e2948eab, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2e956853267c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ac4cc8192ef6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9f045cea5926, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-939f6e88ea08, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9e50412a5241, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-665883fc13de, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-042644a43af8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ad11a87cc6b0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d5f2f19105bc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f7fd933468fa, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e8139fd2775e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-feab57079961, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-df500c7c4260, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aa9e44f7e576, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5c0a4c955558, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ad93e0902a40, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-62665abfed5e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e5cbc433162f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-731501058e40, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-05758db6c0ab, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c577852f40b7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-460e8e570845, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d0db6927751e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b56220e6c16d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f8a01521a912, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-26d66d015f89, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-81a238babab1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ed3263a4a45d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-912f6361afd1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-de880c19180a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e8ae7b7ebd26, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e18061a242ea, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-983c1d960dc2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b91798a8096a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d548e9ed5f9d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-60d5055de72c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-553f391b00fe, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a080acdf1e2b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-abe995790ebd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d16ee37e6293, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-530bcadabf51, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3b0ea34ac84d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fe7edee29ae0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8a746a5481ac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eb27f0131f81, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-06d8e78e7c7e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f2aef44e34be, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ebf2249d899b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f6f56ceb40a6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-24c70152c70d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b3fddc389f2e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d245364fbc02, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-556fffe933db, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-63b2a3f98930, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0744fd897b55, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-73d94ddece86, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a58d828ed5ce, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-35f34d0dcf63, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-64c3bb73d2d8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4f0b2682596c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6f8a2e3e5d89, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-971c6479c6b2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dff585966a1e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-24c5bc880199, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ee41d06f6131, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-841a2e23be8f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-998315d71f16, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ec046de2b28d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f5c8d6c7a8b1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1c3a25445acb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-edb929c44db0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-63db5d9b9282, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-808d38412673, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1520f6b86373, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5625f839d0a7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f3484c2b42af, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2b02834a5fe0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-52737afada80, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a0855a994cda, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2a2d0de0cc5a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6741757ed45a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e8a828f52563, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9da96ea71f7e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b4ca9d390449, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-756cf66ca61a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8e717277da53, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-65892e662356, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ff5a534c2794, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a2ec2f7010dc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d01c812ff7cb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b129a014f373, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4c3ab80c1fe2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f2059d8c5be3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ee28c780a0e0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6d3d83cdb360, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5109f10e0195, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6cdb79502eaf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4a069dd10376, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2b2b23ac4121, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-356899b6f2e2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d4286ba54191, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7e255a008518, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a34dd178bab3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f62b7fa52c56, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a1fea4586ebb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-455fa6fe889a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-582f2307239c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-24db6d8efca4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-57640aebd2ba, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9479f6297ece, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-35dfb894540c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-034b5abdae20, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b3304dc6862c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cadd115269cb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a00a9f3dd32d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fa23c0c7c0d7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-402719508833, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3bc1d7c4a5d8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c0fc77dae4b8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fcc4e79fa0f3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-23d498ada864, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-40c5c6ab8cef, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ca7c13a4b943, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6ffef43ddc08, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-84c914da4340, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e8e9e63b0aa7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-032da6b5652d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-90a1064145ed, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bba4096fde5e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d30316697cc8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-eca0460eb941, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-de9fc13c64a9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ad7f139a01c0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-571a40d350f5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ffe79415ef13, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ef7ca4230ef9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-763c20adafcb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a7e29d3f4ce8, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a03511608ea0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-de6e67e90713, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5e3dd2e96a7f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aca0fdd7b27a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6284341a5593, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8ac2cd8b9d69, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5907a8a96120, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e878f5d6021a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c6d8b6c3609d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8248808b8d50, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b48a8ba36fee, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-efbda9a6188c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8174e9e1aab0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5779458d3ec0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9b4b35cba801, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-00900ab776f1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e803b4e7f7f1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-217614aaa1df, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ebf32277192b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ccacef0c4c5c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-66239f3f9760, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-90c5c20c0a18, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d084e26eb514, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e79bbb440577, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3c4aa1bb5a8b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6266a66141ff, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f7879892fec6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3ecd10185dae, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-02081d0518c6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0f0e4cfb326d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-451c9738c33d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-302f13b79ceb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fb1b181d4caa, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-86736720939d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aade2d0d29c3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-908c43234d62, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a5dddb87addf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ed848d92d4c3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7ab9d8a94241, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3ed84c0fff17, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4848515e642a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6ececba0ae96, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dda707907356, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-846ee91c404f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6abe6882d5b4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c9bba7ec82b1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b90ed7ee1134, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-031421ff658f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-96c69bc0d234, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-568c8da9e805, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-73cf6d5e30ea, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-334ea8d66304, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d8fbc327390b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f383656ffe52, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-01c3dfa83045, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3a17dd9366d2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-22c4fbcf50ce, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e28a94ab311e, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e40710b970af, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cda4158a99f5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7d85145d318c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d676c1a03a9a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0cc181a47a79, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cfe2eb72d743, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-35baacd5fa84, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bb3cc320570e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f47d3bb641e7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-444363c525da, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-246ed01b5db4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-caeeb50da67f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4620443df268, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e2c74420f28c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-89c221b22d84, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d5c7465e402a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9a0b706b98e7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-68fa1c21a9bf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0cc505046379, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d8b2dc58a98d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-acc990425983, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3b4d47bbb19d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0ea3d0f62317, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ba6c060c9bef, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e24603787d1b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6cacea4cffa5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-31eba9f62f4b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0ed2ebc34974, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4db4c45acd48, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-104e89de30e7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ba538fa06c03, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3072fb7c53ed, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6ed7bd99b648, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-483d50c202d8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-05e1fa087c12, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-be09d6fcb0d4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-430f302248de, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2cbecbcdb0d5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b4f9522444b3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6f90764fc9e1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2c143a7c04c7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-71bb9a2e7ab9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-99a342ab8d19, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8861e3a873dc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8b49eb7179dd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-24c3c05e0f83, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3178785710c3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-24808b468146, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-29a230a9c781, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e4d24ea46e44, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9559ef88cbf4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1698721d0124, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fc685037d1d9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a258565e7314, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9753acb4a227, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-21049dc10866, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b3750c74ef0e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-62729269567f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3a0085ffcd57, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cdba1dcd210a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1cc534b41c22, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a5198f2a5ac8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1f798c7f764a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c583c47b415d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5e45c926bbf1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8fda30676ccf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-237d0ecdc626, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e5c0c12accc8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-aaecdac1b950, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d3b28e4e247d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9dee828f66ce, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e2be1ee86989, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a71c5cc2f7c0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-443963ab2720, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3f877eb16173, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9f236f014d3c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-61681a78c2a9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-399926b55ef2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fbfebe889daf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-81399ea1bf4f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-54bedc141d30, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e5e9a2420f31, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9965a299d773, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3162b484508b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-24cb023da887, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cab79546a00d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cfd2fdccdfc0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-856afad66562, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e7acba0cc5ad, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f38e2a473e7b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0206ab590f1e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7573fde90f0d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a440e87fed13, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7cb3088cdd11, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6851d588340e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d516d46330c6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6c21c51b64a2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dd3cc109b17d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3caf970674e8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b7dfd20797db, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-397f2bc7d641, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a6f391cf9242, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f29b6206bac6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5deae7a660eb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6f14b541a7b5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-57002e2b4bfd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-898e1ba8fdac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d0c2f23024d1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9b3a0178a5b0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8b85c17e2fe7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9709caddb93b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-904af220e125, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-355fafd0fa29, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fac878ab0719, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c2a3d247a760, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-87e56b658f4f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-501f23cc3b42, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4fa10ebec88f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-82d92d955c0a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4f0920ec1ba6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9adfaf80dffd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f7bf53aa902d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-907b8b2e93be, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5631b3cdfb74, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0e024e694702, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4fec9ed7588b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f9c7bfa9075d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-13b3279357b0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bae6b749d82b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-885ab2508714, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ed10f90830b7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-460b80f7e95c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-caf24e91ca84, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4f0f914349e9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cb583591d8b8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-262501d59c39, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e8e8336cf012, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7ed411abf3a2, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-58a3d3d8933d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d3d44259c7ce, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7fd3e3421ddf, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c146133ec59c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a9a596544ccc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-229a1c63ea51, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5f237f616a04, please check the acc function.
[36m(TaskRunner pid=147334)[0m 
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6f674d7bedae, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7a2895b292c5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-26cb35b4992d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0bf5b29deed8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c7fa4da4647d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0afc2b2a3d3e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-627783fe3cf9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1e2082945ae8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c700ccc15ef5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6183232faf88, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-78117ae90ee7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d07137980fae, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ea44e7ce80c3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f6360212e584, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1811d5d10e8e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-916ec328483d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4159d6c82182, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a73e9e84b350, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-565c5ca1a4f6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0a5dc45c6bd7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-02eef50a526e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-de2a5db3c967, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-35fd0244338c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5cb5243a1e50, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4114ff119434, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ae41c151db0e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-44e5b7b0359a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6b240804e6aa, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f9bf38c0a134, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-bb3468fe5541, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-37d97844d2d9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-309cc5c0594c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4928b63e6bfd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8c3dd82cba4f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-50c353ce1b84, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4f633b645eb5, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5a2738564469, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5db5edbb35c9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fafe6bbab360, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-79e06fbe9adb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6f7dc2853d5b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c2b921d43499, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-302ddc75f296, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-323a81b75bbd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a672fd904166, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cccee88cecb3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c19e16cb0447, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7ea2c9fb9715, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2676d21eb5b8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5df81c145e81, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-814dd1839eb9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-374c93f1e3ce, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e6f003a0b8be, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-054c8c30fc40, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fc528ef4e1dc, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f0f7007e1fe9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d03ad5d460c4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a184f4e68428, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a8f9a8c36e5e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5167c53b0de8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c8848b650179, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f9bb0b4fe3f4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c9598b8fa7f3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7d589631fcec, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-72756709aa5d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-da4ff5956f04, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8e8d086fb56e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5f9cd13cc70c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-21b5de98bad8, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ff1cf3e518be, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fcb5b01ec0fa, please check the acc function.[36m(TaskRunner pid=147334)[0m 2026-01-11 13:40:35,742 [WARNING] (Process-147334 agentlightning.verl.trainer)   async_rollout_manager does not have async_llm_servers attribute, skipping drain
2026-01-11 13:40:38,001 [ERROR] (Process-138888 agentlightning.trainer.trainer)   Algorithm bundle encountered an error.
Traceback (most recent call last):
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/trainer/trainer.py", line 513, in _algorithm_bundle
    algorithm.run(
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/algorithm/verl/interface.py", line 138, in run
    run_ppo(
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/verl/entrypoint.py", line 58, in run_ppo
    ray.get(
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 2961, in get
    values, debugger_breakpoint = worker.get_objects(
                                  ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 1026, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(EngineDeadError): [36mray::TaskRunner.run()[39m (pid=147334, ip=10.235.192.105, actor_id=546243fcb39a8bf090246daf01000000, repr=<agentlightning.verl.entrypoint.TaskRunner object at 0x7ffff47868a0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/verl/entrypoint.py", line 235, in run
    trainer.fit()
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/verl/trainer.py", line 443, in fit
    val_metrics = self._validate()
                  ^^^^^^^^^^^^^^^^
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/verl/trainer.py", line 191, in _validate
    self.async_rollout_manager.sleep()
  File "/workspace/agent-lightning-spider/verl/verl/experimental/agent_loop/agent_loop.py", line 997, in sleep
    self._run_all([replica.sleep() for replica in self.rollout_replicas])
  File "/workspace/agent-lightning-spider/verl/verl/experimental/agent_loop/agent_loop.py", line 1007, in _run_all
    asyncio.run(run_all())
  File "/usr/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/workspace/agent-lightning-spider/verl/verl/experimental/agent_loop/agent_loop.py", line 1005, in run_all
    await asyncio.gather(*tasks)
  File "/workspace/agent-lightning-spider/verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py", line 777, in sleep
    await asyncio.gather(*[server.sleep.remote() for server in self.servers])
  File "/usr/lib/python3.12/asyncio/tasks.py", line 684, in _wrap_awaitable
    return await awaitable
           ^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(EngineDeadError): [36mray::vLLMHttpServer.sleep()[39m (pid=153929, ip=10.235.192.105, actor_id=7e6aab4307e7dd3e4738856401000000, repr=<verl.workers.rollout.vllm_rollout.vllm_async_server.vLLMHttpServer object at 0x7fff42ad20c0>)
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/agent-lightning-spider/verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py", line 562, in sleep
    await self.engine.reset_prefix_cache()
  File "/workspace/adhoc/vllm/vllm/v1/engine/async_llm.py", line 676, in reset_prefix_cache
    await self.engine_core.reset_prefix_cache_async()
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 959, in reset_prefix_cache_async
    await self.call_utility_async("reset_prefix_cache")
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 924, in call_utility_async
    return await self._call_utility_async(method, *args, engine=self.core_engine)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 936, in _call_utility_async
    await self._send_input_message(message, engine, args)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 905, in _send_input_message
    self.ensure_alive()
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 558, in ensure_alive
    raise EngineDeadError()
vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
2026-01-11 13:40:38,002 [ERROR] (Process-138888 agentlightning.execution.client_server)   Algorithm bundle crashed; signaling stop event
Traceback (most recent call last):
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/execution/client_server.py", line 154, in _execute_algorithm
    await algorithm(wrapper_store, stop_evt)
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/trainer/trainer.py", line 513, in _algorithm_bundle
    algorithm.run(
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/algorithm/verl/interface.py", line 138, in run
    run_ppo(
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/verl/entrypoint.py", line 58, in run_ppo
    ray.get(
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 2961, in get
    values, debugger_breakpoint = worker.get_objects(
                                  ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 1026, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(EngineDeadError): [36mray::TaskRunner.run()[39m (pid=147334, ip=10.235.192.105, actor_id=546243fcb39a8bf090246daf01000000, repr=<agentlightning.verl.entrypoint.TaskRunner object at 0x7ffff47868a0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/verl/entrypoint.py", line 235, in run
    trainer.fit()
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/verl/trainer.py", line 443, in fit
    val_metrics = self._validate()
                  ^^^^^^^^^^^^^^^^
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/verl/trainer.py", line 191, in _validate
    self.async_rollout_manager.sleep()
  File "/workspace/agent-lightning-spider/verl/verl/experimental/agent_loop/agent_loop.py", line 997, in sleep
    self._run_all([replica.sleep() for replica in self.rollout_replicas])
  File "/workspace/agent-lightning-spider/verl/verl/experimental/agent_loop/agent_loop.py", line 1007, in _run_all
    asyncio.run(run_all())
  File "/usr/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/workspace/agent-lightning-spider/verl/verl/experimental/agent_loop/agent_loop.py", line 1005, in run_all
    await asyncio.gather(*tasks)
  File "/workspace/agent-lightning-spider/verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py", line 777, in sleep
    await asyncio.gather(*[server.sleep.remote() for server in self.servers])
  File "/usr/lib/python3.12/asyncio/tasks.py", line 684, in _wrap_awaitable
    return await awaitable
           ^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(EngineDeadError): [36mray::vLLMHttpServer.sleep()[39m (pid=153929, ip=10.235.192.105, actor_id=7e6aab4307e7dd3e4738856401000000, repr=<verl.workers.rollout.vllm_rollout.vllm_async_server.vLLMHttpServer object at 0x7fff42ad20c0>)
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/agent-lightning-spider/verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py", line 562, in sleep
    await self.engine.reset_prefix_cache()
  File "/workspace/adhoc/vllm/vllm/v1/engine/async_llm.py", line 676, in reset_prefix_cache
    await self.engine_core.reset_prefix_cache_async()
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 959, in reset_prefix_cache_async
    await self.call_utility_async("reset_prefix_cache")
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 924, in call_utility_async
    return await self._call_utility_async(method, *args, engine=self.core_engine)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 936, in _call_utility_async
    await self._send_input_message(message, engine, args)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 905, in _send_input_message
    self.ensure_alive()
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 558, in ensure_alive
    raise EngineDeadError()
vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
2026-01-11 13:40:38,002 [ERROR] (Process-138888 agentlightning.execution.client_server)   Unhandled exception in execute method
Traceback (most recent call last):
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/execution/client_server.py", line 368, in execute
    asyncio.run(self._execute_algorithm(algorithm, store, stop_evt))
  File "/usr/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/execution/client_server.py", line 154, in _execute_algorithm
    await algorithm(wrapper_store, stop_evt)
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/trainer/trainer.py", line 513, in _algorithm_bundle
    algorithm.run(
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/algorithm/verl/interface.py", line 138, in run
    run_ppo(
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/verl/entrypoint.py", line 58, in run_ppo
    ray.get(
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 2961, in get
    values, debugger_breakpoint = worker.get_objects(
                                  ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 1026, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(EngineDeadError): [36mray::TaskRunner.run()[39m (pid=147334, ip=10.235.192.105, actor_id=546243fcb39a8bf090246daf01000000, repr=<agentlightning.verl.entrypoint.TaskRunner object at 0x7ffff47868a0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/verl/entrypoint.py", line 235, in run
    trainer.fit()
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/verl/trainer.py", line 443, in fit
    val_metrics = self._validate()
                  ^^^^^^^^^^^^^^^^
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/verl/trainer.py", line 191, in _validate
    self.async_rollout_manager.sleep()
  File "/workspace/agent-lightning-spider/verl/verl/experimental/agent_loop/agent_loop.py", line 997, in sleep
    self._run_all([replica.sleep() for replica in self.rollout_replicas])
  File "/workspace/agent-lightning-spider/verl/verl/experimental/agent_loop/agent_loop.py", line 1007, in _run_all
    asyncio.run(run_all())
  File "/usr/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/workspace/agent-lightning-spider/verl/verl/experimental/agent_loop/agent_loop.py", line 1005, in run_all
    await asyncio.gather(*tasks)
  File "/workspace/agent-lightning-spider/verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py", line 777, in sleep
    await asyncio.gather(*[server.sleep.remote() for server in self.servers])
  File "/usr/lib/python3.12/asyncio/tasks.py", line 684, in _wrap_awaitable
    return await awaitable
           ^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(EngineDeadError): [36mray::vLLMHttpServer.sleep()[39m (pid=153929, ip=10.235.192.105, actor_id=7e6aab4307e7dd3e4738856401000000, repr=<verl.workers.rollout.vllm_rollout.vllm_async_server.vLLMHttpServer object at 0x7fff42ad20c0>)
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/agent-lightning-spider/verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py", line 562, in sleep
    await self.engine.reset_prefix_cache()
  File "/workspace/adhoc/vllm/vllm/v1/engine/async_llm.py", line 676, in reset_prefix_cache
    await self.engine_core.reset_prefix_cache_async()
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 959, in reset_prefix_cache_async
    await self.call_utility_async("reset_prefix_cache")
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 924, in call_utility_async
    return await self._call_utility_async(method, *args, engine=self.core_engine)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 936, in _call_utility_async
    await self._send_input_message(message, engine, args)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 905, in _send_input_message
    self.ensure_alive()
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 558, in ensure_alive
    raise EngineDeadError()
vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
2026-01-11 13:40:38,003 [INFO] (Process-138888 agentlightning.execution.client_server)   Shutting down subprocesses

[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-0b8ce880a9f9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e8ab7340e177, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1d1f5729a97b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-77a1927887e4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-9001c178ac97, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f64e2e03f623, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-220a27cbb2fd, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d11b2ea7376d, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f98b4b73f82c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-cd48d86269ca, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b92b518e101b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e9a952fc1830, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-336d8ec5f777, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b8d8af3f978b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6bf37559b57f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c39c61e6c668, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d3014ee79c7f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2fc09b944ec4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-2a92d246e570, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dc0c6593ad58, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c3396dfa5d3f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c058a9d323d4, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dc5396c7b0c7, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-fcc40536e816, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d45137f31fef, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6268fd18a1d9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b1ad157cc85a, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-44f65bf01081, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7957b120a065, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-4e976c2b1733, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-522e9dae458e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-704082ac5cca, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6891d7b3b11e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-65dac6292b1f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-58590c99afc0, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-3a8d0ff3d7f9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-105c37e87ba1, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-c9e7c50bad2f, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d08d7ab13f7e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-65f6ff4063e3, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-6edd00828968, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-5f3f0f5042a9, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8b6fea4eea5c, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-a10526a06018, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b3bccca66f58, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-62b0b6ec62b6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7e0c0c02fa47, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-885a5586d3ed, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-d9a8a1146549, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-1e3a2345f890, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-b83f89c0264e, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-be2dca3b247b, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-dd3c383703ac, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-8dbe35cc1957, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-f19295eb1742, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7840e2a5f0fb, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-86e633f9bbd6, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-ed570d9a9040, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-e0c36d181f50, please check the acc function.
[36m(TaskRunner pid=147334)[0m Acc is None for rollout ro-7a1a36b05d49, please check the acc function.
[36m(vLLMHttpServer pid=153929)[0m INFO 01-11 13:40:35 [v1/engine/async_llm.py:729] Engines are idle, requests have been drained
Traceback (most recent call last):
  File "/workspace/agent-lightning-spider/agent-lightning/examples/spider_async/train_sql_agent.py", line 250, in <module>
    main()
  File "/workspace/agent-lightning-spider/agent-lightning/examples/spider_async/train_sql_agent.py", line 246, in main
    train(config, active_agent, args.external_store_address)
  File "/workspace/agent-lightning-spider/agent-lightning/examples/spider_async/train_sql_agent.py", line 200, in train
    trainer.fit(agent, train_dataset=train_data, val_dataset=val_data)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/trainer/trainer.py", line 424, in fit
    self.strategy.execute(algorithm_bundle, runner_bundle, self.store)
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/execution/client_server.py", line 368, in execute
    asyncio.run(self._execute_algorithm(algorithm, store, stop_evt))
  File "/usr/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/execution/client_server.py", line 154, in _execute_algorithm
    await algorithm(wrapper_store, stop_evt)
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/trainer/trainer.py", line 513, in _algorithm_bundle
    algorithm.run(
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/algorithm/verl/interface.py", line 138, in run
    run_ppo(
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/verl/entrypoint.py", line 58, in run_ppo
    ray.get(
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 2961, in get
    values, debugger_breakpoint = worker.get_objects(
                                  ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 1026, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(EngineDeadError): [36mray::TaskRunner.run()[39m (pid=147334, ip=10.235.192.105, actor_id=546243fcb39a8bf090246daf01000000, repr=<agentlightning.verl.entrypoint.TaskRunner object at 0x7ffff47868a0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/verl/entrypoint.py", line 235, in run
    trainer.fit()
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/verl/trainer.py", line 443, in fit
    val_metrics = self._validate()
                  ^^^^^^^^^^^^^^^^
  File "/workspace/agent-lightning-spider/agent-lightning/agentlightning/verl/trainer.py", line 191, in _validate
    self.async_rollout_manager.sleep()
  File "/workspace/agent-lightning-spider/verl/verl/experimental/agent_loop/agent_loop.py", line 997, in sleep
    self._run_all([replica.sleep() for replica in self.rollout_replicas])
  File "/workspace/agent-lightning-spider/verl/verl/experimental/agent_loop/agent_loop.py", line 1007, in _run_all
    asyncio.run(run_all())
  File "/usr/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/workspace/agent-lightning-spider/verl/verl/experimental/agent_loop/agent_loop.py", line 1005, in run_all
    await asyncio.gather(*tasks)
  File "/workspace/agent-lightning-spider/verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py", line 777, in sleep
    await asyncio.gather(*[server.sleep.remote() for server in self.servers])
  File "/usr/lib/python3.12/asyncio/tasks.py", line 684, in _wrap_awaitable
    return await awaitable
           ^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(EngineDeadError): [36mray::vLLMHttpServer.sleep()[39m (pid=153929, ip=10.235.192.105, actor_id=7e6aab4307e7dd3e4738856401000000, repr=<verl.workers.rollout.vllm_rollout.vllm_async_server.vLLMHttpServer object at 0x7fff42ad20c0>)
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/agent-lightning-spider/verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py", line 562, in sleep
    await self.engine.reset_prefix_cache()
  File "/workspace/adhoc/vllm/vllm/v1/engine/async_llm.py", line 676, in reset_prefix_cache
    await self.engine_core.reset_prefix_cache_async()
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 959, in reset_prefix_cache_async
    await self.call_utility_async("reset_prefix_cache")
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 924, in call_utility_async
    return await self._call_utility_async(method, *args, engine=self.core_engine)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 936, in _call_utility_async
    await self._send_input_message(message, engine, args)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 905, in _send_input_message
    self.ensure_alive()
  File "/workspace/adhoc/vllm/vllm/v1/engine/core_client.py", line 558, in ensure_alive
    raise EngineDeadError()
vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
[36m(vLLMHttpServer pid=153928)[0m INFO 01-11 13:40:35 [v1/engine/async_llm.py:729] Engines are idle, requests have been drained[32m [repeated 7x across cluster][0m
